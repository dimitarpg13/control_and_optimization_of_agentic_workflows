{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context Window Prioritization with Contextual Bandits\n",
        "\n",
        "This notebook demonstrates how to use **Contextual Bandits** for intelligent context window management in LLM-based agentic systems.\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "When working with LLMs, we often have more context available than fits in the context window. We need to intelligently select which pieces of context to include to maximize response quality while staying within token limits.\n",
        "\n",
        "**Key Challenges:**\n",
        "- Variable relevance of context items depending on query type\n",
        "- Limited token budget\n",
        "- Need to balance exploration (trying different strategies) vs exploitation (using known good strategies)\n",
        "- Different users may have different preferences\n",
        "\n",
        "## Why Contextual Bandits?\n",
        "\n",
        "Contextual bandits are ideal for this problem because:\n",
        "1. **Low complexity** - Simple to implement and deploy\n",
        "2. **Fast adaptation** - Learn from each interaction\n",
        "3. **Context-aware** - Decisions depend on query features\n",
        "4. **No delayed rewards needed** - Immediate feedback after each selection\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "```\n",
        "Query Features â†’ Contextual Bandit â†’ Selection Strategy â†’ Context Assembly â†’ LLM â†’ Reward\n",
        "      â†‘                                                                              â”‚\n",
        "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install numpy scipy matplotlib seaborn scikit-learn tiktoken --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from enum import Enum\n",
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"âœ… Imports successful!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Context Window Prioritization with Contextual Bandits\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Models and Enums\n",
        "\n",
        "Define the core data structures for queries, context items, and prioritization strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QueryType(Enum):\n",
        "    \"\"\"Types of queries that require different context prioritization\"\"\"\n",
        "    FACTUAL = \"factual\"           # Need accurate, specific facts\n",
        "    ANALYTICAL = \"analytical\"      # Need comprehensive background\n",
        "    CREATIVE = \"creative\"          # Need diverse inspiration\n",
        "    TROUBLESHOOTING = \"troubleshooting\"  # Need recent, specific docs\n",
        "    SUMMARIZATION = \"summarization\"      # Need broad coverage\n",
        "\n",
        "\n",
        "class ContextSource(Enum):\n",
        "    \"\"\"Sources of context information\"\"\"\n",
        "    CONVERSATION_HISTORY = \"conversation_history\"\n",
        "    USER_PROFILE = \"user_profile\"\n",
        "    RETRIEVED_DOCS = \"retrieved_docs\"\n",
        "    SYSTEM_STATE = \"system_state\"\n",
        "    EXTERNAL_API = \"external_api\"\n",
        "    CACHED_RESULTS = \"cached_results\"\n",
        "\n",
        "\n",
        "class PrioritizationStrategy(Enum):\n",
        "    \"\"\"Available prioritization strategies (arms of the bandit)\"\"\"\n",
        "    RECENT_FIRST = \"recent_first\"           # Prioritize most recent context\n",
        "    RELEVANCE_FIRST = \"relevance_first\"     # Prioritize semantic similarity\n",
        "    DIVERSE_COVERAGE = \"diverse_coverage\"   # Maximize topic diversity\n",
        "    USER_HISTORY = \"user_history\"           # Prioritize user-specific info\n",
        "    HYBRID_BALANCED = \"hybrid_balanced\"     # Weighted combination\n",
        "    SOURCE_PRIORITY = \"source_priority\"     # Prioritize by source type\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ContextItem:\n",
        "    \"\"\"A single piece of context that could be included in the prompt\"\"\"\n",
        "    id: str\n",
        "    content: str\n",
        "    source: ContextSource\n",
        "    token_count: int\n",
        "    timestamp: float  # Unix timestamp, more recent = higher\n",
        "    relevance_score: float  # Pre-computed semantic similarity to query\n",
        "    topic_embedding: np.ndarray  # For diversity calculation\n",
        "    user_specific: bool = False\n",
        "    \n",
        "    def __hash__(self):\n",
        "        return hash(self.id)\n",
        "\n",
        "\n",
        "@dataclass \n",
        "class Query:\n",
        "    \"\"\"Represents an incoming user query\"\"\"\n",
        "    text: str\n",
        "    query_type: QueryType\n",
        "    complexity: float  # 0-1, higher = more complex\n",
        "    user_id: str\n",
        "    embedding: np.ndarray  # Query embedding for similarity\n",
        "    timestamp: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ContextFeatures:\n",
        "    \"\"\"Features extracted from the context for the bandit\"\"\"\n",
        "    query_type: QueryType\n",
        "    query_complexity: float\n",
        "    num_available_items: int\n",
        "    avg_relevance: float\n",
        "    token_budget: int\n",
        "    user_history_length: int\n",
        "    source_distribution: Dict[ContextSource, int]\n",
        "    \n",
        "    def to_vector(self) -> np.ndarray:\n",
        "        \"\"\"Convert features to numerical vector for the bandit\"\"\"\n",
        "        # One-hot encode query type\n",
        "        query_type_vec = np.zeros(len(QueryType))\n",
        "        query_type_vec[list(QueryType).index(self.query_type)] = 1\n",
        "        \n",
        "        # Source distribution as proportions\n",
        "        total_sources = sum(self.source_distribution.values()) or 1\n",
        "        source_vec = np.array([\n",
        "            self.source_distribution.get(s, 0) / total_sources \n",
        "            for s in ContextSource\n",
        "        ])\n",
        "        \n",
        "        # Combine all features\n",
        "        return np.concatenate([\n",
        "            query_type_vec,\n",
        "            [self.query_complexity],\n",
        "            [self.num_available_items / 100],  # Normalize\n",
        "            [self.avg_relevance],\n",
        "            [self.token_budget / 8000],  # Normalize to typical context size\n",
        "            [min(self.user_history_length / 50, 1.0)],  # Cap and normalize\n",
        "            source_vec\n",
        "        ])\n",
        "\n",
        "\n",
        "print(\"âœ… Data models defined!\")\n",
        "print(f\"   - {len(QueryType)} query types\")\n",
        "print(f\"   - {len(ContextSource)} context sources\")\n",
        "print(f\"   - {len(PrioritizationStrategy)} prioritization strategies (bandit arms)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prioritization Strategy Implementations\n",
        "\n",
        "Each strategy represents an \"arm\" of the contextual bandit. Given a set of context items and a token budget, each strategy selects which items to include."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextPrioritizer:\n",
        "    \"\"\"Implements various context prioritization strategies\"\"\"\n",
        "    \n",
        "    # Priority weights for source-based prioritization by query type\n",
        "    SOURCE_PRIORITIES = {\n",
        "        QueryType.FACTUAL: {\n",
        "            ContextSource.RETRIEVED_DOCS: 1.0,\n",
        "            ContextSource.EXTERNAL_API: 0.9,\n",
        "            ContextSource.CACHED_RESULTS: 0.7,\n",
        "            ContextSource.CONVERSATION_HISTORY: 0.4,\n",
        "            ContextSource.USER_PROFILE: 0.3,\n",
        "            ContextSource.SYSTEM_STATE: 0.2,\n",
        "        },\n",
        "        QueryType.ANALYTICAL: {\n",
        "            ContextSource.RETRIEVED_DOCS: 1.0,\n",
        "            ContextSource.CONVERSATION_HISTORY: 0.8,\n",
        "            ContextSource.EXTERNAL_API: 0.7,\n",
        "            ContextSource.USER_PROFILE: 0.5,\n",
        "            ContextSource.CACHED_RESULTS: 0.4,\n",
        "            ContextSource.SYSTEM_STATE: 0.3,\n",
        "        },\n",
        "        QueryType.CREATIVE: {\n",
        "            ContextSource.USER_PROFILE: 0.9,\n",
        "            ContextSource.CONVERSATION_HISTORY: 0.8,\n",
        "            ContextSource.RETRIEVED_DOCS: 0.6,\n",
        "            ContextSource.CACHED_RESULTS: 0.4,\n",
        "            ContextSource.EXTERNAL_API: 0.3,\n",
        "            ContextSource.SYSTEM_STATE: 0.2,\n",
        "        },\n",
        "        QueryType.TROUBLESHOOTING: {\n",
        "            ContextSource.SYSTEM_STATE: 1.0,\n",
        "            ContextSource.CONVERSATION_HISTORY: 0.9,\n",
        "            ContextSource.RETRIEVED_DOCS: 0.8,\n",
        "            ContextSource.EXTERNAL_API: 0.6,\n",
        "            ContextSource.CACHED_RESULTS: 0.5,\n",
        "            ContextSource.USER_PROFILE: 0.3,\n",
        "        },\n",
        "        QueryType.SUMMARIZATION: {\n",
        "            ContextSource.CONVERSATION_HISTORY: 1.0,\n",
        "            ContextSource.RETRIEVED_DOCS: 0.8,\n",
        "            ContextSource.USER_PROFILE: 0.5,\n",
        "            ContextSource.CACHED_RESULTS: 0.4,\n",
        "            ContextSource.EXTERNAL_API: 0.3,\n",
        "            ContextSource.SYSTEM_STATE: 0.2,\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    @staticmethod\n",
        "    def select_items(\n",
        "        items: List[ContextItem],\n",
        "        strategy: PrioritizationStrategy,\n",
        "        token_budget: int,\n",
        "        query: Query\n",
        "    ) -> List[ContextItem]:\n",
        "        \"\"\"Select context items using the specified strategy\"\"\"\n",
        "        \n",
        "        if strategy == PrioritizationStrategy.RECENT_FIRST:\n",
        "            return ContextPrioritizer._recent_first(items, token_budget)\n",
        "        elif strategy == PrioritizationStrategy.RELEVANCE_FIRST:\n",
        "            return ContextPrioritizer._relevance_first(items, token_budget)\n",
        "        elif strategy == PrioritizationStrategy.DIVERSE_COVERAGE:\n",
        "            return ContextPrioritizer._diverse_coverage(items, token_budget)\n",
        "        elif strategy == PrioritizationStrategy.USER_HISTORY:\n",
        "            return ContextPrioritizer._user_history(items, token_budget)\n",
        "        elif strategy == PrioritizationStrategy.HYBRID_BALANCED:\n",
        "            return ContextPrioritizer._hybrid_balanced(items, token_budget, query)\n",
        "        elif strategy == PrioritizationStrategy.SOURCE_PRIORITY:\n",
        "            return ContextPrioritizer._source_priority(items, token_budget, query)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def _greedy_knapsack(items: List[Tuple[float, ContextItem]], budget: int) -> List[ContextItem]:\n",
        "        \"\"\"Greedy knapsack selection based on score/token ratio\"\"\"\n",
        "        # Sort by score (descending)\n",
        "        sorted_items = sorted(items, key=lambda x: x[0], reverse=True)\n",
        "        \n",
        "        selected = []\n",
        "        remaining_budget = budget\n",
        "        \n",
        "        for score, item in sorted_items:\n",
        "            if item.token_count <= remaining_budget:\n",
        "                selected.append(item)\n",
        "                remaining_budget -= item.token_count\n",
        "                \n",
        "            if remaining_budget <= 0:\n",
        "                break\n",
        "                \n",
        "        return selected\n",
        "    \n",
        "    @staticmethod\n",
        "    def _recent_first(items: List[ContextItem], budget: int) -> List[ContextItem]:\n",
        "        \"\"\"Prioritize most recent context items\"\"\"\n",
        "        scored = [(item.timestamp, item) for item in items]\n",
        "        return ContextPrioritizer._greedy_knapsack(scored, budget)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _relevance_first(items: List[ContextItem], budget: int) -> List[ContextItem]:\n",
        "        \"\"\"Prioritize highest semantic relevance\"\"\"\n",
        "        scored = [(item.relevance_score, item) for item in items]\n",
        "        return ContextPrioritizer._greedy_knapsack(scored, budget)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _diverse_coverage(items: List[ContextItem], budget: int) -> List[ContextItem]:\n",
        "        \"\"\"Maximize topic diversity using MMR-like selection\"\"\"\n",
        "        if not items:\n",
        "            return []\n",
        "            \n",
        "        selected = []\n",
        "        remaining = list(items)\n",
        "        remaining_budget = budget\n",
        "        \n",
        "        # Start with highest relevance item\n",
        "        first = max(remaining, key=lambda x: x.relevance_score)\n",
        "        if first.token_count <= remaining_budget:\n",
        "            selected.append(first)\n",
        "            remaining.remove(first)\n",
        "            remaining_budget -= first.token_count\n",
        "        \n",
        "        # Iteratively add items that maximize diversity\n",
        "        lambda_param = 0.5  # Balance relevance vs diversity\n",
        "        \n",
        "        while remaining and remaining_budget > 0:\n",
        "            best_score = -float('inf')\n",
        "            best_item = None\n",
        "            \n",
        "            for item in remaining:\n",
        "                if item.token_count > remaining_budget:\n",
        "                    continue\n",
        "                    \n",
        "                # Calculate diversity as min distance to selected items\n",
        "                if selected:\n",
        "                    min_similarity = min(\n",
        "                        np.dot(item.topic_embedding, s.topic_embedding) / \n",
        "                        (np.linalg.norm(item.topic_embedding) * np.linalg.norm(s.topic_embedding) + 1e-8)\n",
        "                        for s in selected\n",
        "                    )\n",
        "                    diversity = 1 - min_similarity\n",
        "                else:\n",
        "                    diversity = 1.0\n",
        "                \n",
        "                # MMR score\n",
        "                score = lambda_param * item.relevance_score + (1 - lambda_param) * diversity\n",
        "                \n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_item = item\n",
        "            \n",
        "            if best_item is None:\n",
        "                break\n",
        "                \n",
        "            selected.append(best_item)\n",
        "            remaining.remove(best_item)\n",
        "            remaining_budget -= best_item.token_count\n",
        "        \n",
        "        return selected\n",
        "    \n",
        "    @staticmethod\n",
        "    def _user_history(items: List[ContextItem], budget: int) -> List[ContextItem]:\n",
        "        \"\"\"Prioritize user-specific context\"\"\"\n",
        "        # Score: user_specific boost + relevance\n",
        "        scored = [\n",
        "            (item.relevance_score + (0.5 if item.user_specific else 0), item)\n",
        "            for item in items\n",
        "        ]\n",
        "        return ContextPrioritizer._greedy_knapsack(scored, budget)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _hybrid_balanced(items: List[ContextItem], budget: int, query: Query) -> List[ContextItem]:\n",
        "        \"\"\"Balanced combination of recency, relevance, and source priority\"\"\"\n",
        "        max_ts = max(i.timestamp for i in items) if items else 1\n",
        "        min_ts = min(i.timestamp for i in items) if items else 0\n",
        "        ts_range = max_ts - min_ts if max_ts != min_ts else 1\n",
        "        \n",
        "        source_weights = ContextPrioritizer.SOURCE_PRIORITIES.get(\n",
        "            query.query_type, \n",
        "            {s: 0.5 for s in ContextSource}\n",
        "        )\n",
        "        \n",
        "        scored = []\n",
        "        for item in items:\n",
        "            recency = (item.timestamp - min_ts) / ts_range\n",
        "            relevance = item.relevance_score\n",
        "            source_score = source_weights.get(item.source, 0.5)\n",
        "            \n",
        "            # Weighted combination\n",
        "            score = 0.4 * relevance + 0.3 * recency + 0.3 * source_score\n",
        "            scored.append((score, item))\n",
        "        \n",
        "        return ContextPrioritizer._greedy_knapsack(scored, budget)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _source_priority(items: List[ContextItem], budget: int, query: Query) -> List[ContextItem]:\n",
        "        \"\"\"Prioritize by source type based on query type\"\"\"\n",
        "        source_weights = ContextPrioritizer.SOURCE_PRIORITIES.get(\n",
        "            query.query_type,\n",
        "            {s: 0.5 for s in ContextSource}\n",
        "        )\n",
        "        \n",
        "        scored = [\n",
        "            (source_weights.get(item.source, 0.5) * item.relevance_score, item)\n",
        "            for item in items\n",
        "        ]\n",
        "        return ContextPrioritizer._greedy_knapsack(scored, budget)\n",
        "\n",
        "\n",
        "print(\"âœ… Prioritization strategies implemented!\")\n",
        "print(\"   Strategies:\")\n",
        "for strategy in PrioritizationStrategy:\n",
        "    print(f\"   - {strategy.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simulated Environment\n",
        "\n",
        "Create a simulated environment that:\n",
        "1. Generates realistic queries and context items\n",
        "2. Simulates LLM response quality based on context selection\n",
        "3. Provides rewards based on response quality and token efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextWindowSimulator:\n",
        "    \"\"\"Simulates the context prioritization environment\"\"\"\n",
        "    \n",
        "    # Ground truth: which strategies work best for each query type\n",
        "    # This simulates what we'd learn from real user feedback\n",
        "    OPTIMAL_STRATEGIES = {\n",
        "        QueryType.FACTUAL: {\n",
        "            PrioritizationStrategy.RELEVANCE_FIRST: 0.9,\n",
        "            PrioritizationStrategy.SOURCE_PRIORITY: 0.85,\n",
        "            PrioritizationStrategy.HYBRID_BALANCED: 0.75,\n",
        "            PrioritizationStrategy.RECENT_FIRST: 0.5,\n",
        "            PrioritizationStrategy.DIVERSE_COVERAGE: 0.6,\n",
        "            PrioritizationStrategy.USER_HISTORY: 0.55,\n",
        "        },\n",
        "        QueryType.ANALYTICAL: {\n",
        "            PrioritizationStrategy.DIVERSE_COVERAGE: 0.9,\n",
        "            PrioritizationStrategy.HYBRID_BALANCED: 0.85,\n",
        "            PrioritizationStrategy.RELEVANCE_FIRST: 0.7,\n",
        "            PrioritizationStrategy.SOURCE_PRIORITY: 0.65,\n",
        "            PrioritizationStrategy.RECENT_FIRST: 0.5,\n",
        "            PrioritizationStrategy.USER_HISTORY: 0.6,\n",
        "        },\n",
        "        QueryType.CREATIVE: {\n",
        "            PrioritizationStrategy.USER_HISTORY: 0.9,\n",
        "            PrioritizationStrategy.DIVERSE_COVERAGE: 0.85,\n",
        "            PrioritizationStrategy.HYBRID_BALANCED: 0.7,\n",
        "            PrioritizationStrategy.RELEVANCE_FIRST: 0.5,\n",
        "            PrioritizationStrategy.RECENT_FIRST: 0.55,\n",
        "            PrioritizationStrategy.SOURCE_PRIORITY: 0.45,\n",
        "        },\n",
        "        QueryType.TROUBLESHOOTING: {\n",
        "            PrioritizationStrategy.RECENT_FIRST: 0.9,\n",
        "            PrioritizationStrategy.SOURCE_PRIORITY: 0.85,\n",
        "            PrioritizationStrategy.HYBRID_BALANCED: 0.8,\n",
        "            PrioritizationStrategy.RELEVANCE_FIRST: 0.65,\n",
        "            PrioritizationStrategy.DIVERSE_COVERAGE: 0.5,\n",
        "            PrioritizationStrategy.USER_HISTORY: 0.7,\n",
        "        },\n",
        "        QueryType.SUMMARIZATION: {\n",
        "            PrioritizationStrategy.HYBRID_BALANCED: 0.9,\n",
        "            PrioritizationStrategy.DIVERSE_COVERAGE: 0.85,\n",
        "            PrioritizationStrategy.RECENT_FIRST: 0.75,\n",
        "            PrioritizationStrategy.RELEVANCE_FIRST: 0.7,\n",
        "            PrioritizationStrategy.USER_HISTORY: 0.6,\n",
        "            PrioritizationStrategy.SOURCE_PRIORITY: 0.65,\n",
        "        },\n",
        "    }\n",
        "    \n",
        "    def __init__(self, num_users: int = 10, seed: int = 42):\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        self.num_users = num_users\n",
        "        self.users = self._create_users()\n",
        "        self.interaction_count = 0\n",
        "        \n",
        "    def _create_users(self) -> Dict[str, Dict]:\n",
        "        \"\"\"Create simulated user profiles with preferences\"\"\"\n",
        "        users = {}\n",
        "        for i in range(self.num_users):\n",
        "            user_id = f\"user_{i}\"\n",
        "            # Each user has slightly different preferences\n",
        "            preference_noise = {\n",
        "                strategy: np.random.normal(0, 0.1)\n",
        "                for strategy in PrioritizationStrategy\n",
        "            }\n",
        "            users[user_id] = {\n",
        "                \"id\": user_id,\n",
        "                \"history_length\": np.random.randint(5, 100),\n",
        "                \"preference_noise\": preference_noise,\n",
        "                \"favorite_query_type\": random.choice(list(QueryType)),\n",
        "            }\n",
        "        return users\n",
        "    \n",
        "    def generate_query(self, user_id: Optional[str] = None) -> Query:\n",
        "        \"\"\"Generate a random query\"\"\"\n",
        "        if user_id is None:\n",
        "            user_id = random.choice(list(self.users.keys()))\n",
        "            \n",
        "        query_type = random.choice(list(QueryType))\n",
        "        complexity = np.random.beta(2, 5)  # Skewed toward simpler queries\n",
        "        \n",
        "        return Query(\n",
        "            text=f\"Sample {query_type.value} query\",\n",
        "            query_type=query_type,\n",
        "            complexity=complexity,\n",
        "            user_id=user_id,\n",
        "            embedding=np.random.randn(64),  # Simulated embedding\n",
        "            timestamp=1000000 + self.interaction_count\n",
        "        )\n",
        "    \n",
        "    def generate_context_items(self, query: Query, num_items: int = 20) -> List[ContextItem]:\n",
        "        \"\"\"Generate simulated context items for a query\"\"\"\n",
        "        items = []\n",
        "        base_timestamp = query.timestamp - 1000\n",
        "        \n",
        "        for i in range(num_items):\n",
        "            source = random.choice(list(ContextSource))\n",
        "            \n",
        "            # Relevance depends on source and query type\n",
        "            base_relevance = np.random.beta(2, 3)\n",
        "            source_bonus = 0.2 if source in [\n",
        "                ContextSource.RETRIEVED_DOCS, \n",
        "                ContextSource.CONVERSATION_HISTORY\n",
        "            ] else 0\n",
        "            relevance = min(1.0, base_relevance + source_bonus + np.random.normal(0, 0.1))\n",
        "            \n",
        "            items.append(ContextItem(\n",
        "                id=f\"ctx_{i}\",\n",
        "                content=f\"Context item {i} from {source.value}\",\n",
        "                source=source,\n",
        "                token_count=np.random.randint(50, 500),\n",
        "                timestamp=base_timestamp + i * 10 + np.random.randint(0, 5),\n",
        "                relevance_score=max(0, min(1, relevance)),\n",
        "                topic_embedding=np.random.randn(32),\n",
        "                user_specific=(source == ContextSource.USER_PROFILE or \n",
        "                              (source == ContextSource.CONVERSATION_HISTORY and random.random() > 0.5))\n",
        "            ))\n",
        "        \n",
        "        return items\n",
        "    \n",
        "    def get_reward(\n",
        "        self, \n",
        "        query: Query, \n",
        "        strategy: PrioritizationStrategy,\n",
        "        selected_items: List[ContextItem],\n",
        "        token_budget: int\n",
        "    ) -> Tuple[float, Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Calculate reward based on strategy effectiveness.\n",
        "        Returns (reward, details_dict)\n",
        "        \"\"\"\n",
        "        self.interaction_count += 1\n",
        "        \n",
        "        # Base quality from strategy-query match\n",
        "        base_quality = self.OPTIMAL_STRATEGIES[query.query_type][strategy]\n",
        "        \n",
        "        # User-specific adjustment\n",
        "        user = self.users[query.user_id]\n",
        "        user_adjustment = user[\"preference_noise\"][strategy]\n",
        "        \n",
        "        # Token efficiency bonus/penalty\n",
        "        tokens_used = sum(item.token_count for item in selected_items)\n",
        "        efficiency = 1.0 - (tokens_used / token_budget) * 0.1  # Small penalty for using more tokens\n",
        "        \n",
        "        # Coverage bonus (did we include relevant items?)\n",
        "        if selected_items:\n",
        "            avg_relevance = np.mean([item.relevance_score for item in selected_items])\n",
        "        else:\n",
        "            avg_relevance = 0\n",
        "        \n",
        "        # Complexity adjustment (harder queries benefit more from good strategies)\n",
        "        complexity_factor = 1.0 + query.complexity * 0.2\n",
        "        \n",
        "        # Final quality with noise\n",
        "        quality = (base_quality + user_adjustment) * efficiency * complexity_factor\n",
        "        quality = quality * (0.8 + 0.4 * avg_relevance)  # Relevance matters\n",
        "        quality += np.random.normal(0, 0.05)  # Add noise\n",
        "        quality = max(0, min(1, quality))\n",
        "        \n",
        "        # Reward combines quality and efficiency\n",
        "        reward = 0.8 * quality + 0.2 * efficiency\n",
        "        \n",
        "        details = {\n",
        "            \"quality\": quality,\n",
        "            \"efficiency\": efficiency,\n",
        "            \"tokens_used\": tokens_used,\n",
        "            \"avg_relevance\": avg_relevance,\n",
        "            \"base_strategy_quality\": base_quality,\n",
        "        }\n",
        "        \n",
        "        return reward, details\n",
        "\n",
        "\n",
        "# Test the simulator\n",
        "simulator = ContextWindowSimulator()\n",
        "test_query = simulator.generate_query()\n",
        "test_items = simulator.generate_context_items(test_query)\n",
        "\n",
        "print(\"âœ… Simulator created!\")\n",
        "print(f\"\\nðŸ“‹ Sample query:\")\n",
        "print(f\"   Type: {test_query.query_type.value}\")\n",
        "print(f\"   Complexity: {test_query.complexity:.2f}\")\n",
        "print(f\"   User: {test_query.user_id}\")\n",
        "print(f\"\\nðŸ“¦ Generated {len(test_items)} context items:\")\n",
        "print(f\"   Total tokens: {sum(i.token_count for i in test_items)}\")\n",
        "print(f\"   Sources: {set(i.source.value for i in test_items)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Contextual Bandit Implementations\n",
        "\n",
        "We implement three contextual bandit algorithms:\n",
        "1. **Epsilon-Greedy with Linear Model** - Simple but effective\n",
        "2. **Thompson Sampling with Bayesian Linear Regression** - Better exploration\n",
        "3. **Upper Confidence Bound (LinUCB)** - Optimism under uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextualBandit(ABC):\n",
        "    \"\"\"Abstract base class for contextual bandits\"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, context_dim: int):\n",
        "        self.n_arms = n_arms\n",
        "        self.context_dim = context_dim\n",
        "        self.arms = list(PrioritizationStrategy)\n",
        "        self.history: List[Dict] = []\n",
        "        \n",
        "    @abstractmethod\n",
        "    def select_arm(self, context: np.ndarray) -> int:\n",
        "        \"\"\"Select an arm given the context\"\"\"\n",
        "        pass\n",
        "    \n",
        "    @abstractmethod\n",
        "    def update(self, context: np.ndarray, arm: int, reward: float):\n",
        "        \"\"\"Update the model with observed reward\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def get_strategy(self, arm_index: int) -> PrioritizationStrategy:\n",
        "        \"\"\"Convert arm index to strategy\"\"\"\n",
        "        return self.arms[arm_index]\n",
        "\n",
        "\n",
        "class EpsilonGreedyLinear(ContextualBandit):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy contextual bandit with linear reward model.\n",
        "    Uses ridge regression to estimate expected rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, context_dim: int, epsilon: float = 0.1, \n",
        "                 lambda_reg: float = 1.0, decay_rate: float = 0.995):\n",
        "        super().__init__(n_arms, context_dim)\n",
        "        self.epsilon = epsilon\n",
        "        self.initial_epsilon = epsilon\n",
        "        self.decay_rate = decay_rate\n",
        "        self.lambda_reg = lambda_reg\n",
        "        \n",
        "        # Initialize linear models for each arm\n",
        "        # A[a] = X'X + lambda*I, b[a] = X'y\n",
        "        self.A = [np.eye(context_dim) * lambda_reg for _ in range(n_arms)]\n",
        "        self.b = [np.zeros(context_dim) for _ in range(n_arms)]\n",
        "        self.theta = [np.zeros(context_dim) for _ in range(n_arms)]  # Weights\n",
        "        \n",
        "        self.arm_counts = np.zeros(n_arms)\n",
        "        self.total_reward = np.zeros(n_arms)\n",
        "        \n",
        "    def select_arm(self, context: np.ndarray) -> int:\n",
        "        \"\"\"Epsilon-greedy selection with linear predictions\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.n_arms)\n",
        "        \n",
        "        # Predict expected reward for each arm\n",
        "        expected_rewards = [\n",
        "            np.dot(self.theta[a], context) for a in range(self.n_arms)\n",
        "        ]\n",
        "        return int(np.argmax(expected_rewards))\n",
        "    \n",
        "    def update(self, context: np.ndarray, arm: int, reward: float):\n",
        "        \"\"\"Update the linear model for the selected arm\"\"\"\n",
        "        # Update sufficient statistics\n",
        "        self.A[arm] += np.outer(context, context)\n",
        "        self.b[arm] += reward * context\n",
        "        \n",
        "        # Recompute weights (ridge regression solution)\n",
        "        try:\n",
        "            self.theta[arm] = np.linalg.solve(self.A[arm], self.b[arm])\n",
        "        except np.linalg.LinAlgError:\n",
        "            self.theta[arm] = np.linalg.lstsq(self.A[arm], self.b[arm], rcond=None)[0]\n",
        "        \n",
        "        self.arm_counts[arm] += 1\n",
        "        self.total_reward[arm] += reward\n",
        "        \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(0.01, self.epsilon * self.decay_rate)\n",
        "        \n",
        "        self.history.append({\n",
        "            \"arm\": arm,\n",
        "            \"reward\": reward,\n",
        "            \"epsilon\": self.epsilon,\n",
        "        })\n",
        "\n",
        "\n",
        "class ThompsonSamplingLinear(ContextualBandit):\n",
        "    \"\"\"\n",
        "    Thompson Sampling with Bayesian linear regression.\n",
        "    Maintains posterior distribution over weights.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, context_dim: int, \n",
        "                 lambda_reg: float = 1.0, sigma: float = 0.5):\n",
        "        super().__init__(n_arms, context_dim)\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.sigma = sigma  # Observation noise\n",
        "        \n",
        "        # Prior: theta ~ N(0, lambda^-1 * I)\n",
        "        # Posterior after observing data: theta ~ N(mu, Sigma)\n",
        "        self.B = [np.eye(context_dim) * lambda_reg for _ in range(n_arms)]  # Precision\n",
        "        self.mu = [np.zeros(context_dim) for _ in range(n_arms)]  # Mean\n",
        "        self.f = [np.zeros(context_dim) for _ in range(n_arms)]  # X'y accumulator\n",
        "        \n",
        "        self.arm_counts = np.zeros(n_arms)\n",
        "        \n",
        "    def select_arm(self, context: np.ndarray) -> int:\n",
        "        \"\"\"Thompson sampling: sample from posterior and select best arm\"\"\"\n",
        "        sampled_rewards = []\n",
        "        \n",
        "        for a in range(self.n_arms):\n",
        "            # Sample weights from posterior\n",
        "            try:\n",
        "                cov = np.linalg.inv(self.B[a]) * (self.sigma ** 2)\n",
        "                # Ensure covariance is positive definite\n",
        "                cov = (cov + cov.T) / 2\n",
        "                eigvals = np.linalg.eigvalsh(cov)\n",
        "                if np.min(eigvals) < 0:\n",
        "                    cov += np.eye(self.context_dim) * (abs(np.min(eigvals)) + 1e-6)\n",
        "                theta_sample = np.random.multivariate_normal(self.mu[a], cov)\n",
        "            except np.linalg.LinAlgError:\n",
        "                theta_sample = self.mu[a] + np.random.randn(self.context_dim) * 0.1\n",
        "            \n",
        "            # Predict reward with sampled weights\n",
        "            expected_reward = np.dot(theta_sample, context)\n",
        "            sampled_rewards.append(expected_reward)\n",
        "        \n",
        "        return int(np.argmax(sampled_rewards))\n",
        "    \n",
        "    def update(self, context: np.ndarray, arm: int, reward: float):\n",
        "        \"\"\"Update posterior for the selected arm\"\"\"\n",
        "        # Update precision matrix\n",
        "        self.B[arm] += np.outer(context, context) / (self.sigma ** 2)\n",
        "        \n",
        "        # Update reward accumulator\n",
        "        self.f[arm] += reward * context / (self.sigma ** 2)\n",
        "        \n",
        "        # Update posterior mean\n",
        "        try:\n",
        "            self.mu[arm] = np.linalg.solve(self.B[arm], self.f[arm])\n",
        "        except np.linalg.LinAlgError:\n",
        "            self.mu[arm] = np.linalg.lstsq(self.B[arm], self.f[arm], rcond=None)[0]\n",
        "        \n",
        "        self.arm_counts[arm] += 1\n",
        "        \n",
        "        self.history.append({\n",
        "            \"arm\": arm,\n",
        "            \"reward\": reward,\n",
        "        })\n",
        "\n",
        "\n",
        "class LinUCB(ContextualBandit):\n",
        "    \"\"\"\n",
        "    Linear Upper Confidence Bound algorithm.\n",
        "    Balances exploration and exploitation using confidence bounds.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, context_dim: int, \n",
        "                 alpha: float = 1.0, lambda_reg: float = 1.0):\n",
        "        super().__init__(n_arms, context_dim)\n",
        "        self.alpha = alpha  # Exploration parameter\n",
        "        self.lambda_reg = lambda_reg\n",
        "        \n",
        "        # Initialize for each arm\n",
        "        self.A = [np.eye(context_dim) * lambda_reg for _ in range(n_arms)]\n",
        "        self.b = [np.zeros(context_dim) for _ in range(n_arms)]\n",
        "        self.theta = [np.zeros(context_dim) for _ in range(n_arms)]\n",
        "        \n",
        "        self.arm_counts = np.zeros(n_arms)\n",
        "        \n",
        "    def select_arm(self, context: np.ndarray) -> int:\n",
        "        \"\"\"Select arm with highest UCB\"\"\"\n",
        "        ucb_values = []\n",
        "        \n",
        "        for a in range(self.n_arms):\n",
        "            # Compute inverse of A\n",
        "            try:\n",
        "                A_inv = np.linalg.inv(self.A[a])\n",
        "            except np.linalg.LinAlgError:\n",
        "                A_inv = np.linalg.pinv(self.A[a])\n",
        "            \n",
        "            # Expected reward\n",
        "            expected = np.dot(self.theta[a], context)\n",
        "            \n",
        "            # Confidence bonus\n",
        "            confidence = self.alpha * np.sqrt(\n",
        "                np.dot(context, np.dot(A_inv, context))\n",
        "            )\n",
        "            \n",
        "            ucb = expected + confidence\n",
        "            ucb_values.append(ucb)\n",
        "        \n",
        "        return int(np.argmax(ucb_values))\n",
        "    \n",
        "    def update(self, context: np.ndarray, arm: int, reward: float):\n",
        "        \"\"\"Update the model for the selected arm\"\"\"\n",
        "        self.A[arm] += np.outer(context, context)\n",
        "        self.b[arm] += reward * context\n",
        "        \n",
        "        try:\n",
        "            self.theta[arm] = np.linalg.solve(self.A[arm], self.b[arm])\n",
        "        except np.linalg.LinAlgError:\n",
        "            self.theta[arm] = np.linalg.lstsq(self.A[arm], self.b[arm], rcond=None)[0]\n",
        "        \n",
        "        self.arm_counts[arm] += 1\n",
        "        \n",
        "        self.history.append({\n",
        "            \"arm\": arm,\n",
        "            \"reward\": reward,\n",
        "        })\n",
        "\n",
        "\n",
        "print(\"âœ… Contextual bandit algorithms implemented!\")\n",
        "print(\"   - EpsilonGreedyLinear: Simple epsilon-greedy with linear model\")\n",
        "print(\"   - ThompsonSamplingLinear: Bayesian approach with uncertainty sampling\")\n",
        "print(\"   - LinUCB: Optimistic exploration using confidence bounds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Context Prioritization System\n",
        "\n",
        "The main system that orchestrates the bandit, context selection, and learning loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextPrioritizationSystem:\n",
        "    \"\"\"\n",
        "    Main system that uses contextual bandits for intelligent context prioritization.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        bandit: ContextualBandit,\n",
        "        token_budget: int = 4000,\n",
        "        simulator: Optional[ContextWindowSimulator] = None\n",
        "    ):\n",
        "        self.bandit = bandit\n",
        "        self.token_budget = token_budget\n",
        "        self.simulator = simulator or ContextWindowSimulator()\n",
        "        \n",
        "        self.interaction_history: List[Dict] = []\n",
        "        self.cumulative_reward = 0\n",
        "        self.optimal_cumulative = 0  # For regret calculation\n",
        "        \n",
        "    def extract_features(\n",
        "        self, \n",
        "        query: Query, \n",
        "        items: List[ContextItem]\n",
        "    ) -> ContextFeatures:\n",
        "        \"\"\"Extract features from query and available context\"\"\"\n",
        "        source_dist = defaultdict(int)\n",
        "        for item in items:\n",
        "            source_dist[item.source] += 1\n",
        "            \n",
        "        user = self.simulator.users.get(query.user_id, {})\n",
        "        \n",
        "        return ContextFeatures(\n",
        "            query_type=query.query_type,\n",
        "            query_complexity=query.complexity,\n",
        "            num_available_items=len(items),\n",
        "            avg_relevance=np.mean([i.relevance_score for i in items]) if items else 0,\n",
        "            token_budget=self.token_budget,\n",
        "            user_history_length=user.get(\"history_length\", 0),\n",
        "            source_distribution=dict(source_dist)\n",
        "        )\n",
        "    \n",
        "    def process_query(\n",
        "        self, \n",
        "        query: Optional[Query] = None,\n",
        "        items: Optional[List[ContextItem]] = None,\n",
        "        return_details: bool = False\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a query using the contextual bandit to select prioritization strategy.\n",
        "        \"\"\"\n",
        "        # Generate query and items if not provided\n",
        "        if query is None:\n",
        "            query = self.simulator.generate_query()\n",
        "        if items is None:\n",
        "            items = self.simulator.generate_context_items(query)\n",
        "            \n",
        "        # Extract features and convert to vector\n",
        "        features = self.extract_features(query, items)\n",
        "        context_vector = features.to_vector()\n",
        "        \n",
        "        # Select strategy using bandit\n",
        "        arm_index = self.bandit.select_arm(context_vector)\n",
        "        strategy = self.bandit.get_strategy(arm_index)\n",
        "        \n",
        "        # Apply the selected strategy\n",
        "        selected_items = ContextPrioritizer.select_items(\n",
        "            items, strategy, self.token_budget, query\n",
        "        )\n",
        "        \n",
        "        # Get reward from simulator\n",
        "        reward, reward_details = self.simulator.get_reward(\n",
        "            query, strategy, selected_items, self.token_budget\n",
        "        )\n",
        "        \n",
        "        # Update bandit\n",
        "        self.bandit.update(context_vector, arm_index, reward)\n",
        "        \n",
        "        # Track metrics\n",
        "        self.cumulative_reward += reward\n",
        "        \n",
        "        # Calculate optimal reward for regret\n",
        "        optimal_strategy = max(\n",
        "            PrioritizationStrategy,\n",
        "            key=lambda s: self.simulator.OPTIMAL_STRATEGIES[query.query_type][s]\n",
        "        )\n",
        "        optimal_items = ContextPrioritizer.select_items(\n",
        "            items, optimal_strategy, self.token_budget, query\n",
        "        )\n",
        "        optimal_reward, _ = self.simulator.get_reward(\n",
        "            query, optimal_strategy, optimal_items, self.token_budget\n",
        "        )\n",
        "        self.optimal_cumulative += optimal_reward\n",
        "        \n",
        "        # Record interaction\n",
        "        interaction = {\n",
        "            \"query_type\": query.query_type.value,\n",
        "            \"strategy\": strategy.value,\n",
        "            \"reward\": reward,\n",
        "            \"optimal_reward\": optimal_reward,\n",
        "            \"regret\": optimal_reward - reward,\n",
        "            \"tokens_used\": sum(i.token_count for i in selected_items),\n",
        "            \"items_selected\": len(selected_items),\n",
        "            \"cumulative_reward\": self.cumulative_reward,\n",
        "            \"cumulative_regret\": self.optimal_cumulative - self.cumulative_reward,\n",
        "        }\n",
        "        self.interaction_history.append(interaction)\n",
        "        \n",
        "        if return_details:\n",
        "            interaction[\"features\"] = features\n",
        "            interaction[\"reward_details\"] = reward_details\n",
        "            interaction[\"selected_items\"] = selected_items\n",
        "            \n",
        "        return interaction\n",
        "    \n",
        "    def run_simulation(self, n_iterations: int, verbose: bool = True) -> List[Dict]:\n",
        "        \"\"\"Run multiple iterations of the simulation\"\"\"\n",
        "        if verbose:\n",
        "            print(f\"Running {n_iterations} iterations...\")\n",
        "            \n",
        "        for i in range(n_iterations):\n",
        "            self.process_query()\n",
        "            \n",
        "            if verbose and (i + 1) % 200 == 0:\n",
        "                recent_rewards = [h[\"reward\"] for h in self.interaction_history[-200:]]\n",
        "                avg_reward = np.mean(recent_rewards)\n",
        "                cumulative_regret = self.interaction_history[-1][\"cumulative_regret\"]\n",
        "                print(f\"  Iteration {i+1}: Avg Reward (last 200) = {avg_reward:.3f}, \"\n",
        "                      f\"Cumulative Regret = {cumulative_regret:.2f}\")\n",
        "                \n",
        "        return self.interaction_history\n",
        "    \n",
        "    def get_learned_policy(self) -> Dict[str, str]:\n",
        "        \"\"\"Extract the learned policy (best strategy for each query type)\"\"\"\n",
        "        policy = {}\n",
        "        \n",
        "        for query_type in QueryType:\n",
        "            # Create a synthetic context for this query type\n",
        "            features = ContextFeatures(\n",
        "                query_type=query_type,\n",
        "                query_complexity=0.5,\n",
        "                num_available_items=20,\n",
        "                avg_relevance=0.5,\n",
        "                token_budget=self.token_budget,\n",
        "                user_history_length=25,\n",
        "                source_distribution={s: 3 for s in ContextSource}\n",
        "            )\n",
        "            context_vector = features.to_vector()\n",
        "            \n",
        "            # Get predictions for all arms\n",
        "            if hasattr(self.bandit, 'theta'):\n",
        "                predictions = [\n",
        "                    np.dot(self.bandit.theta[a], context_vector)\n",
        "                    for a in range(self.bandit.n_arms)\n",
        "                ]\n",
        "            elif hasattr(self.bandit, 'mu'):\n",
        "                predictions = [\n",
        "                    np.dot(self.bandit.mu[a], context_vector)\n",
        "                    for a in range(self.bandit.n_arms)\n",
        "                ]\n",
        "            else:\n",
        "                predictions = [0] * self.bandit.n_arms\n",
        "                \n",
        "            best_arm = int(np.argmax(predictions))\n",
        "            best_strategy = self.bandit.get_strategy(best_arm)\n",
        "            \n",
        "            policy[query_type.value] = {\n",
        "                \"strategy\": best_strategy.value,\n",
        "                \"confidence\": predictions[best_arm] if predictions else 0\n",
        "            }\n",
        "            \n",
        "        return policy\n",
        "\n",
        "\n",
        "# Quick test\n",
        "n_arms = len(PrioritizationStrategy)\n",
        "context_dim = len(ContextFeatures(\n",
        "    QueryType.FACTUAL, 0.5, 20, 0.5, 4000, 25, \n",
        "    {s: 3 for s in ContextSource}\n",
        ").to_vector())\n",
        "\n",
        "print(f\"âœ… Context Prioritization System defined!\")\n",
        "print(f\"   Arms (strategies): {n_arms}\")\n",
        "print(f\"   Context dimension: {context_dim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and Algorithm Comparison\n",
        "\n",
        "Train all three contextual bandit algorithms and compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_compare(n_iterations: int = 1000, seed: int = 42):\n",
        "    \"\"\"Train all bandit algorithms and compare performance\"\"\"\n",
        "    \n",
        "    # Reset random state for fair comparison\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "    n_arms = len(PrioritizationStrategy)\n",
        "    context_dim = len(ContextFeatures(\n",
        "        QueryType.FACTUAL, 0.5, 20, 0.5, 4000, 25,\n",
        "        {s: 3 for s in ContextSource}\n",
        "    ).to_vector())\n",
        "    \n",
        "    # Create bandits\n",
        "    bandits = {\n",
        "        \"Epsilon-Greedy\": EpsilonGreedyLinear(n_arms, context_dim, epsilon=0.2),\n",
        "        \"Thompson Sampling\": ThompsonSamplingLinear(n_arms, context_dim),\n",
        "        \"LinUCB\": LinUCB(n_arms, context_dim, alpha=0.5),\n",
        "    }\n",
        "    \n",
        "    # Also add random baseline\n",
        "    class RandomBandit(ContextualBandit):\n",
        "        def select_arm(self, context): return np.random.randint(self.n_arms)\n",
        "        def update(self, context, arm, reward): pass\n",
        "    \n",
        "    bandits[\"Random\"] = RandomBandit(n_arms, context_dim)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"TRAINING CONTEXTUAL BANDITS FOR CONTEXT PRIORITIZATION\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for name, bandit in bandits.items():\n",
        "        print(f\"\\n{'â”€' * 50}\")\n",
        "        print(f\"Training: {name}\")\n",
        "        print(f\"{'â”€' * 50}\")\n",
        "        \n",
        "        # Create fresh simulator for each bandit\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        simulator = ContextWindowSimulator(seed=seed)\n",
        "        \n",
        "        system = ContextPrioritizationSystem(\n",
        "            bandit=bandit,\n",
        "            token_budget=4000,\n",
        "            simulator=simulator\n",
        "        )\n",
        "        \n",
        "        history = system.run_simulation(n_iterations, verbose=True)\n",
        "        \n",
        "        results[name] = {\n",
        "            \"history\": history,\n",
        "            \"system\": system,\n",
        "            \"final_reward\": np.mean([h[\"reward\"] for h in history[-100:]]),\n",
        "            \"final_regret\": history[-1][\"cumulative_regret\"],\n",
        "            \"learned_policy\": system.get_learned_policy() if name != \"Random\" else None\n",
        "        }\n",
        "        \n",
        "    return results\n",
        "\n",
        "\n",
        "# Run training\n",
        "results = train_and_compare(n_iterations=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualization and Analysis\n",
        "\n",
        "Visualize the learning curves, cumulative regret, and learned policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def moving_average(data, window=50):\n",
        "    \"\"\"Compute moving average\"\"\"\n",
        "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
        "\n",
        "\n",
        "def plot_results(results):\n",
        "    \"\"\"Create comprehensive visualization of results\"\"\"\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 12))\n",
        "    colors = {'Epsilon-Greedy': '#e74c3c', 'Thompson Sampling': '#3498db', \n",
        "              'LinUCB': '#2ecc71', 'Random': '#95a5a6'}\n",
        "    \n",
        "    # 1. Cumulative Reward\n",
        "    ax1 = fig.add_subplot(2, 2, 1)\n",
        "    for name, data in results.items():\n",
        "        rewards = [h[\"cumulative_reward\"] for h in data[\"history\"]]\n",
        "        ax1.plot(rewards, label=name, color=colors[name], linewidth=2)\n",
        "    ax1.set_xlabel('Iteration', fontsize=12)\n",
        "    ax1.set_ylabel('Cumulative Reward', fontsize=12)\n",
        "    ax1.set_title('Cumulative Reward Over Time', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(loc='lower right')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Cumulative Regret\n",
        "    ax2 = fig.add_subplot(2, 2, 2)\n",
        "    for name, data in results.items():\n",
        "        regrets = [h[\"cumulative_regret\"] for h in data[\"history\"]]\n",
        "        ax2.plot(regrets, label=name, color=colors[name], linewidth=2)\n",
        "    ax2.set_xlabel('Iteration', fontsize=12)\n",
        "    ax2.set_ylabel('Cumulative Regret', fontsize=12)\n",
        "    ax2.set_title('Cumulative Regret Over Time', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(loc='upper left')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Moving Average Reward\n",
        "    ax3 = fig.add_subplot(2, 2, 3)\n",
        "    for name, data in results.items():\n",
        "        rewards = [h[\"reward\"] for h in data[\"history\"]]\n",
        "        ma = moving_average(rewards, window=50)\n",
        "        ax3.plot(ma, label=name, color=colors[name], linewidth=2)\n",
        "    ax3.set_xlabel('Iteration', fontsize=12)\n",
        "    ax3.set_ylabel('Reward (50-iter MA)', fontsize=12)\n",
        "    ax3.set_title('Average Reward (Smoothed)', fontsize=14, fontweight='bold')\n",
        "    ax3.legend(loc='lower right')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.axhline(y=0.85, color='black', linestyle='--', alpha=0.5, label='Optimal ~0.85')\n",
        "    \n",
        "    # 4. Final Performance Comparison\n",
        "    ax4 = fig.add_subplot(2, 2, 4)\n",
        "    names = list(results.keys())\n",
        "    final_rewards = [results[n][\"final_reward\"] for n in names]\n",
        "    bar_colors = [colors[n] for n in names]\n",
        "    bars = ax4.bar(names, final_rewards, color=bar_colors, edgecolor='black', linewidth=1.5)\n",
        "    ax4.set_ylabel('Final Avg Reward (last 100)', fontsize=12)\n",
        "    ax4.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\n",
        "    ax4.set_ylim(0, 1)\n",
        "    ax4.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, final_rewards):\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "fig = plot_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print learned policies and compare to ground truth\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LEARNED POLICIES vs GROUND TRUTH\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Ground truth optimal strategies\n",
        "ground_truth = {\n",
        "    query_type.value: max(\n",
        "        strategies.items(), \n",
        "        key=lambda x: x[1]\n",
        "    )[0].value\n",
        "    for query_type, strategies in ContextWindowSimulator.OPTIMAL_STRATEGIES.items()\n",
        "}\n",
        "\n",
        "print(\"\\nðŸ“Š Ground Truth Optimal Strategies:\")\n",
        "print(\"-\" * 40)\n",
        "for qt, strategy in ground_truth.items():\n",
        "    print(f\"  {qt:18s} â†’ {strategy}\")\n",
        "\n",
        "for algo_name in [\"Epsilon-Greedy\", \"Thompson Sampling\", \"LinUCB\"]:\n",
        "    policy = results[algo_name][\"learned_policy\"]\n",
        "    print(f\"\\nðŸ¤– {algo_name} Learned Policy:\")\n",
        "    print(\"-\" * 40)\n",
        "    correct = 0\n",
        "    for qt, info in policy.items():\n",
        "        learned = info[\"strategy\"]\n",
        "        is_correct = learned == ground_truth[qt]\n",
        "        correct += is_correct\n",
        "        symbol = \"âœ…\" if is_correct else \"âŒ\"\n",
        "        print(f\"  {qt:18s} â†’ {learned:20s} {symbol}\")\n",
        "    print(f\"  Accuracy: {correct}/{len(policy)} ({100*correct/len(policy):.0f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize strategy selection distribution by query type\n",
        "def plot_strategy_distribution(results, algo_name=\"Thompson Sampling\"):\n",
        "    \"\"\"Plot heatmap of strategy selections by query type\"\"\"\n",
        "    \n",
        "    history = results[algo_name][\"history\"]\n",
        "    \n",
        "    # Count strategy selections per query type\n",
        "    query_types = [qt.value for qt in QueryType]\n",
        "    strategies = [s.value for s in PrioritizationStrategy]\n",
        "    \n",
        "    counts = np.zeros((len(query_types), len(strategies)))\n",
        "    \n",
        "    for h in history:\n",
        "        qt_idx = query_types.index(h[\"query_type\"])\n",
        "        st_idx = strategies.index(h[\"strategy\"])\n",
        "        counts[qt_idx, st_idx] += 1\n",
        "    \n",
        "    # Normalize to percentages\n",
        "    row_sums = counts.sum(axis=1, keepdims=True)\n",
        "    percentages = np.where(row_sums > 0, counts / row_sums * 100, 0)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    im = ax.imshow(percentages, cmap='YlGnBu', aspect='auto')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = ax.figure.colorbar(im, ax=ax)\n",
        "    cbar.ax.set_ylabel('Selection %', rotation=-90, va=\"bottom\", fontsize=12)\n",
        "    \n",
        "    # Set ticks\n",
        "    ax.set_xticks(np.arange(len(strategies)))\n",
        "    ax.set_yticks(np.arange(len(query_types)))\n",
        "    ax.set_xticklabels([s.replace('_', '\\n') for s in strategies], fontsize=10)\n",
        "    ax.set_yticklabels(query_types, fontsize=11)\n",
        "    \n",
        "    # Rotate x labels\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    \n",
        "    # Add percentage labels\n",
        "    for i in range(len(query_types)):\n",
        "        for j in range(len(strategies)):\n",
        "            val = percentages[i, j]\n",
        "            color = \"white\" if val > 50 else \"black\"\n",
        "            ax.text(j, i, f'{val:.0f}%', ha=\"center\", va=\"center\", \n",
        "                   color=color, fontsize=9, fontweight='bold')\n",
        "    \n",
        "    ax.set_title(f'Strategy Selection Distribution by Query Type ({algo_name})', \n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_xlabel('Prioritization Strategy', fontsize=12)\n",
        "    ax.set_ylabel('Query Type', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "# Plot strategy distribution for best performing algorithm\n",
        "fig = plot_strategy_distribution(results, \"Thompson Sampling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Integration Example\n",
        "\n",
        "A production-ready class that can be integrated into real agentic systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "\n",
        "class ProductionContextOptimizer:\n",
        "    \"\"\"\n",
        "    Production-ready context window optimizer using contextual bandits.\n",
        "    \n",
        "    Features:\n",
        "    - Multiple algorithm support (Thompson Sampling recommended)\n",
        "    - Persistence (save/load learned models)\n",
        "    - Monitoring and logging\n",
        "    - Fallback strategies for cold start\n",
        "    - A/B testing support\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        algorithm: str = \"thompson_sampling\",\n",
        "        token_budget: int = 4000,\n",
        "        exploration_rate: float = 0.1,\n",
        "        model_path: Optional[str] = None\n",
        "    ):\n",
        "        self.token_budget = token_budget\n",
        "        self.algorithm = algorithm\n",
        "        \n",
        "        # Initialize bandit\n",
        "        n_arms = len(PrioritizationStrategy)\n",
        "        # Feature dim: query_type(5) + complexity(1) + num_items(1) + avg_relevance(1) \n",
        "        #              + budget(1) + history(1) + sources(6) = 16\n",
        "        self.context_dim = 16\n",
        "        \n",
        "        if algorithm == \"thompson_sampling\":\n",
        "            self.bandit = ThompsonSamplingLinear(n_arms, self.context_dim)\n",
        "        elif algorithm == \"linucb\":\n",
        "            self.bandit = LinUCB(n_arms, self.context_dim, alpha=0.5)\n",
        "        elif algorithm == \"epsilon_greedy\":\n",
        "            self.bandit = EpsilonGreedyLinear(n_arms, self.context_dim, epsilon=exploration_rate)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown algorithm: {algorithm}\")\n",
        "        \n",
        "        # Load existing model if provided\n",
        "        if model_path:\n",
        "            self.load_model(model_path)\n",
        "        \n",
        "        # Monitoring\n",
        "        self.request_count = 0\n",
        "        self.total_reward = 0\n",
        "        self.strategy_counts = defaultdict(int)\n",
        "        self.recent_rewards = []\n",
        "        \n",
        "    def prioritize_context(\n",
        "        self,\n",
        "        query_text: str,\n",
        "        query_type: str,\n",
        "        query_complexity: float,\n",
        "        available_items: List[Dict],\n",
        "        user_history_length: int = 0\n",
        "    ) -> Tuple[List[Dict], str, Dict]:\n",
        "        \"\"\"\n",
        "        Select and prioritize context items for an LLM call.\n",
        "        \n",
        "        Args:\n",
        "            query_text: The user's query\n",
        "            query_type: One of: factual, analytical, creative, troubleshooting, summarization\n",
        "            query_complexity: 0-1, estimated complexity\n",
        "            available_items: List of context items with keys: \n",
        "                            id, content, source, token_count, timestamp, relevance_score\n",
        "            user_history_length: Length of user's conversation history\n",
        "            \n",
        "        Returns:\n",
        "            (selected_items, strategy_used, metadata)\n",
        "        \"\"\"\n",
        "        self.request_count += 1\n",
        "        \n",
        "        # Convert to internal format\n",
        "        context_items = self._convert_items(available_items)\n",
        "        query_type_enum = QueryType(query_type)\n",
        "        \n",
        "        # Build context features\n",
        "        source_dist = defaultdict(int)\n",
        "        for item in context_items:\n",
        "            source_dist[item.source] += 1\n",
        "            \n",
        "        features = ContextFeatures(\n",
        "            query_type=query_type_enum,\n",
        "            query_complexity=query_complexity,\n",
        "            num_available_items=len(context_items),\n",
        "            avg_relevance=np.mean([i.relevance_score for i in context_items]) if context_items else 0,\n",
        "            token_budget=self.token_budget,\n",
        "            user_history_length=user_history_length,\n",
        "            source_distribution=dict(source_dist)\n",
        "        )\n",
        "        context_vector = features.to_vector()\n",
        "        \n",
        "        # Select strategy\n",
        "        arm_index = self.bandit.select_arm(context_vector)\n",
        "        strategy = self.bandit.get_strategy(arm_index)\n",
        "        self.strategy_counts[strategy.value] += 1\n",
        "        \n",
        "        # Create dummy query for prioritizer\n",
        "        query = Query(\n",
        "            text=query_text,\n",
        "            query_type=query_type_enum,\n",
        "            complexity=query_complexity,\n",
        "            user_id=\"user\",\n",
        "            embedding=np.zeros(64),\n",
        "            timestamp=datetime.now().timestamp()\n",
        "        )\n",
        "        \n",
        "        # Apply strategy\n",
        "        selected = ContextPrioritizer.select_items(\n",
        "            context_items, strategy, self.token_budget, query\n",
        "        )\n",
        "        \n",
        "        # Convert back to dict format\n",
        "        selected_dicts = [\n",
        "            {\n",
        "                \"id\": item.id,\n",
        "                \"content\": item.content,\n",
        "                \"source\": item.source.value,\n",
        "                \"token_count\": item.token_count,\n",
        "            }\n",
        "            for item in selected\n",
        "        ]\n",
        "        \n",
        "        metadata = {\n",
        "            \"strategy\": strategy.value,\n",
        "            \"items_selected\": len(selected),\n",
        "            \"tokens_used\": sum(i.token_count for i in selected),\n",
        "            \"context_vector\": context_vector.tolist(),\n",
        "            \"arm_index\": arm_index,\n",
        "        }\n",
        "        \n",
        "        return selected_dicts, strategy.value, metadata\n",
        "    \n",
        "    def record_feedback(\n",
        "        self,\n",
        "        metadata: Dict,\n",
        "        response_quality: float,\n",
        "        task_success: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Record feedback after LLM response to update the bandit.\n",
        "        \n",
        "        Args:\n",
        "            metadata: The metadata returned from prioritize_context\n",
        "            response_quality: 0-1, quality of the LLM response\n",
        "            task_success: Whether the task was completed successfully\n",
        "        \"\"\"\n",
        "        # Compute reward\n",
        "        efficiency = 1.0 - (metadata[\"tokens_used\"] / self.token_budget) * 0.1\n",
        "        reward = 0.7 * response_quality + 0.2 * float(task_success) + 0.1 * efficiency\n",
        "        reward = max(0, min(1, reward))\n",
        "        \n",
        "        # Update bandit\n",
        "        context_vector = np.array(metadata[\"context_vector\"])\n",
        "        arm_index = metadata[\"arm_index\"]\n",
        "        self.bandit.update(context_vector, arm_index, reward)\n",
        "        \n",
        "        # Track metrics\n",
        "        self.total_reward += reward\n",
        "        self.recent_rewards.append(reward)\n",
        "        if len(self.recent_rewards) > 100:\n",
        "            self.recent_rewards.pop(0)\n",
        "    \n",
        "    def _convert_items(self, items: List[Dict]) -> List[ContextItem]:\n",
        "        \"\"\"Convert dict items to ContextItem objects\"\"\"\n",
        "        converted = []\n",
        "        for i, item in enumerate(items):\n",
        "            source = item.get(\"source\", \"retrieved_docs\")\n",
        "            try:\n",
        "                source_enum = ContextSource(source)\n",
        "            except ValueError:\n",
        "                source_enum = ContextSource.RETRIEVED_DOCS\n",
        "                \n",
        "            converted.append(ContextItem(\n",
        "                id=item.get(\"id\", f\"item_{i}\"),\n",
        "                content=item.get(\"content\", \"\"),\n",
        "                source=source_enum,\n",
        "                token_count=item.get(\"token_count\", 100),\n",
        "                timestamp=item.get(\"timestamp\", datetime.now().timestamp()),\n",
        "                relevance_score=item.get(\"relevance_score\", 0.5),\n",
        "                topic_embedding=np.random.randn(32),\n",
        "                user_specific=item.get(\"user_specific\", False)\n",
        "            ))\n",
        "        return converted\n",
        "    \n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get current optimizer statistics\"\"\"\n",
        "        return {\n",
        "            \"total_requests\": self.request_count,\n",
        "            \"avg_reward\": self.total_reward / max(1, self.request_count),\n",
        "            \"recent_avg_reward\": np.mean(self.recent_rewards) if self.recent_rewards else 0,\n",
        "            \"strategy_distribution\": dict(self.strategy_counts),\n",
        "            \"algorithm\": self.algorithm,\n",
        "        }\n",
        "    \n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"Save the learned bandit model\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'bandit': self.bandit,\n",
        "                'stats': self.get_stats(),\n",
        "            }, f)\n",
        "        print(f\"Model saved to {path}\")\n",
        "    \n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"Load a previously saved model\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.bandit = data['bandit']\n",
        "        print(f\"Model loaded from {path}\")\n",
        "\n",
        "\n",
        "print(\"âœ… ProductionContextOptimizer defined!\")\n",
        "print(\"   Ready for integration with real agentic systems\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: Using the Production Optimizer\n",
        "print(\"=\" * 70)\n",
        "print(\"PRODUCTION OPTIMIZER DEMO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = ProductionContextOptimizer(\n",
        "    algorithm=\"thompson_sampling\",\n",
        "    token_budget=4000\n",
        ")\n",
        "\n",
        "# Simulate some context items (in practice, these come from your RAG system)\n",
        "sample_items = [\n",
        "    {\"id\": \"doc_1\", \"content\": \"API documentation for authentication...\", \n",
        "     \"source\": \"retrieved_docs\", \"token_count\": 350, \"relevance_score\": 0.9},\n",
        "    {\"id\": \"doc_2\", \"content\": \"User guide for setup...\", \n",
        "     \"source\": \"retrieved_docs\", \"token_count\": 280, \"relevance_score\": 0.7},\n",
        "    {\"id\": \"history_1\", \"content\": \"Previous conversation about login issues...\", \n",
        "     \"source\": \"conversation_history\", \"token_count\": 420, \"relevance_score\": 0.85},\n",
        "    {\"id\": \"user_pref\", \"content\": \"User preferences and settings...\", \n",
        "     \"source\": \"user_profile\", \"token_count\": 150, \"relevance_score\": 0.5},\n",
        "    {\"id\": \"system_1\", \"content\": \"Current system status: all services healthy...\", \n",
        "     \"source\": \"system_state\", \"token_count\": 200, \"relevance_score\": 0.4},\n",
        "    {\"id\": \"doc_3\", \"content\": \"Troubleshooting guide for common errors...\", \n",
        "     \"source\": \"retrieved_docs\", \"token_count\": 500, \"relevance_score\": 0.95},\n",
        "    {\"id\": \"api_1\", \"content\": \"Live API response with latest data...\", \n",
        "     \"source\": \"external_api\", \"token_count\": 300, \"relevance_score\": 0.6},\n",
        "    {\"id\": \"cache_1\", \"content\": \"Cached response from similar query...\", \n",
        "     \"source\": \"cached_results\", \"token_count\": 250, \"relevance_score\": 0.55},\n",
        "]\n",
        "\n",
        "# Process a query\n",
        "selected, strategy, metadata = optimizer.prioritize_context(\n",
        "    query_text=\"How do I fix the authentication error I'm seeing?\",\n",
        "    query_type=\"troubleshooting\",\n",
        "    query_complexity=0.6,\n",
        "    available_items=sample_items,\n",
        "    user_history_length=15\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“ Query: 'How do I fix the authentication error I'm seeing?'\")\n",
        "print(f\"ðŸ“Š Query type: troubleshooting\")\n",
        "print(f\"\\nðŸŽ¯ Selected Strategy: {strategy}\")\n",
        "print(f\"ðŸ“¦ Items selected: {metadata['items_selected']}\")\n",
        "print(f\"ðŸ”¢ Tokens used: {metadata['tokens_used']} / 4000\")\n",
        "\n",
        "print(\"\\nðŸ“‹ Selected context items:\")\n",
        "for item in selected:\n",
        "    print(f\"   - [{item['source']}] {item['id']}: {item['token_count']} tokens\")\n",
        "\n",
        "# Simulate feedback (in practice, this comes from user rating or task completion)\n",
        "optimizer.record_feedback(metadata, response_quality=0.85, task_success=True)\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Optimizer stats after 1 interaction:\")\n",
        "stats = optimizer.get_stats()\n",
        "for k, v in stats.items():\n",
        "    print(f\"   {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Key Takeaways\n",
        "\n",
        "### Why Contextual Bandits for Context Prioritization?\n",
        "\n",
        "| Aspect | Contextual Bandits | Deep RL (PPO/DQN) |\n",
        "|--------|-------------------|-------------------|\n",
        "| **Sample Efficiency** | âœ… Learns quickly | âŒ Needs many samples |\n",
        "| **Implementation** | âœ… Simple | âŒ Complex |\n",
        "| **Interpretability** | âœ… Clear strategy selection | âŒ Black box |\n",
        "| **Cold Start** | âœ… Works with heuristics | âŒ Requires pretraining |\n",
        "| **Real-time Updates** | âœ… Online learning | âŒ Batch updates |\n",
        "\n",
        "### Recommended Approach\n",
        "\n",
        "1. **Start with Thompson Sampling** - Best balance of exploration/exploitation\n",
        "2. **Use meaningful features** - Query type, complexity, source distribution\n",
        "3. **Design good strategies** - Each arm should be a distinct prioritization approach\n",
        "4. **Collect quality feedback** - User ratings, task success, or automated metrics\n",
        "\n",
        "### Integration Checklist\n",
        "\n",
        "- [ ] Define your context sources (docs, history, user profile, etc.)\n",
        "- [ ] Implement feature extraction for your queries\n",
        "- [ ] Create prioritization strategies tailored to your use case\n",
        "- [ ] Set up feedback collection (explicit ratings or implicit signals)\n",
        "- [ ] Deploy with A/B testing against baseline\n",
        "- [ ] Monitor and iterate\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "For more complex scenarios, consider:\n",
        "- **Hierarchical bandits** for multi-level decisions\n",
        "- **Neural contextual bandits** for richer feature representations\n",
        "- **Full RL (DQN/PPO)** for sequential selection with complex dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"=\" * 70)\n",
        "print(\"NOTEBOOK COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\"\"\n",
        "This notebook demonstrated:\n",
        "\n",
        "ðŸ“š CONCEPTS\n",
        "   â€¢ Context window prioritization as a bandit problem\n",
        "   â€¢ Mapping prioritization strategies to bandit arms\n",
        "   â€¢ Feature engineering for query context\n",
        "\n",
        "ðŸ”§ IMPLEMENTATIONS\n",
        "   â€¢ Epsilon-Greedy with linear models\n",
        "   â€¢ Thompson Sampling (Bayesian approach)\n",
        "   â€¢ LinUCB (optimistic exploration)\n",
        "   â€¢ 6 different prioritization strategies\n",
        "\n",
        "ðŸ“Š RESULTS\n",
        "   â€¢ Thompson Sampling typically performs best\n",
        "   â€¢ Learns query-type specific policies\n",
        "   â€¢ Low regret compared to random baseline\n",
        "\n",
        "ðŸš€ PRODUCTION\n",
        "   â€¢ ProductionContextOptimizer class ready for integration\n",
        "   â€¢ Save/load trained models\n",
        "   â€¢ Real-time feedback and learning\n",
        "\n",
        "Next: Try integrating with your RAG pipeline!\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
