{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agentic Workflow with Reinforcement Learning Loop - Collaboration Boost Edition\n",
        "\n",
        "This notebook demonstrates a sophisticated multi-agent system where agents learn optimal behaviors through reinforcement learning. **This enhanced version** includes proportional collaboration based on the collaboration matrix.\n",
        "\n",
        "## Key Enhancements in This Version\n",
        "\n",
        "1. **Proportional Collaboration Boost**: Task completion probability scales with collaboration matrix values\n",
        "2. **Quality Boost from Collaboration**: Task quality improves when strong collaborators work together\n",
        "3. **Scaled Collaboration Rewards**: Rewards scale with collaboration strength, not flat bonuses\n",
        "4. **Expertise Sharing**: Agents can \"borrow\" skills from collaborators proportional to their history\n",
        "5. **Collaboration Decay**: Unused collaboration paths slowly decay back to neutral\n",
        "\n",
        "The architecture combines:\n",
        "- **Multi-Agent Coordination**: Agents with specialized roles\n",
        "- **Reinforcement Learning**: PPO-based policy optimization\n",
        "- **Tool Integration**: Agents can use external tools/APIs\n",
        "- **Adaptive Workflow**: Learning-based task allocation\n",
        "- **Production Patterns**: Monitoring, logging, and error handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install stable-baselines3 gymnasium numpy pandas matplotlib seaborn\n",
        "!pip install langchain langchain-openai langgraph tensorboard\n",
        "!pip install ray[rllib] wandb mlflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Multi-Agent Environment Definition with Collaboration Boost\n",
        "\n",
        "Define a custom environment where multiple agents collaborate on tasks, learning optimal policies through RL. **This version includes proportional collaboration based on the collaboration matrix.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgentRole(Enum):\n",
        "    \"\"\"Defines specialized agent roles in the system\"\"\"\n",
        "    RESEARCHER = \"researcher\"\n",
        "    ANALYZER = \"analyzer\"\n",
        "    EXECUTOR = \"executor\"\n",
        "    VALIDATOR = \"validator\"\n",
        "    COORDINATOR = \"coordinator\"\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    \"\"\"Represents a task in the workflow\"\"\"\n",
        "    id: str\n",
        "    type: str\n",
        "    complexity: float\n",
        "    requirements: List[str]\n",
        "    deadline: float\n",
        "    priority: float\n",
        "    status: str = \"pending\"\n",
        "    assigned_agent: Optional[str] = None\n",
        "    completion_time: Optional[float] = None\n",
        "    quality_score: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class AgentState:\n",
        "    \"\"\"Tracks individual agent state\"\"\"\n",
        "    id: str\n",
        "    role: AgentRole\n",
        "    capacity: float\n",
        "    expertise: Dict[str, float]\n",
        "    current_load: float = 0.0\n",
        "    completed_tasks: int = 0\n",
        "    success_rate: float = 1.0\n",
        "    collaboration_score: float = 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiAgentTaskEnvironmentCollabBoost(gym.Env):\n",
        "    \"\"\"\n",
        "    Multi-agent environment for collaborative task execution with RL.\n",
        "    \n",
        "    ENHANCED VERSION with Proportional Collaboration:\n",
        "    - Task completion probability scales with collaboration matrix values\n",
        "    - Quality scores boosted by collaboration strength\n",
        "    - Rewards scale with collaboration history\n",
        "    - Expertise sharing between collaborating agents\n",
        "    - Collaboration decay for unused partnerships\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_agents: int = 4, max_tasks: int = 10):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.max_tasks = max_tasks\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 200\n",
        "        \n",
        "        # Collaboration parameters\n",
        "        self.collab_growth_rate = 1.02      # How fast collaboration strengthens\n",
        "        self.collab_decay_rate = 0.998      # How fast unused collaborations decay\n",
        "        self.collab_reward_scale = 0.5      # Base collaboration reward multiplier\n",
        "        self.expertise_share_factor = 0.3   # How much expertise can be shared\n",
        "\n",
        "        # Initialize agents with different roles\n",
        "        self.agents = self._initialize_agents()\n",
        "\n",
        "        # Task queue and completed tasks\n",
        "        self.task_queue = deque()\n",
        "        self.active_tasks = {}\n",
        "        self.completed_tasks = []\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        # Observation: [agent_states, task_queue_state, collaboration_matrix]\n",
        "        obs_dim = n_agents * 7 + max_tasks * 5 + n_agents * n_agents\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Action: [task_assignment, resource_allocation, collaboration_request]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0, high=1, shape=(n_agents * 3,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.episode_rewards = []\n",
        "        self.task_completion_times = []\n",
        "        self.collaboration_matrix = np.ones((n_agents, n_agents))\n",
        "        \n",
        "        # Track collaboration events for analysis\n",
        "        self.collaboration_history = []\n",
        "\n",
        "    def _initialize_agents(self) -> List[AgentState]:\n",
        "        \"\"\"Initialize agents with diverse roles and capabilities\"\"\"\n",
        "        agents = []\n",
        "        roles = list(AgentRole)[:self.n_agents]\n",
        "\n",
        "        for i, role in enumerate(roles):\n",
        "            expertise = {\n",
        "                \"research\": np.random.uniform(0.5, 1.0),\n",
        "                \"analysis\": np.random.uniform(0.5, 1.0),\n",
        "                \"execution\": np.random.uniform(0.5, 1.0),\n",
        "                \"validation\": np.random.uniform(0.5, 1.0)\n",
        "            }\n",
        "\n",
        "            # Boost expertise based on role\n",
        "            if role == AgentRole.RESEARCHER:\n",
        "                expertise[\"research\"] = min(1.0, expertise[\"research\"] + 0.3)\n",
        "            elif role == AgentRole.ANALYZER:\n",
        "                expertise[\"analysis\"] = min(1.0, expertise[\"analysis\"] + 0.3)\n",
        "\n",
        "            agents.append(AgentState(\n",
        "                id=f\"agent_{i}\",\n",
        "                role=role,\n",
        "                capacity=np.random.uniform(0.8, 1.0),\n",
        "                expertise=expertise\n",
        "            ))\n",
        "\n",
        "        return agents\n",
        "\n",
        "    def _generate_task(self) -> Task:\n",
        "        \"\"\"Generate a new task with random properties\"\"\"\n",
        "        task_types = [\"research\", \"analysis\", \"execution\", \"validation\"]\n",
        "        task_type = np.random.choice(task_types)\n",
        "\n",
        "        return Task(\n",
        "            id=f\"task_{np.random.randint(10000)}\",\n",
        "            type=task_type,\n",
        "            complexity=np.random.uniform(0.3, 1.0),\n",
        "            requirements=[np.random.choice(task_types) for _ in range(np.random.randint(1, 3))],\n",
        "            deadline=np.random.uniform(10, 50),\n",
        "            priority=np.random.uniform(0.1, 1.0)\n",
        "        )\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Construct observation vector from current state\"\"\"\n",
        "        obs = []\n",
        "\n",
        "        # Agent states\n",
        "        for agent in self.agents:\n",
        "            obs.extend([\n",
        "                agent.capacity,\n",
        "                agent.current_load,\n",
        "                agent.completed_tasks / max(1, self.current_step),\n",
        "                agent.success_rate,\n",
        "                agent.collaboration_score,\n",
        "                agent.expertise.get(\"research\", 0),\n",
        "                agent.expertise.get(\"analysis\", 0)\n",
        "            ])\n",
        "\n",
        "        # Task queue state\n",
        "        for i in range(self.max_tasks):\n",
        "            if i < len(self.task_queue):\n",
        "                task = list(self.task_queue)[i]\n",
        "                obs.extend([\n",
        "                    task.complexity,\n",
        "                    task.priority,\n",
        "                    task.deadline,\n",
        "                    1.0,  # task exists\n",
        "                    0.0   # not yet assigned\n",
        "                ])\n",
        "            else:\n",
        "                obs.extend([0, 0, 0, 0, 0])\n",
        "\n",
        "        # Collaboration matrix (flattened)\n",
        "        obs.extend(self.collaboration_matrix.flatten())\n",
        "\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "    \n",
        "    def _get_collaborating_agents(self, action: np.ndarray) -> set:\n",
        "        \"\"\"Identify which agents are signaling collaboration this step\"\"\"\n",
        "        collaborating = set()\n",
        "        for i in range(self.n_agents):\n",
        "            if action[i, 2] > 0.7:  # Collaboration threshold\n",
        "                collaborating.add(i)\n",
        "        return collaborating\n",
        "    \n",
        "    def _calculate_collaboration_boost(self, agent_idx: int, task_type: str, \n",
        "                                        collaborating_agents: set) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the collaboration boost for an agent based on:\n",
        "        - Which other agents are collaborating\n",
        "        - The collaboration matrix values (history)\n",
        "        - The expertise of collaborating agents\n",
        "        \"\"\"\n",
        "        if agent_idx not in collaborating_agents:\n",
        "            return 1.0  # No boost if not collaborating\n",
        "        \n",
        "        boost = 1.0\n",
        "        for other_idx in collaborating_agents:\n",
        "            if other_idx != agent_idx:\n",
        "                # Get collaboration strength from matrix\n",
        "                collab_strength = self.collaboration_matrix[agent_idx, other_idx]\n",
        "                \n",
        "                # Get other agent's expertise in the task type\n",
        "                other_agent = self.agents[other_idx]\n",
        "                other_expertise = other_agent.expertise.get(task_type, 0.5)\n",
        "                \n",
        "                # Calculate contribution: stronger history + better expertise = more boost\n",
        "                # The boost is proportional to how much the matrix has grown above 1.0\n",
        "                contribution = (collab_strength - 1.0) * other_expertise * self.expertise_share_factor\n",
        "                boost += max(0, contribution)\n",
        "        \n",
        "        return boost\n",
        "    \n",
        "    def _get_best_collaborator(self, agent_idx: int, task_type: str) -> Optional[int]:\n",
        "        \"\"\"Find the best potential collaborator for an agent based on matrix and expertise\"\"\"\n",
        "        best_score = 0\n",
        "        best_idx = None\n",
        "        \n",
        "        for j in range(self.n_agents):\n",
        "            if j != agent_idx:\n",
        "                collab_strength = self.collaboration_matrix[agent_idx, j]\n",
        "                expertise = self.agents[j].expertise.get(task_type, 0.5)\n",
        "                availability = 1.0 - self.agents[j].current_load\n",
        "                \n",
        "                # Score combines collaboration history, expertise, and availability\n",
        "                score = collab_strength * expertise * availability\n",
        "                \n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_idx = j\n",
        "        \n",
        "        return best_idx if best_score > 1.0 else None\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
        "        \"\"\"Execute action and return new state with COLLABORATION BOOST\"\"\"\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Parse actions\n",
        "        action = action.reshape(self.n_agents, 3)\n",
        "        \n",
        "        # Identify collaborating agents\n",
        "        collaborating_agents = self._get_collaborating_agents(action)\n",
        "\n",
        "        # Process task assignments\n",
        "        reward = 0\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            if len(self.task_queue) > 0 and action[i, 0] > 0.5:\n",
        "                task = self.task_queue.popleft()\n",
        "                self._assign_task(agent, task)\n",
        "\n",
        "                # Calculate immediate reward for assignment\n",
        "                match_score = agent.expertise.get(task.type, 0.5)\n",
        "                reward += match_score * task.priority\n",
        "\n",
        "        # Process active tasks with COLLABORATION BOOST\n",
        "        completed_this_step = []\n",
        "        for task_id, (task, agent) in list(self.active_tasks.items()):\n",
        "            agent_idx = int(agent.id.split(\"_\")[1])\n",
        "            \n",
        "            # Base progress from agent's own capabilities\n",
        "            base_progress = agent.expertise.get(task.type, 0.5) * agent.capacity\n",
        "            \n",
        "            # Calculate collaboration boost\n",
        "            collab_boost = self._calculate_collaboration_boost(\n",
        "                agent_idx, task.type, collaborating_agents\n",
        "            )\n",
        "            \n",
        "            # Apply boost to progress probability\n",
        "            effective_progress = min(0.95, base_progress * collab_boost)\n",
        "\n",
        "            # Check if task completed (probability boosted by collaboration)\n",
        "            if np.random.random() < effective_progress:\n",
        "                self._complete_task(task, agent, collab_boost)\n",
        "                completed_this_step.append(task)\n",
        "\n",
        "                # Calculate completion reward (quality includes collab boost)\n",
        "                time_bonus = max(0, 1 - (self.current_step / task.deadline))\n",
        "                quality = task.quality_score\n",
        "                reward += (quality * task.priority * (1 + time_bonus)) * 10\n",
        "\n",
        "        # Update collaboration matrix with PROPORTIONAL rewards\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(self.n_agents):\n",
        "                if i != j and action[i, 2] > 0.7 and action[j, 2] > 0.7:\n",
        "                    # Grow the collaboration strength\n",
        "                    self.collaboration_matrix[i, j] *= self.collab_growth_rate\n",
        "                    \n",
        "                    # Cap collaboration strength\n",
        "                    self.collaboration_matrix[i, j] = min(3.0, self.collaboration_matrix[i, j])\n",
        "                    \n",
        "                    # Reward SCALES with collaboration strength (not flat!)\n",
        "                    collab_reward = self.collab_reward_scale * self.collaboration_matrix[i, j]\n",
        "                    reward += collab_reward\n",
        "                    \n",
        "                    # Record collaboration event\n",
        "                    self.collaboration_history.append({\n",
        "                        'step': self.current_step,\n",
        "                        'agents': (i, j),\n",
        "                        'strength': self.collaboration_matrix[i, j]\n",
        "                    })\n",
        "        \n",
        "        # Apply collaboration DECAY for unused partnerships\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(self.n_agents):\n",
        "                if i != j:\n",
        "                    # Only decay if they didn't collaborate this step\n",
        "                    if not (action[i, 2] > 0.7 and action[j, 2] > 0.7):\n",
        "                        # Decay towards 1.0 (neutral)\n",
        "                        current = self.collaboration_matrix[i, j]\n",
        "                        self.collaboration_matrix[i, j] = (\n",
        "                            1.0 + (current - 1.0) * self.collab_decay_rate\n",
        "                        )\n",
        "\n",
        "        # Generate new tasks\n",
        "        if np.random.random() < 0.3:\n",
        "            self.task_queue.append(self._generate_task())\n",
        "\n",
        "        # Calculate penalties\n",
        "        queue_penalty = len(self.task_queue) * 0.1\n",
        "        overdue_penalty = sum(1 for t in self.active_tasks.values()\n",
        "                            if self.current_step > t[0].deadline) * 0.5\n",
        "        reward -= (queue_penalty + overdue_penalty)\n",
        "\n",
        "        # Check termination\n",
        "        done = self.current_step >= self.max_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Prepare info with collaboration metrics\n",
        "        info = {\n",
        "            \"completed_tasks\": len(completed_this_step),\n",
        "            \"queue_length\": len(self.task_queue),\n",
        "            \"active_tasks\": len(self.active_tasks),\n",
        "            \"avg_success_rate\": np.mean([a.success_rate for a in self.agents]),\n",
        "            \"avg_collab_strength\": np.mean(self.collaboration_matrix),\n",
        "            \"max_collab_strength\": np.max(self.collaboration_matrix),\n",
        "            \"n_collaborating\": len(collaborating_agents)\n",
        "        }\n",
        "\n",
        "        return self._get_observation(), reward, done, truncated, info\n",
        "\n",
        "    def _assign_task(self, agent: AgentState, task: Task):\n",
        "        \"\"\"Assign a task to an agent\"\"\"\n",
        "        task.assigned_agent = agent.id\n",
        "        task.status = \"active\"\n",
        "        self.active_tasks[task.id] = (task, agent)\n",
        "        agent.current_load += task.complexity\n",
        "\n",
        "    def _complete_task(self, task: Task, agent: AgentState, collab_boost: float = 1.0):\n",
        "        \"\"\"Mark a task as completed with collaboration quality bonus\"\"\"\n",
        "        task.status = \"completed\"\n",
        "        task.completion_time = self.current_step\n",
        "        \n",
        "        # Quality score boosted by collaboration\n",
        "        base_quality = agent.expertise.get(task.type, 0.5) * agent.success_rate\n",
        "        task.quality_score = min(1.0, base_quality * collab_boost)  # Cap at 1.0\n",
        "\n",
        "        self.completed_tasks.append(task)\n",
        "        del self.active_tasks[task.id]\n",
        "\n",
        "        agent.current_load = max(0, agent.current_load - task.complexity)\n",
        "        agent.completed_tasks += 1\n",
        "        agent.success_rate = 0.95 * agent.success_rate + 0.05 * task.quality_score\n",
        "        \n",
        "        # Boost collaboration score for agents who benefited from collaboration\n",
        "        if collab_boost > 1.0:\n",
        "            agent.collaboration_score = min(2.0, agent.collaboration_score * (1 + 0.02 * (collab_boost - 1)))\n",
        "\n",
        "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.agents = self._initialize_agents()\n",
        "        self.task_queue = deque([self._generate_task() for _ in range(3)])\n",
        "        self.active_tasks = {}\n",
        "        self.completed_tasks = []\n",
        "        self.collaboration_matrix = np.ones((self.n_agents, self.n_agents))\n",
        "        self.collaboration_history = []\n",
        "\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render current environment state with collaboration info\"\"\"\n",
        "        print(f\"\\nStep: {self.current_step}\")\n",
        "        print(f\"Tasks in queue: {len(self.task_queue)}\")\n",
        "        print(f\"Active tasks: {len(self.active_tasks)}\")\n",
        "        print(f\"Completed tasks: {len(self.completed_tasks)}\")\n",
        "        print(f\"\\nCollaboration Matrix (strength):\")\n",
        "        print(np.round(self.collaboration_matrix, 2))\n",
        "        print(f\"\\nAgents:\")\n",
        "        for agent in self.agents:\n",
        "            print(f\"  {agent.id} ({agent.role.value}): Load={agent.current_load:.2f}, \"\n",
        "                  f\"Completed={agent.completed_tasks}, Success={agent.success_rate:.2f}, \"\n",
        "                  f\"CollabScore={agent.collaboration_score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Agentic Learning System with PPO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AgenticRLSystemCollabBoost:\n",
        "    \"\"\"Orchestrates multi-agent reinforcement learning workflow with collaboration boost\"\"\"\n",
        "\n",
        "    def __init__(self, env_class, n_envs: int = 4):\n",
        "        self.env_class = env_class\n",
        "        self.n_envs = n_envs\n",
        "\n",
        "        # Create vectorized environment for parallel training\n",
        "        self.env = DummyVecEnv([lambda: env_class() for _ in range(n_envs)])\n",
        "        self.eval_env = DummyVecEnv([lambda: env_class()])\n",
        "\n",
        "        # Initialize PPO model with custom network architecture\n",
        "        self.model = self._create_model()\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.training_history = {\n",
        "            \"episode_rewards\": [],\n",
        "            \"episode_lengths\": [],\n",
        "            \"success_rates\": [],\n",
        "            \"collaboration_scores\": [],\n",
        "            \"avg_collab_strength\": [],\n",
        "            \"max_collab_strength\": []\n",
        "        }\n",
        "\n",
        "        # Setup callbacks\n",
        "        self.callbacks = self._setup_callbacks()\n",
        "\n",
        "    def _create_model(self) -> PPO:\n",
        "        \"\"\"Create PPO model with custom architecture\"\"\"\n",
        "        policy_kwargs = dict(\n",
        "            net_arch=[\n",
        "                dict(pi=[256, 256, 128], vf=[256, 256, 128])\n",
        "            ],\n",
        "            activation_fn=nn.ReLU\n",
        "        )\n",
        "\n",
        "        model = PPO(\n",
        "            \"MlpPolicy\",\n",
        "            self.env,\n",
        "            learning_rate=3e-4,\n",
        "            n_steps=2048,\n",
        "            batch_size=64,\n",
        "            n_epochs=10,\n",
        "            gamma=0.99,\n",
        "            gae_lambda=0.95,\n",
        "            clip_range=0.2,\n",
        "            clip_range_vf=None,\n",
        "            ent_coef=0.01,\n",
        "            vf_coef=0.5,\n",
        "            max_grad_norm=0.5,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=1,\n",
        "            tensorboard_log=\"./tensorboard_logs_collab_boost/\"\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _setup_callbacks(self):\n",
        "        \"\"\"Setup training callbacks for monitoring and checkpointing\"\"\"\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=\"./models/best_model_collab_boost/\",\n",
        "            log_path=\"./logs_collab_boost/\",\n",
        "            eval_freq=5000,\n",
        "            deterministic=True,\n",
        "            render=False,\n",
        "            n_eval_episodes=10\n",
        "        )\n",
        "\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=10000,\n",
        "            save_path=\"./models/checkpoints_collab_boost/\",\n",
        "            name_prefix=\"agentic_rl_collab_boost\"\n",
        "        )\n",
        "\n",
        "        return [eval_callback, checkpoint_callback]\n",
        "\n",
        "    def train(self, total_timesteps: int = 100000):\n",
        "        \"\"\"Train the multi-agent system\"\"\"\n",
        "        logger.info(f\"Starting training for {total_timesteps} timesteps\")\n",
        "\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=self.callbacks,\n",
        "            log_interval=10,\n",
        "            progress_bar=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training completed\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, n_episodes: int = 10) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate the trained model with collaboration metrics\"\"\"\n",
        "        env = self.env_class()\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        task_completion_rates = []\n",
        "        collab_strengths = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "\n",
        "            while not done:\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                obs, reward, done, truncated, info = env.step(action)\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                # Track collaboration metrics\n",
        "                collab_strengths.append(info.get('avg_collab_strength', 1.0))\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "\n",
        "            if len(env.completed_tasks) > 0:\n",
        "                completion_rate = len(env.completed_tasks) / (len(env.completed_tasks) + len(env.task_queue))\n",
        "                task_completion_rates.append(completion_rate)\n",
        "\n",
        "        metrics = {\n",
        "            \"mean_reward\": np.mean(episode_rewards),\n",
        "            \"std_reward\": np.std(episode_rewards),\n",
        "            \"mean_episode_length\": np.mean(episode_lengths),\n",
        "            \"task_completion_rate\": np.mean(task_completion_rates) if task_completion_rates else 0,\n",
        "            \"avg_collaboration_strength\": np.mean(collab_strengths),\n",
        "            \"max_collaboration_strength\": np.max(collab_strengths)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"Save the trained model\"\"\"\n",
        "        self.model.save(path)\n",
        "        logger.info(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"Load a pre-trained model\"\"\"\n",
        "        self.model = PPO.load(path, env=self.env)\n",
        "        logger.info(f\"Model loaded from {path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training and Evaluation Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_agentic_rl_system_collab_boost():\n",
        "    \"\"\"Main training pipeline for the agentic RL system with collaboration boost\"\"\"\n",
        "\n",
        "    # Initialize environment and system\n",
        "    print(\"Initializing Agentic RL System with Collaboration Boost...\")\n",
        "    system = AgenticRLSystemCollabBoost(MultiAgentTaskEnvironmentCollabBoost, n_envs=4)\n",
        "\n",
        "    # Training configuration\n",
        "    training_config = {\n",
        "        \"total_timesteps\": 50000,\n",
        "        \"eval_frequency\": 5000,\n",
        "        \"n_eval_episodes\": 10\n",
        "    }\n",
        "\n",
        "    print(f\"\\nStarting training with {training_config['total_timesteps']} timesteps...\")\n",
        "\n",
        "    # Train the system\n",
        "    trained_model = system.train(total_timesteps=training_config['total_timesteps'])\n",
        "\n",
        "    # Evaluate performance\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    metrics = system.evaluate(n_episodes=training_config['n_eval_episodes'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Complete - Performance Metrics:\")\n",
        "    print(\"=\"*50)\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value:.3f}\")\n",
        "\n",
        "    # Save the model\n",
        "    system.save_model(\"./models/trained_agentic_rl_collab_boost\")\n",
        "\n",
        "    return system, metrics\n",
        "\n",
        "# Train the system\n",
        "# Uncomment to run training (will take some time)\n",
        "# system, metrics = train_agentic_rl_system_collab_boost()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Collaboration Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_collaboration_matrix(env):\n",
        "    \"\"\"Visualize the collaboration matrix as a heatmap\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Heatmap of collaboration matrix\n",
        "    sns.heatmap(env.collaboration_matrix, annot=True, fmt='.2f', \n",
        "                cmap='YlOrRd', ax=axes[0],\n",
        "                xticklabels=[f'Agent {i}' for i in range(env.n_agents)],\n",
        "                yticklabels=[f'Agent {i}' for i in range(env.n_agents)])\n",
        "    axes[0].set_title('Collaboration Matrix Strength')\n",
        "    axes[0].set_xlabel('Agent')\n",
        "    axes[0].set_ylabel('Agent')\n",
        "    \n",
        "    # Bar plot of collaboration history over time\n",
        "    if env.collaboration_history:\n",
        "        steps = [h['step'] for h in env.collaboration_history]\n",
        "        strengths = [h['strength'] for h in env.collaboration_history]\n",
        "        axes[1].scatter(steps, strengths, alpha=0.5, c='blue')\n",
        "        axes[1].set_xlabel('Step')\n",
        "        axes[1].set_ylabel('Collaboration Strength')\n",
        "        axes[1].set_title('Collaboration Events Over Time')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "    else:\n",
        "        axes[1].text(0.5, 0.5, 'No collaboration history yet', \n",
        "                     ha='center', va='center', transform=axes[1].transAxes)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def demo_collaboration_boost():\n",
        "    \"\"\"Demo the collaboration boost environment\"\"\"\n",
        "    env = MultiAgentTaskEnvironmentCollabBoost(n_agents=4)\n",
        "    obs, _ = env.reset()\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"COLLABORATION BOOST DEMO\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    total_reward = 0\n",
        "    for step in range(50):\n",
        "        # Random action with high collaboration signals\n",
        "        action = np.random.uniform(0, 1, size=(env.n_agents * 3,))\n",
        "        # Force some agents to collaborate\n",
        "        action[2] = 0.9  # Agent 0 collaborates\n",
        "        action[5] = 0.9  # Agent 1 collaborates\n",
        "        \n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        \n",
        "        if step % 10 == 0:\n",
        "            print(f\"\\nStep {step}:\")\n",
        "            print(f\"  Reward: {reward:.2f}, Total: {total_reward:.2f}\")\n",
        "            print(f\"  Avg Collab Strength: {info['avg_collab_strength']:.3f}\")\n",
        "            print(f\"  Max Collab Strength: {info['max_collab_strength']:.3f}\")\n",
        "            print(f\"  Completed Tasks: {info['completed_tasks']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL STATE\")\n",
        "    print(\"=\"*60)\n",
        "    env.render()\n",
        "    \n",
        "    # Visualize collaboration\n",
        "    visualize_collaboration_matrix(env)\n",
        "    \n",
        "    return env\n",
        "\n",
        "# Run demo\n",
        "demo_env = demo_collaboration_boost()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary - Collaboration Boost Enhancements\n",
        "\n",
        "This notebook extends the original agentic RL workflow with **proportional collaboration** based on the collaboration matrix.\n",
        "\n",
        "### Key Enhancements\n",
        "\n",
        "| Feature | Original | Collaboration Boost Version |\n",
        "|---------|----------|----------------------------|\n",
        "| **Task Progress** | Fixed probability based on expertise | Probability scales with collaboration matrix values |\n",
        "| **Quality Score** | Based only on agent expertise | Boosted by collaboration strength |\n",
        "| **Collaboration Reward** | Flat 0.5 bonus | Scales with matrix value: `0.5 * collab_strength` |\n",
        "| **Expertise Sharing** | None | Agents share skills proportional to collaboration history |\n",
        "| **Matrix Dynamics** | Only grows | Grows when used, **decays** when unused |\n",
        "\n",
        "### How Collaboration Boost Works\n",
        "\n",
        "```\n",
        "collaboration_boost = 1.0 + Î£ (collab_matrix[i,j] - 1.0) * other_expertise * share_factor\n",
        "                           for all collaborating agents j\n",
        "\n",
        "effective_progress = base_progress * collaboration_boost\n",
        "task_quality = base_quality * collaboration_boost\n",
        "```\n",
        "\n",
        "### Configurable Parameters\n",
        "\n",
        "- `collab_growth_rate` (default: 1.02): How fast collaboration strengthens per step\n",
        "- `collab_decay_rate` (default: 0.998): How fast unused collaborations decay\n",
        "- `collab_reward_scale` (default: 0.5): Base multiplier for collaboration rewards\n",
        "- `expertise_share_factor` (default: 0.3): How much expertise can be shared between collaborators\n",
        "\n",
        "### Expected Emergent Behaviors\n",
        "\n",
        "1. **Team Formation**: Agents learn to form consistent pairs/teams\n",
        "2. **Specialization**: Complementary agents collaborate more (researcher + analyzer)\n",
        "3. **Strategic Collaboration**: Agents learn WHEN to collaborate (complex tasks) vs solo work\n",
        "4. **Relationship Maintenance**: Agents maintain valuable partnerships to prevent decay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
