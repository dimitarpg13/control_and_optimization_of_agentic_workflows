{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Multi-Agent Task Allocation with RL + Load Balancing â€” Real LLM Calls\n",
    "\n",
    "This notebook demonstrates a **multi-agent system** where agents with specialized roles process tasks, with **Reinforcement Learning (Q-Learning)** guiding optimal task allocation **and load balancing** â€” all backed by **real LLM API calls** via **LangChain/LangGraph**.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       MULTI-AGENT RL TASK ALLOCATION + LOAD BALANCING (Real LLM Calls)       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚ INCOMING â”‚â”€â”€â”€â–ºâ”‚  RL COORDINATOR   â”‚â”€â”€â”€â–ºâ”‚  AGENT POOL      â”‚                â”‚\n",
    "â”‚  â”‚   TASK   â”‚    â”‚  (Q-Learning)     â”‚    â”‚                  â”‚                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                   â”‚    â”‚ ðŸ”¬ Researcher    â”‚                â”‚\n",
    "â”‚                 â”‚ State:            â”‚    â”‚ ðŸ“Š Analyst       â”‚                â”‚\n",
    "â”‚                 â”‚  â€¢ task type      â”‚    â”‚ ðŸ’» Coder         â”‚                â”‚\n",
    "â”‚                 â”‚  â€¢ load state     â”‚    â”‚ âœ… Validator     â”‚                â”‚\n",
    "â”‚                 â”‚ Action: assign    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                 â”‚ Reward:           â”‚             â”‚                          â”‚\n",
    "â”‚                 â”‚  â€¢ quality        â”‚             â”‚  Real LLM Calls          â”‚\n",
    "â”‚                 â”‚  âˆ’ load penalty   â”‚             â”‚  (OpenAI / Anthropic)    â”‚\n",
    "â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚                          â”‚\n",
    "â”‚                          â”‚    LangGraph            â”‚                          â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    Orchestration        â”‚                          â”‚\n",
    "â”‚         â”‚                â”‚                        â–¼                          â”‚\n",
    "â”‚         â–¼                â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚              â”‚    AGENT OUTPUT   â”‚                â”‚\n",
    "â”‚  â”‚ LOAD TRACKER â”‚        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚  â”‚              â”‚        â”‚                       â”‚                          â”‚\n",
    "â”‚  â”‚ â€¢ Sliding    â”‚        â–¼                       â”‚                          â”‚\n",
    "â”‚  â”‚   window     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚                          â”‚\n",
    "â”‚  â”‚ â€¢ Per-agent  â”‚  â”‚  LLM-AS-JUDGE    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚  â”‚   utilizationâ”‚  â”‚                  â”‚                                      â”‚\n",
    "â”‚  â”‚ â€¢ Hot-agent  â”‚  â”‚ â€¢ Quality Score  â”‚                                      â”‚\n",
    "â”‚  â”‚   detection  â”‚  â”‚ â€¢ Relevance      â”‚                                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Completeness   â”‚                                      â”‚\n",
    "â”‚                    â”‚ â€¢ Agent Match    â”‚                                      â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\n",
    "â”‚                             â”‚                                                â”‚\n",
    "â”‚                             â–¼                                                â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚                    â”‚  RL UPDATE: quality reward âˆ’ load penalty   â”‚            â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key Differences from the Base Q-Learning Version (without Load Balancing)\n",
    "\n",
    "| Aspect | Base Q-Learning | This Notebook (Load-Balanced) |\n",
    "|--------|----------------|-------------------------------|\n",
    "| **RL State** | Task type only (4 states) | Task type Ã— load state (20 states) |\n",
    "| **Load Tracking** | None | Sliding-window per-agent utilization |\n",
    "| **Reward Signal** | Quality âˆ’ cost âˆ’ latency | Quality âˆ’ cost âˆ’ latency âˆ’ **load penalty** |\n",
    "| **Q-Table Size** | 32 cells | 160 cells |\n",
    "| **Training Iterations** | 30 | 50 (more states need more exploration) |\n",
    "| **Agent Selection** | Best quality only | Best quality **given current load distribution** |\n",
    "| **Production Routing** | Deterministic per task type | **Adaptive** â€” shifts agents when one is overloaded |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- `OPENAI_API_KEY` environment variable set\n",
    "- `ANTHROPIC_API_KEY` environment variable set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q numpy pandas matplotlib seaborn \\\n",
    "    langchain langchain-openai langchain-anthropic langgraph \\\n",
    "    langchain-core pydantic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, TypedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import deque, Counter\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"âŒ OPENAI_API_KEY not set!\"\n",
    "assert os.environ.get(\"ANTHROPIC_API_KEY\"), \"âŒ ANTHROPIC_API_KEY not set!\"\n",
    "print(\"âœ… API keys verified\")\n",
    "print(f\"   OpenAI key:    ...{os.environ['OPENAI_API_KEY'][-6:]}\")\n",
    "print(f\"   Anthropic key: ...{os.environ['ANTHROPIC_API_KEY'][-6:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Service Layer\n",
    "\n",
    "Wraps OpenAI and Anthropic with token tracking and cost estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    content: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_seconds: float\n",
    "    cost_estimate: float\n",
    "\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Manages LLM calls with token & cost tracking.\"\"\"\n",
    "\n",
    "    PRICING = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"claude-3-5-haiku-latest\": {\"input\": 0.80, \"output\": 4.00},\n",
    "        \"claude-3-5-sonnet-latest\": {\"input\": 3.00, \"output\": 15.00},\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "        anthropic_model: str = \"claude-3-5-haiku-latest\",\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "    ):\n",
    "        self.openai_model_name = openai_model\n",
    "        self.anthropic_model_name = anthropic_model\n",
    "        self.openai_llm = ChatOpenAI(model=openai_model, temperature=temperature, max_tokens=max_tokens)\n",
    "        self.anthropic_llm = ChatAnthropic(model=anthropic_model, temperature=temperature, max_tokens=max_tokens)\n",
    "        self.total_cost = 0.0\n",
    "        self.call_count = 0\n",
    "        self.call_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _estimate_cost(self, model: str, inp: int, out: int) -> float:\n",
    "        p = self.PRICING.get(model, {\"input\": 1.0, \"output\": 3.0})\n",
    "        return (inp * p[\"input\"] + out * p[\"output\"]) / 1_000_000\n",
    "\n",
    "    def call(self, system_prompt: str, user_message: str,\n",
    "             provider: LLMProvider = LLMProvider.OPENAI) -> LLMResponse:\n",
    "        msgs = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]\n",
    "        llm = self.openai_llm if provider == LLMProvider.OPENAI else self.anthropic_llm\n",
    "        model_name = self.openai_model_name if provider == LLMProvider.OPENAI else self.anthropic_model_name\n",
    "\n",
    "        start = time.time()\n",
    "        result = llm.invoke(msgs)\n",
    "        latency = time.time() - start\n",
    "\n",
    "        usage = result.usage_metadata or {}\n",
    "        inp = usage.get(\"input_tokens\", 0)\n",
    "        out = usage.get(\"output_tokens\", 0)\n",
    "        cost = self._estimate_cost(model_name, inp, out)\n",
    "        self.total_cost += cost\n",
    "        self.call_count += 1\n",
    "        self.call_log.append({\"model\": model_name, \"input_tokens\": inp,\n",
    "                              \"output_tokens\": out, \"latency\": latency, \"cost\": cost})\n",
    "        return LLMResponse(\n",
    "            content=result.content, provider=provider.value, model=model_name,\n",
    "            input_tokens=inp, output_tokens=out, total_tokens=inp + out,\n",
    "            latency_seconds=round(latency, 3), cost_estimate=cost,\n",
    "        )\n",
    "\n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        if not self.call_log:\n",
    "            return {\"total_calls\": 0, \"total_cost\": 0.0}\n",
    "        df = pd.DataFrame(self.call_log)\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_cost_usd\": round(self.total_cost, 6),\n",
    "            \"avg_latency_s\": round(df[\"latency\"].mean(), 3),\n",
    "            \"avg_tokens\": int(df[\"input_tokens\"].mean() + df[\"output_tokens\"].mean()),\n",
    "            \"by_model\": df.groupby(\"model\")[\"cost\"].sum().to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "llm_service = LLMService(openai_model=\"gpt-4o-mini\", anthropic_model=\"claude-3-5-haiku-latest\",\n",
    "                          temperature=0.7, max_tokens=768)\n",
    "\n",
    "# Smoke test\n",
    "_r = llm_service.call(\"You are a helpful assistant.\", \"Say hello in one sentence.\", LLMProvider.OPENAI)\n",
    "print(f\"âœ… LLM Service ready â€” test: {_r.content[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Definitions and Task Model\n",
    "\n",
    "Each agent has a **specialized role** with a dedicated system prompt. Tasks are described in natural language and routed to the most suitable agent by the RL coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRole(Enum):\n",
    "    RESEARCHER = \"researcher\"\n",
    "    ANALYST = \"analyst\"\n",
    "    CODER = \"coder\"\n",
    "    VALIDATOR = \"validator\"\n",
    "\n",
    "\n",
    "# â”€â”€ System prompts per role â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AGENT_SYSTEM_PROMPTS: Dict[AgentRole, str] = {\n",
    "    AgentRole.RESEARCHER: (\n",
    "        \"You are a meticulous research specialist. \"\n",
    "        \"Given a topic or question, provide well-structured research findings with key facts, \"\n",
    "        \"relevant context, and references where possible. \"\n",
    "        \"Focus on accuracy and breadth of coverage.\"\n",
    "    ),\n",
    "    AgentRole.ANALYST: (\n",
    "        \"You are an expert data and systems analyst. \"\n",
    "        \"Given information or a question, provide structured analysis including pros/cons, \"\n",
    "        \"trade-offs, patterns, and data-driven insights. \"\n",
    "        \"Use clear reasoning and quantitative arguments where possible.\"\n",
    "    ),\n",
    "    AgentRole.CODER: (\n",
    "        \"You are a senior software engineer. \"\n",
    "        \"Given a programming task, produce clean, well-documented code with explanations. \"\n",
    "        \"Include error handling, type hints, and follow best practices.\"\n",
    "    ),\n",
    "    AgentRole.VALIDATOR: (\n",
    "        \"You are a quality assurance specialist. \"\n",
    "        \"Given a piece of work (research, analysis, or code), verify its correctness, \"\n",
    "        \"identify errors or gaps, and provide an improvement checklist. \"\n",
    "        \"Be thorough and constructive.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"A task to be processed by the multi-agent system.\"\"\"\n",
    "    id: str\n",
    "    description: str\n",
    "    task_type: str           # research | analysis | coding | validation\n",
    "    complexity: float        # 0-1\n",
    "    priority: float          # 0-1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Tracks an individual agent's state.\"\"\"\n",
    "    id: str\n",
    "    role: AgentRole\n",
    "    provider: LLMProvider\n",
    "    completed_tasks: int = 0\n",
    "    total_reward: float = 0.0\n",
    "    avg_quality: float = 0.0\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate agent pool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AGENTS: Dict[str, AgentState] = {\n",
    "    \"researcher\": AgentState(\"researcher\", AgentRole.RESEARCHER, LLMProvider.OPENAI),\n",
    "    \"analyst\":    AgentState(\"analyst\",    AgentRole.ANALYST,    LLMProvider.OPENAI),\n",
    "    \"coder\":      AgentState(\"coder\",      AgentRole.CODER,      LLMProvider.ANTHROPIC),\n",
    "    \"validator\":  AgentState(\"validator\",  AgentRole.VALIDATOR,  LLMProvider.OPENAI),\n",
    "}\n",
    "\n",
    "# â”€â”€ Task bank â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TASK_BANK: List[Task] = [\n",
    "    # Research tasks\n",
    "    Task(\"r1\", \"Research the current state of quantum computing and its practical applications.\", \"research\", 0.7, 0.8),\n",
    "    Task(\"r2\", \"Summarize the key differences between supervised, unsupervised, and reinforcement learning.\", \"research\", 0.5, 0.6),\n",
    "    Task(\"r3\", \"What are the main approaches to federated learning and their trade-offs?\", \"research\", 0.6, 0.7),\n",
    "    # Analysis tasks\n",
    "    Task(\"a1\", \"Analyze the trade-offs between microservices and monolithic architecture for a startup.\", \"analysis\", 0.6, 0.7),\n",
    "    Task(\"a2\", \"Compare REST, GraphQL, and gRPC for inter-service communication.\", \"analysis\", 0.5, 0.8),\n",
    "    Task(\"a3\", \"Evaluate the pros and cons of event-driven vs request-driven architectures.\", \"analysis\", 0.7, 0.6),\n",
    "    # Coding tasks\n",
    "    Task(\"c1\", \"Write a Python class that implements a thread-safe LRU cache.\", \"coding\", 0.6, 0.9),\n",
    "    Task(\"c2\", \"Implement a Python decorator that retries a function with exponential backoff.\", \"coding\", 0.5, 0.7),\n",
    "    Task(\"c3\", \"Write a Python async producer-consumer pattern using asyncio queues.\", \"coding\", 0.7, 0.8),\n",
    "    # Validation tasks\n",
    "    Task(\"v1\", \"Review and validate the correctness of this claim: 'TCP guarantees in-order delivery while UDP does not.'\", \"validation\", 0.3, 0.5),\n",
    "    Task(\"v2\", \"Validate the statement: 'B-trees are always more efficient than hash indexes for database lookups.'\", \"validation\", 0.4, 0.6),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Agent pool: {len(AGENTS)} agents\")\n",
    "for aid, a in AGENTS.items():\n",
    "    print(f\"   {a.role.value:12s} â†’ {a.provider.value}\")\n",
    "print(f\"âœ… Task bank:  {len(TASK_BANK)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-as-Judge Reward Model\n",
    "\n",
    "A separate LLM call evaluates the quality of agent outputs, producing a scalar reward for the RL coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScores(BaseModel):\n",
    "    relevance: float = Field(ge=0, le=1, description=\"How relevant the output is to the task\")\n",
    "    accuracy: float = Field(ge=0, le=1, description=\"Factual / logical accuracy\")\n",
    "    completeness: float = Field(ge=0, le=1, description=\"How thoroughly the task is addressed\")\n",
    "    agent_match: float = Field(ge=0, le=1, description=\"How well the assigned agent role fits the task\")\n",
    "    reasoning: str = Field(description=\"Brief justification\")\n",
    "\n",
    "\n",
    "JUDGE_SYSTEM = \"\"\"You are an expert evaluator of AI agent outputs.\n",
    "You will receive:\n",
    "- The original task description and type\n",
    "- The agent role that processed it\n",
    "- The agent's output\n",
    "\n",
    "Score the output on each dimension from 0.0 to 1.0.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{\n",
    "  \"relevance\": <float>,\n",
    "  \"accuracy\": <float>,\n",
    "  \"completeness\": <float>,\n",
    "  \"agent_match\": <float>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}\n",
    "\n",
    "Be critical. 1.0 means perfect.\"\"\"\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, llm_service: LLMService, provider: LLMProvider = LLMProvider.OPENAI):\n",
    "        self.llm_service = llm_service\n",
    "        self.provider = provider\n",
    "        self.log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def evaluate(self, task_desc: str, task_type: str, agent_role: str, output: str) -> QualityScores:\n",
    "        msg = (\n",
    "            f\"## Task\\n{task_desc}\\n\\n\"\n",
    "            f\"## Task Type\\n{task_type}\\n\\n\"\n",
    "            f\"## Agent Role\\n{agent_role}\\n\\n\"\n",
    "            f\"## Agent Output\\n{output[:2000]}\\n\\n\"\n",
    "            f\"Evaluate now.\"\n",
    "        )\n",
    "        resp = self.llm_service.call(JUDGE_SYSTEM, msg, self.provider)\n",
    "        try:\n",
    "            raw = resp.content.strip()\n",
    "            if raw.startswith(\"```\"):\n",
    "                raw = raw.split(\"```\")[1]\n",
    "                if raw.startswith(\"json\"):\n",
    "                    raw = raw[4:]\n",
    "            scores = QualityScores(**json.loads(raw))\n",
    "        except Exception:\n",
    "            scores = QualityScores(relevance=0.5, accuracy=0.5, completeness=0.5,\n",
    "                                   agent_match=0.5, reasoning=\"Parse error; defaults used.\")\n",
    "        self.log.append({\n",
    "            \"task_type\": task_type, \"agent_role\": agent_role,\n",
    "            \"relevance\": scores.relevance, \"accuracy\": scores.accuracy,\n",
    "            \"completeness\": scores.completeness, \"agent_match\": scores.agent_match,\n",
    "        })\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def composite_reward(scores: QualityScores, cost: float, latency: float,\n",
    "                         cost_w: float = 0.10, latency_w: float = 0.05) -> float:\n",
    "        quality = np.mean([scores.relevance, scores.accuracy,\n",
    "                           scores.completeness, scores.agent_match])\n",
    "        norm_cost = min(cost / 0.002, 1.0)\n",
    "        norm_lat = min(latency / 5.0, 1.0)\n",
    "        return float(np.clip(quality - cost_w * norm_cost - latency_w * norm_lat, 0, 1))\n",
    "\n",
    "\n",
    "judge = LLMJudge(llm_service)\n",
    "print(\"âœ… LLM-as-Judge ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Tracker + Q-Learning RL Coordinator\n",
    "\n",
    "The **Load Tracker** maintains a sliding window of recent agent assignments and computes:\n",
    "- **Per-agent utilization** â€” fraction of recent tasks handled by each agent.\n",
    "- **Load state** â€” a discrete label (`\"balanced\"` or `\"<agent>_hot\"`) indicating which, if any, agent is overloaded.\n",
    "- **Load penalty** â€” a scalar subtracted from the reward when assigning to an over-utilized agent.\n",
    "\n",
    "The **Q-Learning coordinator** now uses a **two-part state**: `(task_type, load_state)`. This allows it to learn *conditional* policies â€” e.g., \"when the coder is hot, route coding tasks to the analyst instead.\"\n",
    "\n",
    "| Component | Base Version | Load-Balanced Version |\n",
    "|-----------|-------------|----------------------|\n",
    "| **State** | `task_type` (4 values) | `(task_type, load_state)` (4 Ã— 5 = 20 values) |\n",
    "| **Actions** | 8 agent-provider pairs | 8 agent-provider pairs (unchanged) |\n",
    "| **Q-Table** | 32 cells | 160 cells |\n",
    "| **Reward** | quality âˆ’ cost âˆ’ latency | quality âˆ’ cost âˆ’ latency âˆ’ **load penalty** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAssignment:\n",
    "    \"\"\"Represents an RL action: which agent (role) and provider to assign a task to.\"\"\"\n",
    "    def __init__(self, agent_id: str, role: AgentRole, provider: LLMProvider):\n",
    "        self.agent_id = agent_id\n",
    "        self.role = role\n",
    "        self.provider = provider\n",
    "\n",
    "    def key(self) -> str:\n",
    "        return f\"{self.agent_id}_{self.provider.value}\"\n",
    "\n",
    "\n",
    "# All possible agent-provider assignments\n",
    "ASSIGNMENT_ACTIONS: List[TaskAssignment] = []\n",
    "for aid, agent in AGENTS.items():\n",
    "    for prov in LLMProvider:\n",
    "        ASSIGNMENT_ACTIONS.append(TaskAssignment(aid, agent.role, prov))\n",
    "\n",
    "TASK_TYPES = sorted(set(t.task_type for t in TASK_BANK))\n",
    "AGENT_IDS = sorted(AGENTS.keys())\n",
    "\n",
    "print(f\"Task types:  {TASK_TYPES}\")\n",
    "print(f\"Agent IDs:   {AGENT_IDS}\")\n",
    "print(f\"Action space ({len(ASSIGNMENT_ACTIONS)} assignments):\")\n",
    "for a in ASSIGNMENT_ACTIONS:\n",
    "    print(f\"  {a.agent_id:12s} via {a.provider.value}\")\n",
    "\n",
    "\n",
    "# â”€â”€ Load Tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class LoadTracker:\n",
    "    \"\"\"Tracks agent utilization over a sliding window of recent assignments.\n",
    "\n",
    "    Provides:\n",
    "      â€¢ per-agent utilization  (fraction of recent tasks)\n",
    "      â€¢ discrete load state    (\"balanced\" | \"<agent>_hot\")\n",
    "      â€¢ scalar load penalty    (for reward shaping)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent_ids: List[str], window_size: int = 8,\n",
    "                 hot_threshold: float = 1.5, penalty_weight: float = 0.15,\n",
    "                 max_penalty: float = 0.20):\n",
    "        self.agent_ids = agent_ids\n",
    "        self.window_size = window_size\n",
    "        self.hot_threshold = hot_threshold      # Ã— fair share to become \"hot\"\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.max_penalty = max_penalty\n",
    "        self.recent: deque = deque(maxlen=window_size)\n",
    "        self.lifetime_counts: Dict[str, int] = {aid: 0 for aid in agent_ids}\n",
    "\n",
    "    def record(self, agent_id: str):\n",
    "        \"\"\"Record an assignment (call AFTER computing reward for this step).\"\"\"\n",
    "        self.recent.append(agent_id)\n",
    "        self.lifetime_counts[agent_id] = self.lifetime_counts.get(agent_id, 0) + 1\n",
    "\n",
    "    def get_utilization(self) -> Dict[str, float]:\n",
    "        \"\"\"Return utilization fraction per agent over the sliding window.\"\"\"\n",
    "        if not self.recent:\n",
    "            return {aid: 0.0 for aid in self.agent_ids}\n",
    "        counts = Counter(self.recent)\n",
    "        total = len(self.recent)\n",
    "        return {aid: counts.get(aid, 0) / total for aid in self.agent_ids}\n",
    "\n",
    "    def get_load_state(self) -> str:\n",
    "        \"\"\"Return a discrete load state for use as a Q-table key.\n",
    "\n",
    "        Returns:\n",
    "            \"balanced\"       â€” no agent exceeds hot_threshold Ã— fair_share\n",
    "            \"<agent_id>_hot\" â€” that agent has the highest utilization and exceeds threshold\n",
    "        \"\"\"\n",
    "        if len(self.recent) < 3:\n",
    "            return \"balanced\"\n",
    "        util = self.get_utilization()\n",
    "        fair_share = 1.0 / len(self.agent_ids)          # 0.25 for 4 agents\n",
    "        threshold = fair_share * self.hot_threshold      # 0.375\n",
    "\n",
    "        max_agent = max(util, key=util.get)\n",
    "        if util[max_agent] > threshold:\n",
    "            return f\"{max_agent}_hot\"\n",
    "        return \"balanced\"\n",
    "\n",
    "    def get_load_penalty(self, agent_id: str) -> float:\n",
    "        \"\"\"Compute a penalty for assigning to an over-utilized agent.\n",
    "\n",
    "        Penalty grows linearly with how far the agent's utilization exceeds\n",
    "        fair share, capped at max_penalty.\n",
    "\n",
    "          fair_share  = 1/N_agents  (0.25 for 4 agents)\n",
    "          excess_ratio = util / fair_share âˆ’ 1  (0 if at or below fair share)\n",
    "          penalty = min(excess_ratio Ã— penalty_weight, max_penalty)\n",
    "\n",
    "        Examples (4 agents, penalty_weight=0.15, max_penalty=0.20):\n",
    "          25% util â†’ 0.000,  37.5% â†’ 0.075,  50% â†’ 0.150,  75% â†’ 0.200\n",
    "        \"\"\"\n",
    "        util = self.get_utilization()\n",
    "        fair_share = 1.0 / len(self.agent_ids)\n",
    "        agent_util = util.get(agent_id, 0.0)\n",
    "        excess_ratio = max(0.0, agent_util / fair_share - 1.0)\n",
    "        return min(excess_ratio * self.penalty_weight, self.max_penalty)\n",
    "\n",
    "    def get_imbalance_score(self) -> float:\n",
    "        \"\"\"Return overall imbalance as coefficient of variation (std/mean).\"\"\"\n",
    "        util = self.get_utilization()\n",
    "        vals = list(util.values())\n",
    "        if max(vals) == 0:\n",
    "            return 0.0\n",
    "        return float(np.std(vals) / max(np.mean(vals), 1e-9))\n",
    "\n",
    "\n",
    "# â”€â”€ Load-Aware Q-Learning Coordinator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Load states: \"balanced\" + one per agent\n",
    "LOAD_STATES = [\"balanced\"] + [f\"{aid}_hot\" for aid in AGENT_IDS]\n",
    "\n",
    "class QLearningCoordinator:\n",
    "    \"\"\"Load-aware Q-Learning agent for task + load_state -> agent+provider routing.\"\"\"\n",
    "\n",
    "    def __init__(self, task_types: List[str], load_states: List[str],\n",
    "                 actions: List[TaskAssignment],\n",
    "                 lr: float = 0.15, gamma: float = 0.95, epsilon: float = 1.0,\n",
    "                 epsilon_min: float = 0.05, epsilon_decay: float = 0.98):\n",
    "        self.task_types = task_types\n",
    "        self.load_states = load_states\n",
    "        self.actions = actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Q-table: (task_type, load_state) -> action_idx -> Q-value\n",
    "        self.q_table: Dict[Tuple[str, str], np.ndarray] = {}\n",
    "        for tt in task_types:\n",
    "            for ls in load_states:\n",
    "                self.q_table[(tt, ls)] = np.zeros(len(actions))\n",
    "        self.history: List[Dict[str, Any]] = []\n",
    "\n",
    "    def select_action(self, task_type: str, load_state: str) -> Tuple[int, TaskAssignment]:\n",
    "        state = (task_type, load_state)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            idx = np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            idx = int(np.argmax(self.q_table[state]))\n",
    "        return idx, self.actions[idx]\n",
    "\n",
    "    def update(self, task_type: str, load_state: str, action_idx: int,\n",
    "               reward: float, next_task_type: Optional[str] = None,\n",
    "               next_load_state: Optional[str] = None):\n",
    "        state = (task_type, load_state)\n",
    "        old_q = self.q_table[state][action_idx]\n",
    "        if next_task_type is not None and next_load_state is not None:\n",
    "            max_next = np.max(self.q_table[(next_task_type, next_load_state)])\n",
    "        else:\n",
    "            max_next = 0.0\n",
    "        td_target = reward + self.gamma * max_next\n",
    "        self.q_table[state][action_idx] += self.lr * (td_target - old_q)\n",
    "\n",
    "        self.history.append({\n",
    "            \"task_type\": task_type,\n",
    "            \"load_state\": load_state,\n",
    "            \"action_idx\": action_idx,\n",
    "            \"agent\": self.actions[action_idx].agent_id,\n",
    "            \"provider\": self.actions[action_idx].provider.value,\n",
    "            \"reward\": reward,\n",
    "            \"q_value\": self.q_table[state][action_idx],\n",
    "        })\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "load_tracker = LoadTracker(AGENT_IDS, window_size=8)\n",
    "coordinator = QLearningCoordinator(TASK_TYPES, LOAD_STATES, ASSIGNMENT_ACTIONS)\n",
    "\n",
    "n_states = len(TASK_TYPES) * len(LOAD_STATES)\n",
    "n_cells = n_states * len(ASSIGNMENT_ACTIONS)\n",
    "print(f\"\\nâœ… Load Tracker: window={load_tracker.window_size}, hot_threshold={load_tracker.hot_threshold}Ã—\")\n",
    "print(f\"âœ… Q-Learning Coordinator: {n_states} states ({len(TASK_TYPES)} task Ã— {len(LOAD_STATES)} load) Ã— {len(ASSIGNMENT_ACTIONS)} actions = {n_cells} Q-values\")\n",
    "print(f\"   Load states: {LOAD_STATES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LangGraph Multi-Agent Workflow (Load-Balanced)\n",
    "\n",
    "The training loop is orchestrated as a **LangGraph state machine** with five nodes. The `assign_agent` node now queries the **LoadTracker** for the current load state before consulting the Q-table, and `rl_update` applies a **load penalty** to the reward before updating Q-values.\n",
    "\n",
    "```\n",
    "START --> pick_task --> assign_agent (load-aware) --> agent_execute --> judge_output --> rl_update (load penalty) --> (loop or END)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowState(TypedDict):\n",
    "    \"\"\"LangGraph state for the multi-agent RL training loop (load-balanced).\"\"\"\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    task: Optional[Dict[str, Any]]\n",
    "    assignment: Optional[Dict[str, Any]]\n",
    "    action_idx: int\n",
    "    load_state: str                              # NEW: \"balanced\" | \"<agent>_hot\"\n",
    "    agent_output: str\n",
    "    llm_response_meta: Optional[Dict[str, Any]]\n",
    "    scores: Optional[Dict[str, float]]\n",
    "    reward: float                                # raw quality reward\n",
    "    adjusted_reward: float                       # NEW: reward âˆ’ load penalty\n",
    "    load_penalty: float                          # NEW\n",
    "    # Accumulated metrics\n",
    "    rewards_log: List[float]\n",
    "    scores_log: List[Dict[str, Any]]\n",
    "    assignments_log: List[Dict[str, Any]]\n",
    "    load_log: List[Dict[str, Any]]               # NEW: per-iteration load snapshots\n",
    "\n",
    "\n",
    "# â”€â”€ Node functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def pick_task(state: WorkflowState) -> dict:\n",
    "    \"\"\"Randomly sample a task from the task bank.\"\"\"\n",
    "    task = random.choice(TASK_BANK)\n",
    "    return {\n",
    "        \"task\": {\n",
    "            \"id\": task.id, \"description\": task.description,\n",
    "            \"task_type\": task.task_type, \"complexity\": task.complexity,\n",
    "            \"priority\": task.priority,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_agent(state: WorkflowState) -> dict:\n",
    "    \"\"\"Use the Q-Learning coordinator to choose an agent + provider (load-aware).\"\"\"\n",
    "    task_type = state[\"task\"][\"task_type\"]\n",
    "    load_state = load_tracker.get_load_state()     # â† load state BEFORE assignment\n",
    "    action_idx, assignment = coordinator.select_action(task_type, load_state)\n",
    "    return {\n",
    "        \"assignment\": {\n",
    "            \"agent_id\": assignment.agent_id,\n",
    "            \"role\": assignment.role.value,\n",
    "            \"provider\": assignment.provider.value,\n",
    "        },\n",
    "        \"action_idx\": action_idx,\n",
    "        \"load_state\": load_state,\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_execute(state: WorkflowState) -> dict:\n",
    "    \"\"\"Execute the task with a real LLM call through the assigned agent.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    assign = state[\"assignment\"]\n",
    "\n",
    "    role = AgentRole(assign[\"role\"])\n",
    "    provider = LLMProvider(assign[\"provider\"])\n",
    "\n",
    "    system_prompt = AGENT_SYSTEM_PROMPTS[role]\n",
    "    user_message = (\n",
    "        f\"[{task['task_type'].upper()} TASK | complexity={task['complexity']:.1f} | \"\n",
    "        f\"priority={task['priority']:.1f}]\\n\\n{task['description']}\"\n",
    "    )\n",
    "\n",
    "    response = llm_service.call(system_prompt, user_message, provider)\n",
    "\n",
    "    return {\n",
    "        \"agent_output\": response.content,\n",
    "        \"llm_response_meta\": {\n",
    "            \"provider\": response.provider,\n",
    "            \"model\": response.model,\n",
    "            \"tokens\": response.total_tokens,\n",
    "            \"latency\": response.latency_seconds,\n",
    "            \"cost\": response.cost_estimate,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def judge_output(state: WorkflowState) -> dict:\n",
    "    \"\"\"Use the LLM-as-Judge to score the agent output.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    assign = state[\"assignment\"]\n",
    "    scores = judge.evaluate(\n",
    "        task_desc=task[\"description\"],\n",
    "        task_type=task[\"task_type\"],\n",
    "        agent_role=assign[\"role\"],\n",
    "        output=state[\"agent_output\"],\n",
    "    )\n",
    "    score_dict = {\n",
    "        \"relevance\": scores.relevance,\n",
    "        \"accuracy\": scores.accuracy,\n",
    "        \"completeness\": scores.completeness,\n",
    "        \"agent_match\": scores.agent_match,\n",
    "        \"reasoning\": scores.reasoning,\n",
    "    }\n",
    "    meta = state[\"llm_response_meta\"]\n",
    "    reward = LLMJudge.composite_reward(scores, meta[\"cost\"], meta[\"latency\"])\n",
    "    return {\"scores\": score_dict, \"reward\": reward}\n",
    "\n",
    "\n",
    "def rl_update(state: WorkflowState) -> dict:\n",
    "    \"\"\"Update Q-table with load-adjusted reward and record assignment in load tracker.\"\"\"\n",
    "    task_type = state[\"task\"][\"task_type\"]\n",
    "    load_state = state[\"load_state\"]\n",
    "    action_idx = state[\"action_idx\"]\n",
    "    raw_reward = state[\"reward\"]\n",
    "    aid = state[\"assignment\"][\"agent_id\"]\n",
    "\n",
    "    # â”€â”€ Load penalty â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    load_penalty = load_tracker.get_load_penalty(aid)\n",
    "    adjusted_reward = max(0.0, raw_reward - load_penalty)\n",
    "\n",
    "    # â”€â”€ Record assignment in tracker (AFTER computing penalty) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    load_tracker.record(aid)\n",
    "\n",
    "    # â”€â”€ Peek at next state for TD bootstrapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    next_type = random.choice(TASK_TYPES)\n",
    "    next_load_state = load_tracker.get_load_state()\n",
    "    coordinator.update(task_type, load_state, action_idx, adjusted_reward,\n",
    "                       next_type, next_load_state)\n",
    "    coordinator.decay_epsilon()\n",
    "\n",
    "    # â”€â”€ Update agent-level stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    ag = AGENTS[aid]\n",
    "    ag.completed_tasks += 1\n",
    "    ag.total_reward += adjusted_reward\n",
    "    ag.avg_quality = ag.total_reward / ag.completed_tasks\n",
    "\n",
    "    # â”€â”€ Build new log lists (idiomatic LangGraph: never mutate) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    prev_rewards = state.get(\"rewards_log\", []) or []\n",
    "    prev_scores = state.get(\"scores_log\", []) or []\n",
    "    prev_assignments = state.get(\"assignments_log\", []) or []\n",
    "    prev_loads = state.get(\"load_log\", []) or []\n",
    "\n",
    "    it = state[\"iteration\"] + 1\n",
    "    util = load_tracker.get_utilization()\n",
    "    imbalance = load_tracker.get_imbalance_score()\n",
    "\n",
    "    print(f\"   [{it}/{state['max_iterations']}] reward={adjusted_reward:.3f} \"\n",
    "          f\"(raw={raw_reward:.3f} load_pen={load_penalty:.3f}) \"\n",
    "          f\"agent={aid} load_state={load_state} imbalance={imbalance:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"rewards_log\": prev_rewards + [adjusted_reward],\n",
    "        \"scores_log\": prev_scores + [state[\"scores\"]],\n",
    "        \"assignments_log\": prev_assignments + [{\n",
    "            \"iteration\": state[\"iteration\"],\n",
    "            \"task_type\": task_type,\n",
    "            \"agent\": aid,\n",
    "            \"provider\": state[\"assignment\"][\"provider\"],\n",
    "            \"reward\": adjusted_reward,\n",
    "            \"raw_reward\": raw_reward,\n",
    "            \"load_penalty\": load_penalty,\n",
    "            \"load_state\": load_state,\n",
    "            **{k: v for k, v in state[\"scores\"].items() if k != \"reasoning\"},\n",
    "        }],\n",
    "        \"load_log\": prev_loads + [{\n",
    "            \"iteration\": state[\"iteration\"],\n",
    "            **{f\"util_{k}\": v for k, v in util.items()},\n",
    "            \"load_state\": load_state,\n",
    "            \"imbalance\": imbalance,\n",
    "        }],\n",
    "        \"adjusted_reward\": adjusted_reward,\n",
    "        \"load_penalty\": load_penalty,\n",
    "        \"iteration\": it,\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: WorkflowState) -> str:\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return END\n",
    "    return \"pick_task\"\n",
    "\n",
    "\n",
    "# â”€â”€ Build the graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "workflow.add_node(\"pick_task\", pick_task)\n",
    "workflow.add_node(\"assign_agent\", assign_agent)\n",
    "workflow.add_node(\"agent_execute\", agent_execute)\n",
    "workflow.add_node(\"judge_output\", judge_output)\n",
    "workflow.add_node(\"rl_update\", rl_update)\n",
    "\n",
    "workflow.add_edge(START, \"pick_task\")\n",
    "workflow.add_edge(\"pick_task\", \"assign_agent\")\n",
    "workflow.add_edge(\"assign_agent\", \"agent_execute\")\n",
    "workflow.add_edge(\"agent_execute\", \"judge_output\")\n",
    "workflow.add_edge(\"judge_output\", \"rl_update\")\n",
    "workflow.add_conditional_edges(\"rl_update\", should_continue, {END: END, \"pick_task\": \"pick_task\"})\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"âœ… LangGraph workflow compiled (load-balanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Run\n",
    "\n",
    "Each iteration makes 2 LLM calls (agent + judge). With 50 iterations (increased from 30 to explore the larger state space) that is approximately 100 API calls.\n",
    "\n",
    "Estimated cost: less than $0.20 with gpt-4o-mini / claude-3-5-haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERATIONS = 50  # Increased from 30 to cover the larger (task Ã— load) state space\n",
    "\n",
    "initial_state: WorkflowState = {\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": N_ITERATIONS,\n",
    "    \"task\": None,\n",
    "    \"assignment\": None,\n",
    "    \"action_idx\": 0,\n",
    "    \"load_state\": \"balanced\",\n",
    "    \"agent_output\": \"\",\n",
    "    \"llm_response_meta\": None,\n",
    "    \"scores\": None,\n",
    "    \"reward\": 0.0,\n",
    "    \"adjusted_reward\": 0.0,\n",
    "    \"load_penalty\": 0.0,\n",
    "    \"rewards_log\": [],\n",
    "    \"scores_log\": [],\n",
    "    \"assignments_log\": [],\n",
    "    \"load_log\": [],\n",
    "}\n",
    "\n",
    "print(f\"Starting load-balanced training for {N_ITERATIONS} iterations...\")\n",
    "print(f\"Initial epsilon: {coordinator.epsilon:.2f}\")\n",
    "print(f\"Load tracker window: {load_tracker.window_size}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_state = app.invoke(initial_state)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "# â”€â”€ Compute load balance metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "final_util = load_tracker.get_utilization()\n",
    "final_imbalance = load_tracker.get_imbalance_score()\n",
    "load_penalties = [a.get(\"load_penalty\", 0) for a in final_state[\"assignments_log\"]]\n",
    "mean_penalty = np.mean(load_penalties) if load_penalties else 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete in {elapsed:.1f}s\")\n",
    "print(f\"  Iterations:           {final_state['iteration']}\")\n",
    "print(f\"  Mean adjusted reward: {np.mean(final_state['rewards_log']):.3f}\")\n",
    "print(f\"  Mean load penalty:    {mean_penalty:.4f}\")\n",
    "print(f\"  Final epsilon:        {coordinator.epsilon:.4f}\")\n",
    "print(f\"  Final imbalance:      {final_imbalance:.3f}\")\n",
    "print(f\"  Total LLM calls:      {llm_service.call_count}\")\n",
    "print(f\"  Total cost (USD):     ${llm_service.total_cost:.4f}\")\n",
    "print(f\"\\nFinal agent utilization (last {load_tracker.window_size} tasks):\")\n",
    "for aid, u in final_util.items():\n",
    "    print(f\"  {aid:12s}  {u:.1%}  ({load_tracker.lifetime_counts[aid]} total)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.DataFrame(final_state[\"assignments_log\"])\n",
    "df_load = pd.DataFrame(final_state[\"load_log\"]) if final_state.get(\"load_log\") else pd.DataFrame()\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "fig.suptitle(\"Multi-Agent RL Training Dashboard (Load-Balanced)\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# â”€â”€ 1. Reward curve (raw vs adjusted) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0, 0]\n",
    "rewards = final_state[\"rewards_log\"]\n",
    "raw_rewards = df_log[\"raw_reward\"].tolist() if \"raw_reward\" in df_log.columns else rewards\n",
    "ax.plot(raw_rewards, \"o-\", alpha=0.35, markersize=3, color=\"gray\", label=\"Raw (quality)\")\n",
    "ax.plot(rewards, \"o-\", alpha=0.5, markersize=4, color=\"steelblue\", label=\"Adjusted (âˆ’load penalty)\")\n",
    "window = min(7, len(rewards))\n",
    "if len(rewards) >= window:\n",
    "    rolling = pd.Series(rewards).rolling(window).mean()\n",
    "    ax.plot(rolling, \"r-\", linewidth=2, label=f\"{window}-step MA\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Reward\")\n",
    "ax.set_title(\"Reward Over Training (Raw vs Load-Adjusted)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€ 2. Quality score breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0, 1]\n",
    "score_cols = [\"relevance\", \"accuracy\", \"completeness\", \"agent_match\"]\n",
    "for col in score_cols:\n",
    "    if col in df_log.columns:\n",
    "        ax.plot(df_log[col], \"o-\", markersize=3, alpha=0.6, label=col)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Judge Quality Scores\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€ 3. Agent utilisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1, 0]\n",
    "usage = df_log.groupby([\"agent\", \"provider\"]).size().reset_index(name=\"count\")\n",
    "bars = [f\"{r.agent}\\n({r.provider})\" for _, r in usage.iterrows()]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(bars)))\n",
    "ax.bar(bars, usage[\"count\"], color=colors)\n",
    "# Draw fair-share line\n",
    "fair_count = len(df_log) / len(AGENTS)\n",
    "ax.axhline(fair_count, color=\"red\", linestyle=\"--\", linewidth=1.5, label=f\"Fair share ({fair_count:.1f})\")\n",
    "ax.set_title(\"Agent-Provider Utilisation vs Fair Share\")\n",
    "ax.set_ylabel(\"Tasks Assigned\")\n",
    "ax.legend()\n",
    "\n",
    "# â”€â”€ 4. Avg reward by task type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1, 1]\n",
    "avg_by_type = df_log.groupby(\"task_type\")[\"reward\"].mean().sort_values()\n",
    "avg_by_type.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "ax.set_xlabel(\"Mean Adjusted Reward\")\n",
    "ax.set_title(\"Mean Reward by Task Type\")\n",
    "\n",
    "# â”€â”€ 5. Agent utilization over time (sliding window) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[2, 0]\n",
    "if not df_load.empty:\n",
    "    util_cols = [c for c in df_load.columns if c.startswith(\"util_\")]\n",
    "    for col in util_cols:\n",
    "        agent_name = col.replace(\"util_\", \"\")\n",
    "        ax.plot(df_load[\"iteration\"], df_load[col], \"o-\", markersize=3, alpha=0.7, label=agent_name)\n",
    "    ax.axhline(1.0 / len(AGENTS), color=\"red\", linestyle=\"--\", linewidth=1, label=\"Fair share\")\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Utilization (sliding window)\")\n",
    "    ax.set_title(f\"Per-Agent Utilization Over Time (window={load_tracker.window_size})\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No load data\", ha=\"center\", va=\"center\")\n",
    "    ax.set_title(\"Per-Agent Utilization Over Time\")\n",
    "\n",
    "# â”€â”€ 6. Load penalty and imbalance over time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[2, 1]\n",
    "if \"load_penalty\" in df_log.columns:\n",
    "    ax.plot(df_log[\"load_penalty\"], \"o-\", markersize=3, alpha=0.6, color=\"orange\", label=\"Load penalty\")\n",
    "if not df_load.empty and \"imbalance\" in df_load.columns:\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df_load[\"iteration\"], df_load[\"imbalance\"], \"s-\", markersize=3, alpha=0.6, color=\"purple\", label=\"Imbalance (CV)\")\n",
    "    ax2.set_ylabel(\"Imbalance Score\", color=\"purple\")\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"purple\")\n",
    "    ax2.legend(loc=\"upper right\", fontsize=8)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Load Penalty\", color=\"orange\")\n",
    "ax.tick_params(axis=\"y\", labelcolor=\"orange\")\n",
    "ax.set_title(\"Load Penalty & Imbalance Over Training\")\n",
    "ax.legend(loc=\"upper left\", fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Load state distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if \"load_state\" in df_log.columns:\n",
    "    load_state_counts = df_log[\"load_state\"].value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    load_state_counts.plot(kind=\"bar\", ax=ax, color=plt.cm.Set3(np.linspace(0, 1, len(load_state_counts))))\n",
    "    ax.set_title(\"Load State Distribution During Training\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlabel(\"Load State\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Learned Q-Table Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Q-table heatmap: balanced state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "action_labels = [f\"{a.agent_id}\\n({a.provider.value[:3]})\" for a in ASSIGNMENT_ACTIONS]\n",
    "\n",
    "# Show Q-values for the \"balanced\" load state (primary policy)\n",
    "q_matrix_balanced = np.array([coordinator.q_table[(tt, \"balanced\")] for tt in TASK_TYPES])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.heatmap(q_matrix_balanced, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "            xticklabels=action_labels, yticklabels=TASK_TYPES, ax=ax)\n",
    "ax.set_title(\"Learned Q-Table â€” load_state = 'balanced'\")\n",
    "ax.set_xlabel(\"Agent (Provider)\")\n",
    "ax.set_ylabel(\"Task Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Q-table heatmap: all load states â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Show the full (task_type Ã— load_state) Q-table to see how load states\n",
    "# shift the policy\n",
    "visited_load_states = [\"balanced\"] + [\n",
    "    ls for ls in LOAD_STATES[1:]\n",
    "    if any(np.any(coordinator.q_table[(tt, ls)] != 0) for tt in TASK_TYPES)\n",
    "]\n",
    "\n",
    "if len(visited_load_states) > 1:\n",
    "    fig, axes_q = plt.subplots(1, len(visited_load_states), figsize=(14, 5),\n",
    "                                sharey=True)\n",
    "    if len(visited_load_states) == 1:\n",
    "        axes_q = [axes_q]\n",
    "    for i, ls in enumerate(visited_load_states):\n",
    "        q_mat = np.array([coordinator.q_table[(tt, ls)] for tt in TASK_TYPES])\n",
    "        sns.heatmap(q_mat, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                    xticklabels=action_labels if i == 0 else [],\n",
    "                    yticklabels=TASK_TYPES if i == 0 else [],\n",
    "                    ax=axes_q[i], vmin=q_matrix_balanced.min(), vmax=q_matrix_balanced.max())\n",
    "        axes_q[i].set_title(f\"load: {ls}\", fontsize=10)\n",
    "        if i == 0:\n",
    "            axes_q[i].set_ylabel(\"Task Type\")\n",
    "    fig.suptitle(\"Q-Tables by Load State (only visited states shown)\", fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# â”€â”€ Best assignment per task type (balanced) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nLearned Routing Policy (load_state = 'balanced'):\")\n",
    "print(f\"{'Task Type':<14} {'Best Agent':<14} {'Provider':<12} {'Q-value':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for tt in TASK_TYPES:\n",
    "    best_idx = int(np.argmax(coordinator.q_table[(tt, \"balanced\")]))\n",
    "    best = ASSIGNMENT_ACTIONS[best_idx]\n",
    "    q_val = coordinator.q_table[(tt, \"balanced\")][best_idx]\n",
    "    print(f\"{tt:<14} {best.agent_id:<14} {best.provider.value:<12} {q_val:>8.3f}\")\n",
    "\n",
    "# â”€â”€ Policy shifts when agents are hot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nPolicy Shifts When Agents Are Overloaded:\")\n",
    "print(f\"{'Task Type':<14} {'Load State':<18} {'Best Agent':<14} {'Provider':<12} {'Q-value':>8} {'Shifted?':>10}\")\n",
    "print(\"-\" * 80)\n",
    "for tt in TASK_TYPES:\n",
    "    balanced_best = int(np.argmax(coordinator.q_table[(tt, \"balanced\")]))\n",
    "    for ls in visited_load_states:\n",
    "        best_idx = int(np.argmax(coordinator.q_table[(tt, ls)]))\n",
    "        best = ASSIGNMENT_ACTIONS[best_idx]\n",
    "        q_val = coordinator.q_table[(tt, ls)][best_idx]\n",
    "        shifted = \"â† YES\" if best_idx != balanced_best and ls != \"balanced\" else \"\"\n",
    "        print(f\"{tt:<14} {ls:<18} {best.agent_id:<14} {best.provider.value:<12} {q_val:>8.3f} {shifted:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Live Demo: Load-Aware Routing\n",
    "\n",
    "Route a new, unseen task through the learned load-aware policy. The demo shows the current load state, agent utilization, and how the coordinator factors load into its routing decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_task(description: str, task_type: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Run a single task through the trained policy (greedy, load-aware).\"\"\"\n",
    "    old_eps = coordinator.epsilon\n",
    "    coordinator.epsilon = 0.0  # greedy\n",
    "\n",
    "    load_state = load_tracker.get_load_state()\n",
    "    action_idx, assignment = coordinator.select_action(task_type, load_state)\n",
    "    role = assignment.role\n",
    "    provider = assignment.provider\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Task:       {description}\")\n",
    "        print(f\"Type:       {task_type}\")\n",
    "        print(f\"Load state: {load_state}\")\n",
    "        util = load_tracker.get_utilization()\n",
    "        print(f\"Utilization: { {k: f'{v:.0%}' for k, v in util.items()} }\")\n",
    "        print(f\"Routed ->   {assignment.agent_id} ({role.value}) via {provider.value}\")\n",
    "\n",
    "    response = llm_service.call(\n",
    "        AGENT_SYSTEM_PROMPTS[role],\n",
    "        f\"[{task_type.upper()} TASK]\\n\\n{description}\",\n",
    "        provider,\n",
    "    )\n",
    "    scores = judge.evaluate(description, task_type, role.value, response.content)\n",
    "    raw_reward = LLMJudge.composite_reward(scores, response.cost_estimate, response.latency_seconds)\n",
    "    load_penalty = load_tracker.get_load_penalty(assignment.agent_id)\n",
    "    adjusted_reward = max(0.0, raw_reward - load_penalty)\n",
    "\n",
    "    # Record in load tracker for subsequent calls\n",
    "    load_tracker.record(assignment.agent_id)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Agent Output (first 500 chars) ---\")\n",
    "        print(response.content[:500])\n",
    "        print(f\"\\n--- Judge Scores ---\")\n",
    "        print(f\"  Relevance:      {scores.relevance:.2f}\")\n",
    "        print(f\"  Accuracy:       {scores.accuracy:.2f}\")\n",
    "        print(f\"  Completeness:   {scores.completeness:.2f}\")\n",
    "        print(f\"  Agent Match:    {scores.agent_match:.2f}\")\n",
    "        print(f\"  Raw Reward:     {raw_reward:.3f}\")\n",
    "        print(f\"  Load Penalty:   {load_penalty:.3f}\")\n",
    "        print(f\"  Adj. Reward:    {adjusted_reward:.3f}\")\n",
    "        print(f\"  Reasoning:      {scores.reasoning}\")\n",
    "        print(f\"\\n  Model: {response.model} | Tokens: {response.total_tokens} | \"\n",
    "              f\"Latency: {response.latency_seconds:.2f}s | Cost: ${response.cost_estimate:.5f}\")\n",
    "\n",
    "    coordinator.epsilon = old_eps\n",
    "    return {\"response\": response.content, \"scores\": scores,\n",
    "            \"raw_reward\": raw_reward, \"load_penalty\": load_penalty,\n",
    "            \"adjusted_reward\": adjusted_reward}\n",
    "\n",
    "\n",
    "# â”€â”€ Try a fresh task â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 70)\n",
    "print(\"LIVE DEMO (load-aware routing)\")\n",
    "print(\"=\" * 70)\n",
    "demo_task(\n",
    "    \"Explain the CAP theorem and give real-world examples of systems that \"\n",
    "    \"trade off each guarantee.\",\n",
    "    task_type=\"research\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Production Integration (Load-Balanced)\n",
    "\n",
    "A self-contained class that wraps the trained **load-aware** RL coordinator for production use. The system:\n",
    "- Queries the `LoadTracker` before every routing decision\n",
    "- Applies load penalties to rewards for online learning\n",
    "- Exposes load state, utilization, and imbalance metrics via `health()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMultiAgentSystem:\n",
    "    \"\"\"Wraps the trained RL coordinator + load tracker + real LLM agents for production use.\"\"\"\n",
    "\n",
    "    def __init__(self, coordinator: QLearningCoordinator, llm_service: LLMService,\n",
    "                 judge: LLMJudge, agents: Dict[str, AgentState],\n",
    "                 load_tracker: LoadTracker,\n",
    "                 online_learning: bool = True):\n",
    "        self.coordinator = coordinator\n",
    "        self.llm_service = llm_service\n",
    "        self.judge = judge\n",
    "        self.agents = agents\n",
    "        self.load_tracker = load_tracker\n",
    "        self.online_learning = online_learning\n",
    "        # Rate limiting\n",
    "        self.request_times: deque = deque(maxlen=200)\n",
    "        self.max_rpm: int = 60\n",
    "        self.request_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _check_rate_limit(self) -> bool:\n",
    "        now = time.time()\n",
    "        self.request_times.append(now)\n",
    "        cutoff = now - 60\n",
    "        recent = sum(1 for t in self.request_times if t > cutoff)\n",
    "        return recent <= self.max_rpm\n",
    "\n",
    "    def process(self, description: str, task_type: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route a task (load-aware), execute, evaluate, optionally learn.\"\"\"\n",
    "        if not self._check_rate_limit():\n",
    "            return {\"error\": \"Rate limit exceeded\"}\n",
    "\n",
    "        # 1. Route (load-aware)\n",
    "        self.coordinator.epsilon = 0.0  # greedy in production\n",
    "        load_state = self.load_tracker.get_load_state()\n",
    "        action_idx, assignment = self.coordinator.select_action(task_type, load_state)\n",
    "        role = assignment.role\n",
    "        provider = assignment.provider\n",
    "\n",
    "        # 2. Execute\n",
    "        response = self.llm_service.call(\n",
    "            AGENT_SYSTEM_PROMPTS[role],\n",
    "            f\"[{task_type.upper()} TASK]\\n\\n{description}\",\n",
    "            provider,\n",
    "        )\n",
    "\n",
    "        # 3. Judge\n",
    "        scores = self.judge.evaluate(description, task_type, role.value, response.content)\n",
    "        raw_reward = LLMJudge.composite_reward(scores, response.cost_estimate, response.latency_seconds)\n",
    "\n",
    "        # 4. Load penalty + record\n",
    "        load_penalty = self.load_tracker.get_load_penalty(assignment.agent_id)\n",
    "        adjusted_reward = max(0.0, raw_reward - load_penalty)\n",
    "        self.load_tracker.record(assignment.agent_id)\n",
    "\n",
    "        # 5. Online learning\n",
    "        if self.online_learning:\n",
    "            next_load_state = self.load_tracker.get_load_state()\n",
    "            self.coordinator.update(task_type, load_state, action_idx,\n",
    "                                    adjusted_reward, next_task_type=None,\n",
    "                                    next_load_state=next_load_state)\n",
    "\n",
    "        result = {\n",
    "            \"agent\": assignment.agent_id,\n",
    "            \"provider\": provider.value,\n",
    "            \"model\": response.model,\n",
    "            \"response\": response.content,\n",
    "            \"tokens\": response.total_tokens,\n",
    "            \"latency\": response.latency_seconds,\n",
    "            \"cost\": response.cost_estimate,\n",
    "            \"raw_reward\": raw_reward,\n",
    "            \"load_penalty\": load_penalty,\n",
    "            \"reward\": adjusted_reward,\n",
    "            \"load_state\": load_state,\n",
    "            \"scores\": {\n",
    "                \"relevance\": scores.relevance,\n",
    "                \"accuracy\": scores.accuracy,\n",
    "                \"completeness\": scores.completeness,\n",
    "                \"agent_match\": scores.agent_match,\n",
    "            },\n",
    "        }\n",
    "        self.request_log.append(result)\n",
    "        return result\n",
    "\n",
    "    def health(self) -> Dict[str, Any]:\n",
    "        util = self.load_tracker.get_utilization()\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"total_requests\": len(self.request_log),\n",
    "            \"total_llm_calls\": self.llm_service.call_count,\n",
    "            \"total_cost_usd\": round(self.llm_service.total_cost, 5),\n",
    "            \"avg_reward\": round(np.mean([r[\"reward\"] for r in self.request_log]), 3) if self.request_log else 0.0,\n",
    "            \"current_load_state\": self.load_tracker.get_load_state(),\n",
    "            \"agent_utilization\": {k: round(v, 3) for k, v in util.items()},\n",
    "            \"imbalance_score\": round(self.load_tracker.get_imbalance_score(), 3),\n",
    "        }\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate production system â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prod = ProductionMultiAgentSystem(coordinator, llm_service, judge, AGENTS, load_tracker)\n",
    "\n",
    "# Quick test\n",
    "result = prod.process(\n",
    "    \"Write a Python function that merges two sorted linked lists into one sorted list.\",\n",
    "    \"coding\",\n",
    ")\n",
    "print(\"Production system test (load-aware):\")\n",
    "print(f\"  Agent:       {result['agent']} via {result['provider']}\")\n",
    "print(f\"  Load state:  {result['load_state']}\")\n",
    "print(f\"  Tokens:      {result['tokens']}\")\n",
    "print(f\"  Raw reward:  {result['raw_reward']:.3f}\")\n",
    "print(f\"  Load penalty:{result['load_penalty']:.3f}\")\n",
    "print(f\"  Adj. reward: {result['reward']:.3f}\")\n",
    "print(f\"  Output:      {result['response'][:200]}...\")\n",
    "print(f\"\\nHealth: {json.dumps(prod.health(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cost and Agent Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cost breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cost_summary = llm_service.get_cost_summary()\n",
    "print(\"LLM Cost Summary\")\n",
    "print(\"=\" * 40)\n",
    "for k, v in cost_summary.items():\n",
    "    print(f\"  {k:20s} {v}\")\n",
    "\n",
    "# â”€â”€ Agent performance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nAgent Performance Summary\")\n",
    "print(f\"{'Agent':<14} {'Tasks':>6} {'Avg Reward':>11} {'Total Reward':>13} {'Lifetime %':>12}\")\n",
    "print(\"-\" * 60)\n",
    "total_tasks = sum(ag.completed_tasks for ag in AGENTS.values())\n",
    "for aid, ag in AGENTS.items():\n",
    "    pct = (ag.completed_tasks / total_tasks * 100) if total_tasks > 0 else 0\n",
    "    print(f\"{aid:<14} {ag.completed_tasks:>6} {ag.avg_quality:>11.3f} {ag.total_reward:>13.3f} {pct:>11.1f}%\")\n",
    "\n",
    "# â”€â”€ Load balancing effectiveness â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nLoad Balancing Summary\")\n",
    "print(\"=\" * 40)\n",
    "fair_share_pct = 100.0 / len(AGENTS)\n",
    "print(f\"  Fair share target:  {fair_share_pct:.1f}%\")\n",
    "print(f\"  Final imbalance:    {load_tracker.get_imbalance_score():.3f} (0=perfect)\")\n",
    "print(f\"  Final load state:   {load_tracker.get_load_state()}\")\n",
    "penalties = [a.get(\"load_penalty\", 0) for a in final_state[\"assignments_log\"]]\n",
    "nonzero_penalties = [p for p in penalties if p > 0]\n",
    "print(f\"  Mean load penalty:  {np.mean(penalties):.4f}\")\n",
    "print(f\"  Penalties applied:  {len(nonzero_penalties)}/{len(penalties)} iterations\")\n",
    "if nonzero_penalties:\n",
    "    print(f\"  Avg nonzero penalty:{np.mean(nonzero_penalties):.4f}\")\n",
    "print(f\"  Lifetime counts:    {dict(load_tracker.lifetime_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook extends the base Q-Learning multi-agent workflow with **load balancing** â€” the RL coordinator now considers agent utilization when routing tasks, preventing any single agent from being over-utilized.\n",
    "\n",
    "| Component | Implementation |\n",
    "|---|---|\n",
    "| **Agent execution** | Real LangChain calls to OpenAI (gpt-4o-mini) and Anthropic (claude-3-5-haiku) |\n",
    "| **Task routing** | **Load-aware** Q-Learning: state = `(task_type, load_state)` |\n",
    "| **Load tracking** | `LoadTracker` with sliding-window per-agent utilization + hot-agent detection |\n",
    "| **Reward shaping** | Quality âˆ’ cost âˆ’ latency âˆ’ **load penalty** (penalizes over-utilized agents) |\n",
    "| **Quality evaluation** | LLM-as-Judge scores outputs on relevance, accuracy, completeness, and agent-match |\n",
    "| **Workflow orchestration** | LangGraph state machine: pick_task, assign_agent, agent_execute, judge_output, rl_update |\n",
    "| **Cost control** | Token tracking and USD cost estimation per call; configurable iteration count |\n",
    "| **Production readiness** | ProductionMultiAgentSystem with rate limiting, load-aware routing, and online learning |\n",
    "\n",
    "### How Load Balancing Works\n",
    "\n",
    "1. **LoadTracker** maintains a sliding window (default size 8) of recent agent assignments.\n",
    "2. Before each assignment, the tracker computes a **load state**:\n",
    "   - `\"balanced\"` â€” all agents within 1.5Ã— fair share\n",
    "   - `\"<agent>_hot\"` â€” that agent exceeds 1.5Ã— fair share of recent assignments\n",
    "3. The Q-Learning state is `(task_type, load_state)`, giving **20 states** (4 task types Ã— 5 load states).\n",
    "4. After each assignment, a **load penalty** is subtracted from the quality reward â€” proportional to how much the assigned agent exceeds fair share.\n",
    "5. The coordinator learns *conditional* policies: when an agent is hot, it routes tasks elsewhere (even if that agent normally produces the best quality).\n",
    "\n",
    "### Key Insights\n",
    "- The RL coordinator learns both **quality routing** (which agent+provider is best per task type) and **load balancing** (when to shift away from over-utilized agents).\n",
    "- The **160-cell Q-table** (vs 32 in the base version) remains tiny and sample-efficient â€” no neural networks or GPU needed.\n",
    "- The **load penalty** creates a qualityâ€“balance trade-off: the coordinator may accept slightly lower quality to avoid concentrating all work on one agent.\n",
    "- The **sliding window** makes the system adaptive â€” load states change as assignment patterns shift, and the coordinator reacts in real time.\n",
    "- In production, the coordinator uses the learned load-aware policy with **zero exploration** (Îµ = 0) and continues to update from live feedback."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
