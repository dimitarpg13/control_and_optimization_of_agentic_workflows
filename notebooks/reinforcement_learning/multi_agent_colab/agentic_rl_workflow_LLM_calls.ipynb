{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Multi-Agent Task Allocation with RL â€” Real LLM Calls\n",
    "\n",
    "This notebook demonstrates a **multi-agent system** where agents with specialized roles process tasks, with **Reinforcement Learning (Q-Learning)** guiding optimal task allocation â€” all backed by **real LLM API calls** via **LangChain/LangGraph**.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            MULTI-AGENT RL TASK ALLOCATION (Real LLM Calls)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚ INCOMING â”‚â”€â”€â”€â–ºâ”‚  RL COORDINATOR   â”‚â”€â”€â”€â–ºâ”‚  AGENT POOL      â”‚                â”‚\n",
    "â”‚  â”‚   TASK   â”‚    â”‚  (Q-Learning)     â”‚    â”‚                  â”‚                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                   â”‚    â”‚ ðŸ”¬ Researcher    â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ State: task type â”‚    â”‚ ðŸ“Š Analyst       â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Action: assign   â”‚    â”‚ ðŸ’» Coder         â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Reward: quality  â”‚    â”‚ âœ… Validator     â”‚                â”‚\n",
    "â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                          â”‚                        â”‚                          â”‚\n",
    "â”‚                          â”‚    LangGraph            â”‚  Real LLM Calls         â”‚\n",
    "â”‚                          â”‚    Orchestration        â”‚  (OpenAI / Anthropic)   â”‚\n",
    "â”‚                          â–¼                        â–¼                          â”‚\n",
    "â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚                 â”‚  LLM-AS-JUDGE    â”‚â—„â”€â”€â”€â”‚    AGENT OUTPUT   â”‚                â”‚\n",
    "â”‚                 â”‚                  â”‚    â”‚                  â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Quality Score  â”‚    â”‚ â€¢ Research docs  â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Relevance      â”‚    â”‚ â€¢ Analysis       â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Completeness   â”‚    â”‚ â€¢ Code           â”‚                â”‚\n",
    "â”‚                 â”‚ â€¢ Agent Match    â”‚    â”‚ â€¢ Validation     â”‚                â”‚\n",
    "â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                          â”‚                                                  â”‚\n",
    "â”‚                          â–¼                                                  â”‚\n",
    "â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚                 â”‚  RL UPDATE: Learn optimal task routing      â”‚              â”‚\n",
    "â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## Key Differences from Simulated Version\n",
    "\n",
    "| Aspect | Simulated | This Notebook |\n",
    "|--------|-----------|---------------|\n",
    "| **Agent Processing** | Mock tools returning fake data | Real LLM calls via LangChain |\n",
    "| **Task Execution** | Random success simulation | Actual LLM-generated outputs |\n",
    "| **Quality Evaluation** | Heuristic scores | LLM-as-Judge evaluation |\n",
    "| **Workflow Orchestration** | Manual loops | LangGraph state machine |\n",
    "| **RL Algorithm** | PPO (impractical with API calls) | Q-Learning (practical, sample-efficient) |\n",
    "| **Cost Tracking** | None | Real token usage and USD costs |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- `OPENAI_API_KEY` environment variable set\n",
    "- `ANTHROPIC_API_KEY` environment variable set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q numpy pandas matplotlib seaborn \\\n",
    "    langchain langchain-openai langchain-anthropic langgraph \\\n",
    "    langchain-core pydantic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, TypedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from collections import deque\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"âŒ OPENAI_API_KEY not set!\"\n",
    "assert os.environ.get(\"ANTHROPIC_API_KEY\"), \"âŒ ANTHROPIC_API_KEY not set!\"\n",
    "print(\"âœ… API keys verified\")\n",
    "print(f\"   OpenAI key:    ...{os.environ['OPENAI_API_KEY'][-6:]}\")\n",
    "print(f\"   Anthropic key: ...{os.environ['ANTHROPIC_API_KEY'][-6:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Service Layer\n",
    "\n",
    "Wraps OpenAI and Anthropic with token tracking and cost estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    content: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_seconds: float\n",
    "    cost_estimate: float\n",
    "\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Manages LLM calls with token & cost tracking.\"\"\"\n",
    "\n",
    "    PRICING = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"claude-3-5-haiku-latest\": {\"input\": 0.80, \"output\": 4.00},\n",
    "        \"claude-3-5-sonnet-latest\": {\"input\": 3.00, \"output\": 15.00},\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "        anthropic_model: str = \"claude-3-5-haiku-latest\",\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "    ):\n",
    "        self.openai_model_name = openai_model\n",
    "        self.anthropic_model_name = anthropic_model\n",
    "        self.openai_llm = ChatOpenAI(model=openai_model, temperature=temperature, max_tokens=max_tokens)\n",
    "        self.anthropic_llm = ChatAnthropic(model=anthropic_model, temperature=temperature, max_tokens=max_tokens)\n",
    "        self.total_cost = 0.0\n",
    "        self.call_count = 0\n",
    "        self.call_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _estimate_cost(self, model: str, inp: int, out: int) -> float:\n",
    "        p = self.PRICING.get(model, {\"input\": 1.0, \"output\": 3.0})\n",
    "        return (inp * p[\"input\"] + out * p[\"output\"]) / 1_000_000\n",
    "\n",
    "    def call(self, system_prompt: str, user_message: str,\n",
    "             provider: LLMProvider = LLMProvider.OPENAI) -> LLMResponse:\n",
    "        msgs = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]\n",
    "        llm = self.openai_llm if provider == LLMProvider.OPENAI else self.anthropic_llm\n",
    "        model_name = self.openai_model_name if provider == LLMProvider.OPENAI else self.anthropic_model_name\n",
    "\n",
    "        start = time.time()\n",
    "        result = llm.invoke(msgs)\n",
    "        latency = time.time() - start\n",
    "\n",
    "        usage = result.usage_metadata or {}\n",
    "        inp = usage.get(\"input_tokens\", 0)\n",
    "        out = usage.get(\"output_tokens\", 0)\n",
    "        cost = self._estimate_cost(model_name, inp, out)\n",
    "        self.total_cost += cost\n",
    "        self.call_count += 1\n",
    "        self.call_log.append({\"model\": model_name, \"input_tokens\": inp,\n",
    "                              \"output_tokens\": out, \"latency\": latency, \"cost\": cost})\n",
    "        return LLMResponse(\n",
    "            content=result.content, provider=provider.value, model=model_name,\n",
    "            input_tokens=inp, output_tokens=out, total_tokens=inp + out,\n",
    "            latency_seconds=round(latency, 3), cost_estimate=cost,\n",
    "        )\n",
    "\n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        if not self.call_log:\n",
    "            return {\"total_calls\": 0, \"total_cost\": 0.0}\n",
    "        df = pd.DataFrame(self.call_log)\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_cost_usd\": round(self.total_cost, 6),\n",
    "            \"avg_latency_s\": round(df[\"latency\"].mean(), 3),\n",
    "            \"avg_tokens\": int(df[\"input_tokens\"].mean() + df[\"output_tokens\"].mean()),\n",
    "            \"by_model\": df.groupby(\"model\")[\"cost\"].sum().to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "llm_service = LLMService(openai_model=\"gpt-4o-mini\", anthropic_model=\"claude-3-5-haiku-latest\",\n",
    "                          temperature=0.7, max_tokens=768)\n",
    "\n",
    "# Smoke test\n",
    "_r = llm_service.call(\"You are a helpful assistant.\", \"Say hello in one sentence.\", LLMProvider.OPENAI)\n",
    "print(f\"âœ… LLM Service ready â€” test: {_r.content[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Definitions and Task Model\n",
    "\n",
    "Each agent has a **specialized role** with a dedicated system prompt. Tasks are described in natural language and routed to the most suitable agent by the RL coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentRole(Enum):\n",
    "    RESEARCHER = \"researcher\"\n",
    "    ANALYST = \"analyst\"\n",
    "    CODER = \"coder\"\n",
    "    VALIDATOR = \"validator\"\n",
    "\n",
    "\n",
    "# â”€â”€ System prompts per role â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AGENT_SYSTEM_PROMPTS: Dict[AgentRole, str] = {\n",
    "    AgentRole.RESEARCHER: (\n",
    "        \"You are a meticulous research specialist. \"\n",
    "        \"Given a topic or question, provide well-structured research findings with key facts, \"\n",
    "        \"relevant context, and references where possible. \"\n",
    "        \"Focus on accuracy and breadth of coverage.\"\n",
    "    ),\n",
    "    AgentRole.ANALYST: (\n",
    "        \"You are an expert data and systems analyst. \"\n",
    "        \"Given information or a question, provide structured analysis including pros/cons, \"\n",
    "        \"trade-offs, patterns, and data-driven insights. \"\n",
    "        \"Use clear reasoning and quantitative arguments where possible.\"\n",
    "    ),\n",
    "    AgentRole.CODER: (\n",
    "        \"You are a senior software engineer. \"\n",
    "        \"Given a programming task, produce clean, well-documented code with explanations. \"\n",
    "        \"Include error handling, type hints, and follow best practices.\"\n",
    "    ),\n",
    "    AgentRole.VALIDATOR: (\n",
    "        \"You are a quality assurance specialist. \"\n",
    "        \"Given a piece of work (research, analysis, or code), verify its correctness, \"\n",
    "        \"identify errors or gaps, and provide an improvement checklist. \"\n",
    "        \"Be thorough and constructive.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"A task to be processed by the multi-agent system.\"\"\"\n",
    "    id: str\n",
    "    description: str\n",
    "    task_type: str           # research | analysis | coding | validation\n",
    "    complexity: float        # 0-1\n",
    "    priority: float          # 0-1\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Tracks an individual agent's state.\"\"\"\n",
    "    id: str\n",
    "    role: AgentRole\n",
    "    provider: LLMProvider\n",
    "    completed_tasks: int = 0\n",
    "    total_reward: float = 0.0\n",
    "    avg_quality: float = 0.0\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate agent pool â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AGENTS: Dict[str, AgentState] = {\n",
    "    \"researcher\": AgentState(\"researcher\", AgentRole.RESEARCHER, LLMProvider.OPENAI),\n",
    "    \"analyst\":    AgentState(\"analyst\",    AgentRole.ANALYST,    LLMProvider.OPENAI),\n",
    "    \"coder\":      AgentState(\"coder\",      AgentRole.CODER,      LLMProvider.ANTHROPIC),\n",
    "    \"validator\":  AgentState(\"validator\",  AgentRole.VALIDATOR,  LLMProvider.OPENAI),\n",
    "}\n",
    "\n",
    "# â”€â”€ Task bank â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TASK_BANK: List[Task] = [\n",
    "    # Research tasks\n",
    "    Task(\"r1\", \"Research the current state of quantum computing and its practical applications.\", \"research\", 0.7, 0.8),\n",
    "    Task(\"r2\", \"Summarize the key differences between supervised, unsupervised, and reinforcement learning.\", \"research\", 0.5, 0.6),\n",
    "    Task(\"r3\", \"What are the main approaches to federated learning and their trade-offs?\", \"research\", 0.6, 0.7),\n",
    "    # Analysis tasks\n",
    "    Task(\"a1\", \"Analyze the trade-offs between microservices and monolithic architecture for a startup.\", \"analysis\", 0.6, 0.7),\n",
    "    Task(\"a2\", \"Compare REST, GraphQL, and gRPC for inter-service communication.\", \"analysis\", 0.5, 0.8),\n",
    "    Task(\"a3\", \"Evaluate the pros and cons of event-driven vs request-driven architectures.\", \"analysis\", 0.7, 0.6),\n",
    "    # Coding tasks\n",
    "    Task(\"c1\", \"Write a Python class that implements a thread-safe LRU cache.\", \"coding\", 0.6, 0.9),\n",
    "    Task(\"c2\", \"Implement a Python decorator that retries a function with exponential backoff.\", \"coding\", 0.5, 0.7),\n",
    "    Task(\"c3\", \"Write a Python async producer-consumer pattern using asyncio queues.\", \"coding\", 0.7, 0.8),\n",
    "    # Validation tasks\n",
    "    Task(\"v1\", \"Review and validate the correctness of this claim: 'TCP guarantees in-order delivery while UDP does not.'\", \"validation\", 0.3, 0.5),\n",
    "    Task(\"v2\", \"Validate the statement: 'B-trees are always more efficient than hash indexes for database lookups.'\", \"validation\", 0.4, 0.6),\n",
    "]\n",
    "\n",
    "print(f\"âœ… Agent pool: {len(AGENTS)} agents\")\n",
    "for aid, a in AGENTS.items():\n",
    "    print(f\"   {a.role.value:12s} â†’ {a.provider.value}\")\n",
    "print(f\"âœ… Task bank:  {len(TASK_BANK)} tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-as-Judge Reward Model\n",
    "\n",
    "A separate LLM call evaluates the quality of agent outputs, producing a scalar reward for the RL coordinator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScores(BaseModel):\n",
    "    relevance: float = Field(ge=0, le=1, description=\"How relevant the output is to the task\")\n",
    "    accuracy: float = Field(ge=0, le=1, description=\"Factual / logical accuracy\")\n",
    "    completeness: float = Field(ge=0, le=1, description=\"How thoroughly the task is addressed\")\n",
    "    agent_match: float = Field(ge=0, le=1, description=\"How well the assigned agent role fits the task\")\n",
    "    reasoning: str = Field(description=\"Brief justification\")\n",
    "\n",
    "\n",
    "JUDGE_SYSTEM = \"\"\"You are an expert evaluator of AI agent outputs.\n",
    "You will receive:\n",
    "- The original task description and type\n",
    "- The agent role that processed it\n",
    "- The agent's output\n",
    "\n",
    "Score the output on each dimension from 0.0 to 1.0.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{\n",
    "  \"relevance\": <float>,\n",
    "  \"accuracy\": <float>,\n",
    "  \"completeness\": <float>,\n",
    "  \"agent_match\": <float>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}\n",
    "\n",
    "Be critical. 1.0 means perfect.\"\"\"\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    def __init__(self, llm_service: LLMService, provider: LLMProvider = LLMProvider.OPENAI):\n",
    "        self.llm_service = llm_service\n",
    "        self.provider = provider\n",
    "        self.log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def evaluate(self, task_desc: str, task_type: str, agent_role: str, output: str) -> QualityScores:\n",
    "        msg = (\n",
    "            f\"## Task\\n{task_desc}\\n\\n\"\n",
    "            f\"## Task Type\\n{task_type}\\n\\n\"\n",
    "            f\"## Agent Role\\n{agent_role}\\n\\n\"\n",
    "            f\"## Agent Output\\n{output[:2000]}\\n\\n\"\n",
    "            f\"Evaluate now.\"\n",
    "        )\n",
    "        resp = self.llm_service.call(JUDGE_SYSTEM, msg, self.provider)\n",
    "        try:\n",
    "            raw = resp.content.strip()\n",
    "            if raw.startswith(\"```\"):\n",
    "                raw = raw.split(\"```\")[1]\n",
    "                if raw.startswith(\"json\"):\n",
    "                    raw = raw[4:]\n",
    "            scores = QualityScores(**json.loads(raw))\n",
    "        except Exception:\n",
    "            scores = QualityScores(relevance=0.5, accuracy=0.5, completeness=0.5,\n",
    "                                   agent_match=0.5, reasoning=\"Parse error; defaults used.\")\n",
    "        self.log.append({\n",
    "            \"task_type\": task_type, \"agent_role\": agent_role,\n",
    "            \"relevance\": scores.relevance, \"accuracy\": scores.accuracy,\n",
    "            \"completeness\": scores.completeness, \"agent_match\": scores.agent_match,\n",
    "        })\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def composite_reward(scores: QualityScores, cost: float, latency: float,\n",
    "                         cost_w: float = 0.10, latency_w: float = 0.05) -> float:\n",
    "        quality = np.mean([scores.relevance, scores.accuracy,\n",
    "                           scores.completeness, scores.agent_match])\n",
    "        norm_cost = min(cost / 0.002, 1.0)\n",
    "        norm_lat = min(latency / 5.0, 1.0)\n",
    "        return float(np.clip(quality - cost_w * norm_cost - latency_w * norm_lat, 0, 1))\n",
    "\n",
    "\n",
    "judge = LLMJudge(llm_service)\n",
    "print(\"âœ… LLM-as-Judge ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Q-Learning RL Coordinator\n",
    "\n",
    "The coordinator learns which **agent** (role + provider) should handle each **task type** to maximise quality while minimising cost and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAssignment:\n",
    "    \"\"\"Represents an RL action: which agent (role) and provider to assign a task to.\"\"\"\n",
    "    def __init__(self, agent_id: str, role: AgentRole, provider: LLMProvider):\n",
    "        self.agent_id = agent_id\n",
    "        self.role = role\n",
    "        self.provider = provider\n",
    "\n",
    "    def key(self) -> str:\n",
    "        return f\"{self.agent_id}_{self.provider.value}\"\n",
    "\n",
    "\n",
    "# All possible agent-provider assignments\n",
    "ASSIGNMENT_ACTIONS: List[TaskAssignment] = []\n",
    "for aid, agent in AGENTS.items():\n",
    "    for prov in LLMProvider:\n",
    "        ASSIGNMENT_ACTIONS.append(TaskAssignment(aid, agent.role, prov))\n",
    "\n",
    "TASK_TYPES = sorted(set(t.task_type for t in TASK_BANK))\n",
    "\n",
    "print(f\"State space (task types): {TASK_TYPES}\")\n",
    "print(f\"Action space ({len(ASSIGNMENT_ACTIONS)} assignments):\")\n",
    "for a in ASSIGNMENT_ACTIONS:\n",
    "    print(f\"  {a.agent_id:12s} via {a.provider.value}\")\n",
    "\n",
    "\n",
    "class QLearningCoordinator:\n",
    "    \"\"\"Q-Learning agent for task -> agent+provider routing.\"\"\"\n",
    "\n",
    "    def __init__(self, task_types: List[str], actions: List[TaskAssignment],\n",
    "                 lr: float = 0.15, gamma: float = 0.95, epsilon: float = 1.0,\n",
    "                 epsilon_min: float = 0.05, epsilon_decay: float = 0.97):\n",
    "        self.task_types = task_types\n",
    "        self.actions = actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Q-table: task_type -> action_idx -> Q-value\n",
    "        self.q_table: Dict[str, np.ndarray] = {\n",
    "            tt: np.zeros(len(actions)) for tt in task_types\n",
    "        }\n",
    "        self.history: List[Dict[str, Any]] = []\n",
    "\n",
    "    def select_action(self, task_type: str) -> Tuple[int, TaskAssignment]:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            idx = np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            idx = int(np.argmax(self.q_table[task_type]))\n",
    "        return idx, self.actions[idx]\n",
    "\n",
    "    def update(self, task_type: str, action_idx: int, reward: float,\n",
    "               next_task_type: Optional[str] = None):\n",
    "        old_q = self.q_table[task_type][action_idx]\n",
    "        if next_task_type is not None:\n",
    "            max_next = np.max(self.q_table[next_task_type])\n",
    "        else:\n",
    "            max_next = 0.0\n",
    "        td_target = reward + self.gamma * max_next\n",
    "        self.q_table[task_type][action_idx] += self.lr * (td_target - old_q)\n",
    "\n",
    "        self.history.append({\n",
    "            \"task_type\": task_type,\n",
    "            \"action_idx\": action_idx,\n",
    "            \"agent\": self.actions[action_idx].agent_id,\n",
    "            \"provider\": self.actions[action_idx].provider.value,\n",
    "            \"reward\": reward,\n",
    "            \"q_value\": self.q_table[task_type][action_idx],\n",
    "        })\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "coordinator = QLearningCoordinator(TASK_TYPES, ASSIGNMENT_ACTIONS)\n",
    "print(f\"\\nâœ… Q-Learning Coordinator: {len(TASK_TYPES)} states x {len(ASSIGNMENT_ACTIONS)} actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LangGraph Multi-Agent Workflow\n",
    "\n",
    "The training loop is orchestrated as a **LangGraph state machine** with five nodes:\n",
    "\n",
    "```\n",
    "START --> pick_task --> assign_agent --> agent_execute --> judge_output --> rl_update --> (loop or END)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowState(TypedDict):\n",
    "    \"\"\"LangGraph state for the multi-agent RL training loop.\"\"\"\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    task: Optional[Dict[str, Any]]\n",
    "    assignment: Optional[Dict[str, Any]]\n",
    "    action_idx: int\n",
    "    agent_output: str\n",
    "    llm_response_meta: Optional[Dict[str, Any]]\n",
    "    scores: Optional[Dict[str, float]]\n",
    "    reward: float\n",
    "    # Accumulated metrics\n",
    "    rewards_log: List[float]\n",
    "    scores_log: List[Dict[str, Any]]\n",
    "    assignments_log: List[Dict[str, Any]]\n",
    "\n",
    "\n",
    "# â”€â”€ Node functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def pick_task(state: WorkflowState) -> dict:\n",
    "    \"\"\"Randomly sample a task from the task bank.\"\"\"\n",
    "    task = random.choice(TASK_BANK)\n",
    "    return {\n",
    "        \"task\": {\n",
    "            \"id\": task.id, \"description\": task.description,\n",
    "            \"task_type\": task.task_type, \"complexity\": task.complexity,\n",
    "            \"priority\": task.priority,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def assign_agent(state: WorkflowState) -> dict:\n",
    "    \"\"\"Use the Q-Learning coordinator to choose an agent + provider.\"\"\"\n",
    "    task_type = state[\"task\"][\"task_type\"]\n",
    "    action_idx, assignment = coordinator.select_action(task_type)\n",
    "    return {\n",
    "        \"assignment\": {\n",
    "            \"agent_id\": assignment.agent_id,\n",
    "            \"role\": assignment.role.value,\n",
    "            \"provider\": assignment.provider.value,\n",
    "        },\n",
    "        \"action_idx\": action_idx,\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_execute(state: WorkflowState) -> dict:\n",
    "    \"\"\"Execute the task with a real LLM call through the assigned agent.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    assign = state[\"assignment\"]\n",
    "\n",
    "    role = AgentRole(assign[\"role\"])\n",
    "    provider = LLMProvider(assign[\"provider\"])\n",
    "\n",
    "    system_prompt = AGENT_SYSTEM_PROMPTS[role]\n",
    "    user_message = (\n",
    "        f\"[{task['task_type'].upper()} TASK | complexity={task['complexity']:.1f} | \"\n",
    "        f\"priority={task['priority']:.1f}]\\n\\n{task['description']}\"\n",
    "    )\n",
    "\n",
    "    response = llm_service.call(system_prompt, user_message, provider)\n",
    "\n",
    "    return {\n",
    "        \"agent_output\": response.content,\n",
    "        \"llm_response_meta\": {\n",
    "            \"provider\": response.provider,\n",
    "            \"model\": response.model,\n",
    "            \"tokens\": response.total_tokens,\n",
    "            \"latency\": response.latency_seconds,\n",
    "            \"cost\": response.cost_estimate,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def judge_output(state: WorkflowState) -> dict:\n",
    "    \"\"\"Use the LLM-as-Judge to score the agent output.\"\"\"\n",
    "    task = state[\"task\"]\n",
    "    assign = state[\"assignment\"]\n",
    "    scores = judge.evaluate(\n",
    "        task_desc=task[\"description\"],\n",
    "        task_type=task[\"task_type\"],\n",
    "        agent_role=assign[\"role\"],\n",
    "        output=state[\"agent_output\"],\n",
    "    )\n",
    "    score_dict = {\n",
    "        \"relevance\": scores.relevance,\n",
    "        \"accuracy\": scores.accuracy,\n",
    "        \"completeness\": scores.completeness,\n",
    "        \"agent_match\": scores.agent_match,\n",
    "        \"reasoning\": scores.reasoning,\n",
    "    }\n",
    "    meta = state[\"llm_response_meta\"]\n",
    "    reward = LLMJudge.composite_reward(scores, meta[\"cost\"], meta[\"latency\"])\n",
    "    return {\"scores\": score_dict, \"reward\": reward}\n",
    "\n",
    "\n",
    "def rl_update(state: WorkflowState) -> dict:\n",
    "    \"\"\"Update Q-table and log metrics.\"\"\"\n",
    "    task_type = state[\"task\"][\"task_type\"]\n",
    "    action_idx = state[\"action_idx\"]\n",
    "    reward = state[\"reward\"]\n",
    "\n",
    "    # Peek at next task type for bootstrapping (random)\n",
    "    next_type = random.choice(TASK_TYPES)\n",
    "    coordinator.update(task_type, action_idx, reward, next_type)\n",
    "    coordinator.decay_epsilon()\n",
    "\n",
    "    # Update agent-level stats\n",
    "    aid = state[\"assignment\"][\"agent_id\"]\n",
    "    ag = AGENTS[aid]\n",
    "    ag.completed_tasks += 1\n",
    "    ag.total_reward += reward\n",
    "    ag.avg_quality = ag.total_reward / ag.completed_tasks\n",
    "\n",
    "    # Build new lists (idiomatic LangGraph pattern â€” return new values, never mutate)\n",
    "    prev_rewards = state.get(\"rewards_log\", []) or []\n",
    "    prev_scores = state.get(\"scores_log\", []) or []\n",
    "    prev_assignments = state.get(\"assignments_log\", []) or []\n",
    "\n",
    "    it = state[\"iteration\"] + 1\n",
    "    print(f\"   [{it}/{state['max_iterations']}] reward={reward:.3f} \"\n",
    "          f\"agent={aid} provider={state['assignment']['provider']}\")\n",
    "\n",
    "    return {\n",
    "        \"rewards_log\": prev_rewards + [reward],\n",
    "        \"scores_log\": prev_scores + [state[\"scores\"]],\n",
    "        \"assignments_log\": prev_assignments + [{\n",
    "            \"iteration\": state[\"iteration\"],\n",
    "            \"task_type\": task_type,\n",
    "            \"agent\": aid,\n",
    "            \"provider\": state[\"assignment\"][\"provider\"],\n",
    "            \"reward\": reward,\n",
    "            **{k: v for k, v in state[\"scores\"].items() if k != \"reasoning\"},\n",
    "        }],\n",
    "        \"iteration\": it,\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: WorkflowState) -> str:\n",
    "    if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "        return END\n",
    "    return \"pick_task\"\n",
    "\n",
    "\n",
    "# â”€â”€ Build the graph â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "workflow.add_node(\"pick_task\", pick_task)\n",
    "workflow.add_node(\"assign_agent\", assign_agent)\n",
    "workflow.add_node(\"agent_execute\", agent_execute)\n",
    "workflow.add_node(\"judge_output\", judge_output)\n",
    "workflow.add_node(\"rl_update\", rl_update)\n",
    "\n",
    "workflow.add_edge(START, \"pick_task\")\n",
    "workflow.add_edge(\"pick_task\", \"assign_agent\")\n",
    "workflow.add_edge(\"assign_agent\", \"agent_execute\")\n",
    "workflow.add_edge(\"agent_execute\", \"judge_output\")\n",
    "workflow.add_edge(\"judge_output\", \"rl_update\")\n",
    "workflow.add_conditional_edges(\"rl_update\", should_continue, {END: END, \"pick_task\": \"pick_task\"})\n",
    "\n",
    "app = workflow.compile()\n",
    "print(\"âœ… LangGraph workflow compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Run\n",
    "\n",
    "Each iteration makes 2 LLM calls (agent + judge). With 30 iterations that is approximately 60 API calls.\n",
    "\n",
    "Estimated cost: less than $0.10 with gpt-4o-mini / claude-3-5-haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITERATIONS = 30  # Increase for better convergence (at higher cost)\n",
    "\n",
    "initial_state: WorkflowState = {\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": N_ITERATIONS,\n",
    "    \"task\": None,\n",
    "    \"assignment\": None,\n",
    "    \"action_idx\": 0,\n",
    "    \"agent_output\": \"\",\n",
    "    \"llm_response_meta\": None,\n",
    "    \"scores\": None,\n",
    "    \"reward\": 0.0,\n",
    "    \"rewards_log\": [],\n",
    "    \"scores_log\": [],\n",
    "    \"assignments_log\": [],\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {N_ITERATIONS} iterations...\")\n",
    "print(f\"Initial epsilon: {coordinator.epsilon:.2f}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_state = app.invoke(initial_state)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training complete in {elapsed:.1f}s\")\n",
    "print(f\"  Iterations:        {final_state['iteration']}\")\n",
    "print(f\"  Mean reward:       {np.mean(final_state['rewards_log']):.3f}\")\n",
    "print(f\"  Final epsilon:     {coordinator.epsilon:.4f}\")\n",
    "print(f\"  Total LLM calls:   {llm_service.call_count}\")\n",
    "print(f\"  Total cost (USD):  ${llm_service.total_cost:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = pd.DataFrame(final_state[\"assignments_log\"])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle(\"Multi-Agent RL Training Dashboard\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# â”€â”€ 1. Reward curve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0, 0]\n",
    "rewards = final_state[\"rewards_log\"]\n",
    "ax.plot(rewards, \"o-\", alpha=0.5, markersize=4, label=\"Per-iteration\")\n",
    "window = min(5, len(rewards))\n",
    "if len(rewards) >= window:\n",
    "    rolling = pd.Series(rewards).rolling(window).mean()\n",
    "    ax.plot(rolling, \"r-\", linewidth=2, label=f\"{window}-step MA\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Reward\")\n",
    "ax.set_title(\"Reward Over Training\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€ 2. Quality score breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[0, 1]\n",
    "score_cols = [\"relevance\", \"accuracy\", \"completeness\", \"agent_match\"]\n",
    "for col in score_cols:\n",
    "    ax.plot(df_log[col], \"o-\", markersize=3, alpha=0.6, label=col)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Judge Quality Scores\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€ 3. Agent utilisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1, 0]\n",
    "usage = df_log.groupby([\"agent\", \"provider\"]).size().reset_index(name=\"count\")\n",
    "bars = [f\"{r.agent}\\n({r.provider})\" for _, r in usage.iterrows()]\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(bars)))\n",
    "ax.bar(bars, usage[\"count\"], color=colors)\n",
    "ax.set_title(\"Agent-Provider Utilisation\")\n",
    "ax.set_ylabel(\"Tasks Assigned\")\n",
    "\n",
    "# â”€â”€ 4. Avg reward by task type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ax = axes[1, 1]\n",
    "avg_by_type = df_log.groupby(\"task_type\")[\"reward\"].mean().sort_values()\n",
    "avg_by_type.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "ax.set_xlabel(\"Mean Reward\")\n",
    "ax.set_title(\"Mean Reward by Task Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Learned Q-Table Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Q-table heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "action_labels = [f\"{a.agent_id}\\n({a.provider.value[:3]})\" for a in ASSIGNMENT_ACTIONS]\n",
    "q_matrix = np.array([coordinator.q_table[tt] for tt in TASK_TYPES])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.heatmap(q_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "            xticklabels=action_labels, yticklabels=TASK_TYPES, ax=ax)\n",
    "ax.set_title(\"Learned Q-Table  (task type x agent-provider)\")\n",
    "ax.set_xlabel(\"Agent (Provider)\")\n",
    "ax.set_ylabel(\"Task Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Best assignment per task type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nLearned Routing Policy:\")\n",
    "print(f\"{'Task Type':<14} {'Best Agent':<14} {'Provider':<12} {'Q-value':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for tt in TASK_TYPES:\n",
    "    best_idx = int(np.argmax(coordinator.q_table[tt]))\n",
    "    best = ASSIGNMENT_ACTIONS[best_idx]\n",
    "    q_val = coordinator.q_table[tt][best_idx]\n",
    "    print(f\"{tt:<14} {best.agent_id:<14} {best.provider.value:<12} {q_val:>8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Live Demo: Using the Trained Policy\n",
    "\n",
    "Route a new, unseen task through the learned policy and show the full pipeline output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_task(description: str, task_type: str, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Run a single task through the trained policy (greedy, no exploration).\"\"\"\n",
    "    old_eps = coordinator.epsilon\n",
    "    coordinator.epsilon = 0.0  # greedy\n",
    "\n",
    "    action_idx, assignment = coordinator.select_action(task_type)\n",
    "    role = assignment.role\n",
    "    provider = assignment.provider\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Task:     {description}\")\n",
    "        print(f\"Type:     {task_type}\")\n",
    "        print(f\"Routed -> {assignment.agent_id} ({role.value}) via {provider.value}\")\n",
    "\n",
    "    response = llm_service.call(\n",
    "        AGENT_SYSTEM_PROMPTS[role],\n",
    "        f\"[{task_type.upper()} TASK]\\n\\n{description}\",\n",
    "        provider,\n",
    "    )\n",
    "    scores = judge.evaluate(description, task_type, role.value, response.content)\n",
    "    reward = LLMJudge.composite_reward(scores, response.cost_estimate, response.latency_seconds)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n--- Agent Output (first 500 chars) ---\")\n",
    "        print(response.content[:500])\n",
    "        print(f\"\\n--- Judge Scores ---\")\n",
    "        print(f\"  Relevance:     {scores.relevance:.2f}\")\n",
    "        print(f\"  Accuracy:      {scores.accuracy:.2f}\")\n",
    "        print(f\"  Completeness:  {scores.completeness:.2f}\")\n",
    "        print(f\"  Agent Match:   {scores.agent_match:.2f}\")\n",
    "        print(f\"  Reward:        {reward:.3f}\")\n",
    "        print(f\"  Reasoning:     {scores.reasoning}\")\n",
    "        print(f\"\\n  Model: {response.model} | Tokens: {response.total_tokens} | \"\n",
    "              f\"Latency: {response.latency_seconds:.2f}s | Cost: ${response.cost_estimate:.5f}\")\n",
    "\n",
    "    coordinator.epsilon = old_eps\n",
    "    return {\"response\": response.content, \"scores\": scores, \"reward\": reward}\n",
    "\n",
    "\n",
    "# â”€â”€ Try a fresh task â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 70)\n",
    "print(\"LIVE DEMO\")\n",
    "print(\"=\" * 70)\n",
    "demo_task(\n",
    "    \"Explain the CAP theorem and give real-world examples of systems that \"\n",
    "    \"trade off each guarantee.\",\n",
    "    task_type=\"research\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Production Integration\n",
    "\n",
    "A self-contained class that wraps the trained RL coordinator for use in a production service. Supports online learning by feeding live judge scores back into the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMultiAgentSystem:\n",
    "    \"\"\"Wraps the trained RL coordinator + real LLM agents for production use.\"\"\"\n",
    "\n",
    "    def __init__(self, coordinator: QLearningCoordinator, llm_service: LLMService,\n",
    "                 judge: LLMJudge, agents: Dict[str, AgentState],\n",
    "                 online_learning: bool = True):\n",
    "        self.coordinator = coordinator\n",
    "        self.llm_service = llm_service\n",
    "        self.judge = judge\n",
    "        self.agents = agents\n",
    "        self.online_learning = online_learning\n",
    "        # Rate limiting\n",
    "        self.request_times: deque = deque(maxlen=200)\n",
    "        self.max_rpm: int = 60\n",
    "        self.request_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _check_rate_limit(self) -> bool:\n",
    "        now = time.time()\n",
    "        self.request_times.append(now)\n",
    "        cutoff = now - 60\n",
    "        recent = sum(1 for t in self.request_times if t > cutoff)\n",
    "        return recent <= self.max_rpm\n",
    "\n",
    "    def process(self, description: str, task_type: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route a task, execute, evaluate, optionally learn.\"\"\"\n",
    "        if not self._check_rate_limit():\n",
    "            return {\"error\": \"Rate limit exceeded\"}\n",
    "\n",
    "        # 1. Route\n",
    "        self.coordinator.epsilon = 0.0  # greedy in production\n",
    "        action_idx, assignment = self.coordinator.select_action(task_type)\n",
    "        role = assignment.role\n",
    "        provider = assignment.provider\n",
    "\n",
    "        # 2. Execute\n",
    "        response = self.llm_service.call(\n",
    "            AGENT_SYSTEM_PROMPTS[role],\n",
    "            f\"[{task_type.upper()} TASK]\\n\\n{description}\",\n",
    "            provider,\n",
    "        )\n",
    "\n",
    "        # 3. Judge\n",
    "        scores = self.judge.evaluate(description, task_type, role.value, response.content)\n",
    "        reward = LLMJudge.composite_reward(scores, response.cost_estimate, response.latency_seconds)\n",
    "\n",
    "        # 4. Online learning\n",
    "        if self.online_learning:\n",
    "            self.coordinator.update(task_type, action_idx, reward)\n",
    "\n",
    "        result = {\n",
    "            \"agent\": assignment.agent_id,\n",
    "            \"provider\": provider.value,\n",
    "            \"model\": response.model,\n",
    "            \"response\": response.content,\n",
    "            \"tokens\": response.total_tokens,\n",
    "            \"latency\": response.latency_seconds,\n",
    "            \"cost\": response.cost_estimate,\n",
    "            \"reward\": reward,\n",
    "            \"scores\": {\n",
    "                \"relevance\": scores.relevance,\n",
    "                \"accuracy\": scores.accuracy,\n",
    "                \"completeness\": scores.completeness,\n",
    "                \"agent_match\": scores.agent_match,\n",
    "            },\n",
    "        }\n",
    "        self.request_log.append(result)\n",
    "        return result\n",
    "\n",
    "    def health(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"total_requests\": len(self.request_log),\n",
    "            \"total_llm_calls\": self.llm_service.call_count,\n",
    "            \"total_cost_usd\": round(self.llm_service.total_cost, 5),\n",
    "            \"avg_reward\": round(np.mean([r[\"reward\"] for r in self.request_log]), 3) if self.request_log else 0.0,\n",
    "        }\n",
    "\n",
    "\n",
    "# â”€â”€ Instantiate production system â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prod = ProductionMultiAgentSystem(coordinator, llm_service, judge, AGENTS)\n",
    "\n",
    "# Quick test\n",
    "result = prod.process(\n",
    "    \"Write a Python function that merges two sorted linked lists into one sorted list.\",\n",
    "    \"coding\",\n",
    ")\n",
    "print(\"Production system test:\")\n",
    "print(f\"  Agent:    {result['agent']} via {result['provider']}\")\n",
    "print(f\"  Tokens:   {result['tokens']}\")\n",
    "print(f\"  Reward:   {result['reward']:.3f}\")\n",
    "print(f\"  Output:   {result['response'][:200]}...\")\n",
    "print(f\"\\nHealth: {json.dumps(prod.health(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cost and Agent Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Cost breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cost_summary = llm_service.get_cost_summary()\n",
    "print(\"LLM Cost Summary\")\n",
    "print(\"=\" * 40)\n",
    "for k, v in cost_summary.items():\n",
    "    print(f\"  {k:20s} {v}\")\n",
    "\n",
    "# â”€â”€ Agent performance â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nAgent Performance Summary\")\n",
    "print(f\"{'Agent':<14} {'Tasks':>6} {'Avg Reward':>11} {'Total Reward':>13}\")\n",
    "print(\"-\" * 48)\n",
    "for aid, ag in AGENTS.items():\n",
    "    print(f\"{aid:<14} {ag.completed_tasks:>6} {ag.avg_quality:>11.3f} {ag.total_reward:>13.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook replaced the **simulated multi-agent environment** from the original notebook with **real LLM API calls** orchestrated by **LangGraph**:\n",
    "\n",
    "| Component | Implementation |\n",
    "|---|---|\n",
    "| **Agent execution** | Real LangChain calls to OpenAI (gpt-4o-mini) and Anthropic (claude-3-5-haiku) |\n",
    "| **Task routing** | Q-Learning coordinator learns optimal task-type to agent+provider mapping |\n",
    "| **Quality evaluation** | LLM-as-Judge scores outputs on relevance, accuracy, completeness, and agent-match |\n",
    "| **Workflow orchestration** | LangGraph state machine: pick_task, assign_agent, agent_execute, judge_output, rl_update |\n",
    "| **Cost control** | Token tracking and USD cost estimation per call; configurable iteration count |\n",
    "| **Production readiness** | ProductionMultiAgentSystem class with rate limiting and online learning |\n",
    "\n",
    "### Key insights\n",
    "- The RL coordinator learns which **agent role + LLM provider** combination yields the highest quality for each task type.\n",
    "- The **LLM-as-Judge** provides consistent, multi-dimensional evaluation without human labelling.\n",
    "- **LangGraph** cleanly separates concerns into nodes, making the pipeline easy to extend (e.g., add a reviewer agent, human-in-the-loop, or tool-augmented agents).\n",
    "- Running with budget-friendly models (gpt-4o-mini, claude-3-5-haiku) keeps total training cost minimal while demonstrating the full workflow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
