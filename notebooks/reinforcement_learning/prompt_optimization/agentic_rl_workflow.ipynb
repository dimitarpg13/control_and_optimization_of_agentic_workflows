{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ Reinforcement Learning for Prompt Optimization in Agentic Workflows\n",
        "\n",
        "This notebook demonstrates how to use **Reinforcement Learning (RL)** to dynamically optimize prompts based on user feedback, task performance, and cost efficiency.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚              RL-BASED PROMPT OPTIMIZATION SYSTEM                             â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                              â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
        "â”‚  â”‚  User   â”‚â”€â”€â”€â–ºâ”‚  PROMPT AGENT    â”‚â”€â”€â”€â–ºâ”‚   LLM SERVICE    â”‚                â”‚\n",
        "â”‚  â”‚  Query  â”‚    â”‚                  â”‚    â”‚                  â”‚                â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Select Templateâ”‚    â”‚ â€¢ Generate       â”‚                â”‚\n",
        "â”‚       â”‚         â”‚ â€¢ Add Examples   â”‚    â”‚   Response       â”‚                â”‚\n",
        "â”‚       â”‚         â”‚ â€¢ Set Parameters â”‚    â”‚                  â”‚                â”‚\n",
        "â”‚       â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
        "â”‚       â”‚                  â”‚                       â”‚                          â”‚\n",
        "â”‚       â”‚                  â–¼                       â–¼                          â”‚\n",
        "â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
        "â”‚       â”‚         â”‚  RL OPTIMIZER    â”‚â—„â”€â”€â”€â”‚    RESPONSE      â”‚                â”‚\n",
        "â”‚       â”‚         â”‚                  â”‚    â”‚                  â”‚                â”‚\n",
        "â”‚       â”‚         â”‚ â€¢ Q-Learning     â”‚    â”‚ â€¢ Quality Score  â”‚                â”‚\n",
        "â”‚       â”‚         â”‚ â€¢ Multi-Armed    â”‚    â”‚ â€¢ User Feedback  â”‚                â”‚\n",
        "â”‚       â”‚         â”‚   Bandit         â”‚    â”‚ â€¢ Task Success   â”‚                â”‚\n",
        "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â€¢ Policy Grad    â”‚    â”‚ â€¢ Cost Metric    â”‚                â”‚\n",
        "â”‚   Feedback      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
        "â”‚                          â”‚                                                  â”‚\n",
        "â”‚                          â–¼                                                  â”‚\n",
        "â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚                 â”‚         OPTIMIZED PROMPTS                   â”‚              â”‚\n",
        "â”‚                 â”‚  Personalized, Efficient, High-Quality      â”‚              â”‚\n",
        "â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **Multi-Armed Bandit** for prompt template selection\n",
        "2. **Q-Learning** for dynamic prompt assembly\n",
        "3. **Contextual Bandits** for personalized prompts\n",
        "4. **Policy Gradients** for continuous prompt optimization\n",
        "5. **RLHF-style** reward modeling from user feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q numpy pandas matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from abc import ABC, abstractmethod\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Prompt Components and Templates\n",
        "\n",
        "We define the building blocks of prompts that can be optimized through RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QueryType(Enum):\n",
        "    \"\"\"Types of user queries that require different prompt strategies.\"\"\"\n",
        "    FACTUAL = \"factual\"           # Questions with definitive answers\n",
        "    CREATIVE = \"creative\"         # Open-ended creative tasks\n",
        "    ANALYTICAL = \"analytical\"     # Data analysis and reasoning\n",
        "    CODING = \"coding\"             # Code generation tasks\n",
        "    CONVERSATIONAL = \"conversational\"  # Chat-style interactions\n",
        "\n",
        "\n",
        "class ToneStyle(Enum):\n",
        "    \"\"\"Tone options for prompt responses.\"\"\"\n",
        "    FORMAL = \"formal\"\n",
        "    CASUAL = \"casual\"\n",
        "    TECHNICAL = \"technical\"\n",
        "    FRIENDLY = \"friendly\"\n",
        "\n",
        "\n",
        "class DetailLevel(Enum):\n",
        "    \"\"\"Level of detail in responses.\"\"\"\n",
        "    BRIEF = \"brief\"\n",
        "    MODERATE = \"moderate\"\n",
        "    DETAILED = \"detailed\"\n",
        "    COMPREHENSIVE = \"comprehensive\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PromptTemplate:\n",
        "    \"\"\"A prompt template with configurable components.\"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    base_instruction: str\n",
        "    supports_examples: bool = True\n",
        "    supports_chain_of_thought: bool = True\n",
        "    estimated_cost: float = 1.0  # Relative cost multiplier\n",
        "    \n",
        "    def render(self, query: str, examples: List[str] = None, \n",
        "               tone: ToneStyle = ToneStyle.FORMAL,\n",
        "               detail: DetailLevel = DetailLevel.MODERATE,\n",
        "               chain_of_thought: bool = False) -> str:\n",
        "        \"\"\"Render the complete prompt.\"\"\"\n",
        "        prompt_parts = [self.base_instruction]\n",
        "        \n",
        "        # Add tone instruction\n",
        "        tone_instructions = {\n",
        "            ToneStyle.FORMAL: \"Use a professional and formal tone.\",\n",
        "            ToneStyle.CASUAL: \"Use a casual and conversational tone.\",\n",
        "            ToneStyle.TECHNICAL: \"Use precise technical language.\",\n",
        "            ToneStyle.FRIENDLY: \"Be warm and approachable in your response.\"\n",
        "        }\n",
        "        prompt_parts.append(tone_instructions[tone])\n",
        "        \n",
        "        # Add detail level\n",
        "        detail_instructions = {\n",
        "            DetailLevel.BRIEF: \"Keep your response concise and to the point.\",\n",
        "            DetailLevel.MODERATE: \"Provide a balanced response with key details.\",\n",
        "            DetailLevel.DETAILED: \"Provide a thorough and detailed response.\",\n",
        "            DetailLevel.COMPREHENSIVE: \"Provide an exhaustive response covering all aspects.\"\n",
        "        }\n",
        "        prompt_parts.append(detail_instructions[detail])\n",
        "        \n",
        "        # Add examples if supported and provided\n",
        "        if self.supports_examples and examples:\n",
        "            prompt_parts.append(\"\\nExamples:\")\n",
        "            for i, ex in enumerate(examples, 1):\n",
        "                prompt_parts.append(f\"{i}. {ex}\")\n",
        "        \n",
        "        # Add chain of thought if supported\n",
        "        if self.supports_chain_of_thought and chain_of_thought:\n",
        "            prompt_parts.append(\"\\nThink through this step by step before providing your answer.\")\n",
        "        \n",
        "        prompt_parts.append(f\"\\nUser Query: {query}\")\n",
        "        \n",
        "        return \"\\n\".join(prompt_parts)\n",
        "\n",
        "\n",
        "# Define available prompt templates\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"standard\": PromptTemplate(\n",
        "        id=\"standard\",\n",
        "        name=\"Standard Assistant\",\n",
        "        base_instruction=\"You are a helpful AI assistant. Answer the user's question accurately and helpfully.\",\n",
        "        estimated_cost=1.0\n",
        "    ),\n",
        "    \"expert\": PromptTemplate(\n",
        "        id=\"expert\",\n",
        "        name=\"Domain Expert\",\n",
        "        base_instruction=\"You are an expert in the relevant domain. Provide authoritative and well-researched answers.\",\n",
        "        estimated_cost=1.2\n",
        "    ),\n",
        "    \"teacher\": PromptTemplate(\n",
        "        id=\"teacher\",\n",
        "        name=\"Patient Teacher\",\n",
        "        base_instruction=\"You are a patient teacher. Explain concepts clearly, use analogies, and ensure understanding.\",\n",
        "        estimated_cost=1.3\n",
        "    ),\n",
        "    \"concise\": PromptTemplate(\n",
        "        id=\"concise\",\n",
        "        name=\"Concise Responder\",\n",
        "        base_instruction=\"You are a concise assistant. Provide direct, no-fluff answers.\",\n",
        "        supports_chain_of_thought=False,\n",
        "        estimated_cost=0.8\n",
        "    ),\n",
        "    \"creative\": PromptTemplate(\n",
        "        id=\"creative\",\n",
        "        name=\"Creative Writer\",\n",
        "        base_instruction=\"You are a creative assistant. Think outside the box and provide imaginative solutions.\",\n",
        "        estimated_cost=1.1\n",
        "    ),\n",
        "    \"analyst\": PromptTemplate(\n",
        "        id=\"analyst\",\n",
        "        name=\"Data Analyst\",\n",
        "        base_instruction=\"You are a data analyst. Approach problems systematically with data-driven insights.\",\n",
        "        estimated_cost=1.4\n",
        "    )\n",
        "}\n",
        "\n",
        "print(f\"âœ… Defined {len(PROMPT_TEMPLATES)} prompt templates:\")\n",
        "for template_id, template in PROMPT_TEMPLATES.items():\n",
        "    print(f\"   ðŸ“ {template.name} (cost: {template.estimated_cost}x)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Simulated User Environment and Reward Model\n",
        "\n",
        "We create a simulated environment that models user preferences and provides feedback signals for RL training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class UserProfile:\n",
        "    \"\"\"Simulates a user with specific preferences.\"\"\"\n",
        "    user_id: str\n",
        "    preferred_tone: ToneStyle\n",
        "    preferred_detail: DetailLevel\n",
        "    preferred_templates: List[str]  # Template IDs user responds well to\n",
        "    patience: float = 0.5  # 0-1, tolerance for verbose responses\n",
        "    expertise_level: float = 0.5  # 0-1, technical sophistication\n",
        "    \n",
        "    def get_satisfaction(self, template_id: str, tone: ToneStyle, \n",
        "                         detail: DetailLevel, response_quality: float) -> float:\n",
        "        \"\"\"Calculate user satisfaction based on preferences.\"\"\"\n",
        "        satisfaction = response_quality  # Base satisfaction from quality\n",
        "        \n",
        "        # Template preference bonus\n",
        "        if template_id in self.preferred_templates:\n",
        "            satisfaction += 0.2\n",
        "        \n",
        "        # Tone match bonus\n",
        "        if tone == self.preferred_tone:\n",
        "            satisfaction += 0.15\n",
        "        elif tone in [ToneStyle.TECHNICAL, ToneStyle.FORMAL] and self.expertise_level > 0.7:\n",
        "            satisfaction += 0.1\n",
        "        elif tone in [ToneStyle.CASUAL, ToneStyle.FRIENDLY] and self.expertise_level < 0.3:\n",
        "            satisfaction += 0.1\n",
        "        \n",
        "        # Detail level preference\n",
        "        detail_scores = {\n",
        "            DetailLevel.BRIEF: 0.25,\n",
        "            DetailLevel.MODERATE: 0.5,\n",
        "            DetailLevel.DETAILED: 0.75,\n",
        "            DetailLevel.COMPREHENSIVE: 1.0\n",
        "        }\n",
        "        detail_value = detail_scores[detail]\n",
        "        \n",
        "        # Users with low patience prefer brief responses\n",
        "        if self.patience < 0.3 and detail_value > 0.5:\n",
        "            satisfaction -= 0.2\n",
        "        # Users with high patience and expertise prefer detailed responses\n",
        "        elif self.patience > 0.7 and self.expertise_level > 0.6 and detail_value < 0.5:\n",
        "            satisfaction -= 0.1\n",
        "        \n",
        "        if detail == self.preferred_detail:\n",
        "            satisfaction += 0.1\n",
        "        \n",
        "        return np.clip(satisfaction, 0, 1)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Query:\n",
        "    \"\"\"Represents a user query with context.\"\"\"\n",
        "    text: str\n",
        "    query_type: QueryType\n",
        "    complexity: float  # 0-1\n",
        "    requires_examples: bool = False\n",
        "    requires_reasoning: bool = False\n",
        "\n",
        "\n",
        "class SimulatedLLMEnvironment:\n",
        "    \"\"\"Simulates LLM responses and user feedback for RL training.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.user_profiles = self._create_user_profiles()\n",
        "        self.query_bank = self._create_query_bank()\n",
        "        self.interaction_history = []\n",
        "        \n",
        "        # Ground truth: optimal templates per query type (for simulation)\n",
        "        self.optimal_mappings = {\n",
        "            QueryType.FACTUAL: [\"expert\", \"concise\", \"standard\"],\n",
        "            QueryType.CREATIVE: [\"creative\", \"teacher\"],\n",
        "            QueryType.ANALYTICAL: [\"analyst\", \"expert\"],\n",
        "            QueryType.CODING: [\"expert\", \"analyst\", \"concise\"],\n",
        "            QueryType.CONVERSATIONAL: [\"standard\", \"teacher\", \"creative\"]\n",
        "        }\n",
        "    \n",
        "    def _create_user_profiles(self) -> Dict[str, UserProfile]:\n",
        "        \"\"\"Create diverse user profiles.\"\"\"\n",
        "        return {\n",
        "            \"tech_expert\": UserProfile(\n",
        "                user_id=\"tech_expert\",\n",
        "                preferred_tone=ToneStyle.TECHNICAL,\n",
        "                preferred_detail=DetailLevel.DETAILED,\n",
        "                preferred_templates=[\"expert\", \"analyst\"],\n",
        "                patience=0.8,\n",
        "                expertise_level=0.9\n",
        "            ),\n",
        "            \"casual_user\": UserProfile(\n",
        "                user_id=\"casual_user\",\n",
        "                preferred_tone=ToneStyle.FRIENDLY,\n",
        "                preferred_detail=DetailLevel.MODERATE,\n",
        "                preferred_templates=[\"standard\", \"teacher\"],\n",
        "                patience=0.4,\n",
        "                expertise_level=0.3\n",
        "            ),\n",
        "            \"busy_professional\": UserProfile(\n",
        "                user_id=\"busy_professional\",\n",
        "                preferred_tone=ToneStyle.FORMAL,\n",
        "                preferred_detail=DetailLevel.BRIEF,\n",
        "                preferred_templates=[\"concise\", \"expert\"],\n",
        "                patience=0.2,\n",
        "                expertise_level=0.7\n",
        "            ),\n",
        "            \"student\": UserProfile(\n",
        "                user_id=\"student\",\n",
        "                preferred_tone=ToneStyle.FRIENDLY,\n",
        "                preferred_detail=DetailLevel.COMPREHENSIVE,\n",
        "                preferred_templates=[\"teacher\", \"standard\"],\n",
        "                patience=0.9,\n",
        "                expertise_level=0.4\n",
        "            ),\n",
        "            \"creative_writer\": UserProfile(\n",
        "                user_id=\"creative_writer\",\n",
        "                preferred_tone=ToneStyle.CASUAL,\n",
        "                preferred_detail=DetailLevel.DETAILED,\n",
        "                preferred_templates=[\"creative\", \"teacher\"],\n",
        "                patience=0.7,\n",
        "                expertise_level=0.5\n",
        "            )\n",
        "        }\n",
        "    \n",
        "    def _create_query_bank(self) -> List[Query]:\n",
        "        \"\"\"Create sample queries for simulation.\"\"\"\n",
        "        return [\n",
        "            Query(\"What is machine learning?\", QueryType.FACTUAL, 0.3),\n",
        "            Query(\"Explain quantum computing\", QueryType.FACTUAL, 0.7, requires_reasoning=True),\n",
        "            Query(\"Write a poem about AI\", QueryType.CREATIVE, 0.5),\n",
        "            Query(\"Create a story about robots\", QueryType.CREATIVE, 0.6),\n",
        "            Query(\"Analyze this sales data trend\", QueryType.ANALYTICAL, 0.7, requires_reasoning=True),\n",
        "            Query(\"What patterns do you see here?\", QueryType.ANALYTICAL, 0.5),\n",
        "            Query(\"Write a Python function for sorting\", QueryType.CODING, 0.4, requires_examples=True),\n",
        "            Query(\"Debug this code snippet\", QueryType.CODING, 0.6),\n",
        "            Query(\"How are you today?\", QueryType.CONVERSATIONAL, 0.1),\n",
        "            Query(\"What do you think about this?\", QueryType.CONVERSATIONAL, 0.3),\n",
        "        ]\n",
        "    \n",
        "    def get_random_query(self) -> Tuple[Query, str]:\n",
        "        \"\"\"Get a random query and user for interaction.\"\"\"\n",
        "        query = random.choice(self.query_bank)\n",
        "        user_id = random.choice(list(self.user_profiles.keys()))\n",
        "        return query, user_id\n",
        "    \n",
        "    def simulate_response(self, query: Query, template_id: str, \n",
        "                         tone: ToneStyle, detail: DetailLevel,\n",
        "                         use_examples: bool, use_cot: bool) -> Dict[str, float]:\n",
        "        \"\"\"Simulate LLM response quality based on prompt configuration.\"\"\"\n",
        "        \n",
        "        # Base quality from template-query match\n",
        "        optimal_templates = self.optimal_mappings.get(query.query_type, [\"standard\"])\n",
        "        if template_id in optimal_templates:\n",
        "            base_quality = np.random.uniform(0.7, 0.95)\n",
        "        else:\n",
        "            base_quality = np.random.uniform(0.4, 0.7)\n",
        "        \n",
        "        # Bonus for using examples when needed\n",
        "        if query.requires_examples and use_examples:\n",
        "            base_quality += 0.1\n",
        "        \n",
        "        # Bonus for chain-of-thought on complex queries\n",
        "        if query.requires_reasoning and use_cot:\n",
        "            base_quality += 0.15\n",
        "        elif not query.requires_reasoning and use_cot:\n",
        "            base_quality -= 0.05  # Unnecessary CoT can hurt simple queries\n",
        "        \n",
        "        # Add noise\n",
        "        quality = np.clip(base_quality + np.random.normal(0, 0.1), 0, 1)\n",
        "        \n",
        "        # Calculate cost\n",
        "        template = PROMPT_TEMPLATES[template_id]\n",
        "        cost = template.estimated_cost\n",
        "        if use_examples:\n",
        "            cost *= 1.2\n",
        "        if use_cot:\n",
        "            cost *= 1.3\n",
        "        \n",
        "        return {\n",
        "            \"quality\": quality,\n",
        "            \"cost\": cost,\n",
        "            \"latency\": cost * np.random.uniform(0.8, 1.2)  # Simulated latency\n",
        "        }\n",
        "    \n",
        "    def get_reward(self, query: Query, user_id: str, template_id: str,\n",
        "                   tone: ToneStyle, detail: DetailLevel,\n",
        "                   use_examples: bool, use_cot: bool,\n",
        "                   cost_weight: float = 0.2) -> Tuple[float, Dict]:\n",
        "        \"\"\"\n",
        "        Calculate reward combining user satisfaction and cost efficiency.\n",
        "        \n",
        "        Reward = satisfaction - cost_weight * normalized_cost\n",
        "        \"\"\"\n",
        "        # Simulate response\n",
        "        response_metrics = self.simulate_response(\n",
        "            query, template_id, tone, detail, use_examples, use_cot\n",
        "        )\n",
        "        \n",
        "        # Get user satisfaction\n",
        "        user = self.user_profiles[user_id]\n",
        "        satisfaction = user.get_satisfaction(\n",
        "            template_id, tone, detail, response_metrics[\"quality\"]\n",
        "        )\n",
        "        \n",
        "        # Calculate reward\n",
        "        normalized_cost = response_metrics[\"cost\"] / 2.0  # Normalize to ~0-1 range\n",
        "        reward = satisfaction - cost_weight * normalized_cost\n",
        "        \n",
        "        # Store interaction\n",
        "        interaction = {\n",
        "            \"query_type\": query.query_type.value,\n",
        "            \"user_id\": user_id,\n",
        "            \"template\": template_id,\n",
        "            \"tone\": tone.value,\n",
        "            \"detail\": detail.value,\n",
        "            \"use_examples\": use_examples,\n",
        "            \"use_cot\": use_cot,\n",
        "            \"quality\": response_metrics[\"quality\"],\n",
        "            \"satisfaction\": satisfaction,\n",
        "            \"cost\": response_metrics[\"cost\"],\n",
        "            \"reward\": reward\n",
        "        }\n",
        "        self.interaction_history.append(interaction)\n",
        "        \n",
        "        return reward, interaction\n",
        "\n",
        "\n",
        "# Create the simulation environment\n",
        "sim_env = SimulatedLLMEnvironment()\n",
        "\n",
        "print(\"âœ… Simulated environment created!\")\n",
        "print(f\"   ðŸ‘¥ {len(sim_env.user_profiles)} user profiles\")\n",
        "print(f\"   ðŸ“‹ {len(sim_env.query_bank)} sample queries\")\n",
        "print(f\"   ðŸ“ {len(PROMPT_TEMPLATES)} prompt templates\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Multi-Armed Bandit for Prompt Template Selection\n",
        "\n",
        "The simplest RL approach: treat each prompt template as an \"arm\" and learn which templates work best for different query types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiArmedBandit(ABC):\n",
        "    \"\"\"Abstract base class for bandit algorithms.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, arm_names: List[str] = None):\n",
        "        self.n_arms = n_arms\n",
        "        self.arm_names = arm_names or [f\"arm_{i}\" for i in range(n_arms)]\n",
        "        self.counts = np.zeros(n_arms)\n",
        "        self.values = np.zeros(n_arms)\n",
        "        self.history = []\n",
        "    \n",
        "    @abstractmethod\n",
        "    def select_arm(self) -> int:\n",
        "        \"\"\"Select an arm to pull.\"\"\"\n",
        "        pass\n",
        "    \n",
        "    def update(self, arm: int, reward: float):\n",
        "        \"\"\"Update estimates after receiving reward.\"\"\"\n",
        "        self.counts[arm] += 1\n",
        "        n = self.counts[arm]\n",
        "        value = self.values[arm]\n",
        "        # Incremental mean update\n",
        "        self.values[arm] = value + (reward - value) / n\n",
        "        self.history.append({\"arm\": arm, \"reward\": reward})\n",
        "    \n",
        "    def get_best_arm(self) -> int:\n",
        "        \"\"\"Return the arm with highest estimated value.\"\"\"\n",
        "        return np.argmax(self.values)\n",
        "\n",
        "\n",
        "class EpsilonGreedyBandit(MultiArmedBandit):\n",
        "    \"\"\"Epsilon-Greedy Multi-Armed Bandit.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, epsilon: float = 0.1, arm_names: List[str] = None):\n",
        "        super().__init__(n_arms, arm_names)\n",
        "        self.epsilon = epsilon\n",
        "    \n",
        "    def select_arm(self) -> int:\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.n_arms)\n",
        "        return self.get_best_arm()\n",
        "\n",
        "\n",
        "class UCBBandit(MultiArmedBandit):\n",
        "    \"\"\"Upper Confidence Bound (UCB1) Bandit.\"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, c: float = 2.0, arm_names: List[str] = None):\n",
        "        super().__init__(n_arms, arm_names)\n",
        "        self.c = c\n",
        "        self.total_counts = 0\n",
        "    \n",
        "    def select_arm(self) -> int:\n",
        "        self.total_counts += 1\n",
        "        \n",
        "        # If any arm hasn't been tried, try it\n",
        "        for arm in range(self.n_arms):\n",
        "            if self.counts[arm] == 0:\n",
        "                return arm\n",
        "        \n",
        "        # UCB formula\n",
        "        ucb_values = self.values + self.c * np.sqrt(\n",
        "            np.log(self.total_counts) / self.counts\n",
        "        )\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "\n",
        "class ThompsonSamplingBandit(MultiArmedBandit):\n",
        "    \"\"\"Thompson Sampling Bandit (Beta-Bernoulli).\"\"\"\n",
        "    \n",
        "    def __init__(self, n_arms: int, arm_names: List[str] = None):\n",
        "        super().__init__(n_arms, arm_names)\n",
        "        self.alpha = np.ones(n_arms)  # Successes + 1\n",
        "        self.beta = np.ones(n_arms)   # Failures + 1\n",
        "    \n",
        "    def select_arm(self) -> int:\n",
        "        samples = np.random.beta(self.alpha, self.beta)\n",
        "        return np.argmax(samples)\n",
        "    \n",
        "    def update(self, arm: int, reward: float):\n",
        "        super().update(arm, reward)\n",
        "        # Update Beta distribution parameters\n",
        "        if reward > 0.5:  # Treat as success\n",
        "            self.alpha[arm] += 1\n",
        "        else:\n",
        "            self.beta[arm] += 1\n",
        "\n",
        "\n",
        "class PromptTemplateBandit:\n",
        "    \"\"\"Uses bandit algorithm to select optimal prompt templates.\"\"\"\n",
        "    \n",
        "    def __init__(self, templates: Dict[str, PromptTemplate], \n",
        "                 algorithm: str = \"thompson\"):\n",
        "        self.templates = templates\n",
        "        self.template_ids = list(templates.keys())\n",
        "        self.n_templates = len(self.template_ids)\n",
        "        \n",
        "        # Create bandits for each query type\n",
        "        self.bandits: Dict[QueryType, MultiArmedBandit] = {}\n",
        "        for query_type in QueryType:\n",
        "            if algorithm == \"epsilon_greedy\":\n",
        "                self.bandits[query_type] = EpsilonGreedyBandit(\n",
        "                    self.n_templates, epsilon=0.1, arm_names=self.template_ids\n",
        "                )\n",
        "            elif algorithm == \"ucb\":\n",
        "                self.bandits[query_type] = UCBBandit(\n",
        "                    self.n_templates, c=2.0, arm_names=self.template_ids\n",
        "                )\n",
        "            else:  # thompson\n",
        "                self.bandits[query_type] = ThompsonSamplingBandit(\n",
        "                    self.n_templates, arm_names=self.template_ids\n",
        "                )\n",
        "    \n",
        "    def select_template(self, query_type: QueryType) -> str:\n",
        "        \"\"\"Select a template for the given query type.\"\"\"\n",
        "        bandit = self.bandits[query_type]\n",
        "        arm = bandit.select_arm()\n",
        "        return self.template_ids[arm]\n",
        "    \n",
        "    def update(self, query_type: QueryType, template_id: str, reward: float):\n",
        "        \"\"\"Update bandit after receiving feedback.\"\"\"\n",
        "        arm = self.template_ids.index(template_id)\n",
        "        self.bandits[query_type].update(arm, reward)\n",
        "    \n",
        "    def get_best_templates(self) -> Dict[str, str]:\n",
        "        \"\"\"Get the best template for each query type.\"\"\"\n",
        "        result = {}\n",
        "        for query_type in QueryType:\n",
        "            bandit = self.bandits[query_type]\n",
        "            best_arm = bandit.get_best_arm()\n",
        "            result[query_type.value] = self.template_ids[best_arm]\n",
        "        return result\n",
        "\n",
        "\n",
        "print(\"âœ… Bandit algorithms defined:\")\n",
        "print(\"   ðŸŽ° Epsilon-Greedy\")\n",
        "print(\"   ðŸŽ° Upper Confidence Bound (UCB1)\")\n",
        "print(\"   ðŸŽ° Thompson Sampling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Q-Learning for Full Prompt Configuration\n",
        "\n",
        "Q-Learning allows us to learn optimal combinations of template, tone, detail level, and other prompt parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PromptAction:\n",
        "    \"\"\"Represents a complete prompt configuration action.\"\"\"\n",
        "    template_id: str\n",
        "    tone: ToneStyle\n",
        "    detail: DetailLevel\n",
        "    use_examples: bool\n",
        "    use_chain_of_thought: bool\n",
        "    \n",
        "    def to_tuple(self) -> tuple:\n",
        "        return (self.template_id, self.tone.value, self.detail.value, \n",
        "                self.use_examples, self.use_chain_of_thought)\n",
        "    \n",
        "    @classmethod\n",
        "    def from_tuple(cls, t: tuple) -> 'PromptAction':\n",
        "        return cls(\n",
        "            template_id=t[0],\n",
        "            tone=ToneStyle(t[1]),\n",
        "            detail=DetailLevel(t[2]),\n",
        "            use_examples=t[3],\n",
        "            use_chain_of_thought=t[4]\n",
        "        )\n",
        "\n",
        "\n",
        "class PromptQLearning:\n",
        "    \"\"\"Q-Learning agent for prompt optimization.\"\"\"\n",
        "    \n",
        "    def __init__(self, templates: Dict[str, PromptTemplate],\n",
        "                 learning_rate: float = 0.1,\n",
        "                 discount_factor: float = 0.95,\n",
        "                 epsilon: float = 0.2,\n",
        "                 epsilon_decay: float = 0.995,\n",
        "                 min_epsilon: float = 0.05):\n",
        "        self.templates = templates\n",
        "        self.template_ids = list(templates.keys())\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.min_epsilon = min_epsilon\n",
        "        \n",
        "        # Q-table: state -> action -> value\n",
        "        # State: (query_type, user_id) or just query_type for simpler version\n",
        "        self.q_table: Dict[str, Dict[tuple, float]] = defaultdict(lambda: defaultdict(float))\n",
        "        \n",
        "        # Generate all possible actions\n",
        "        self.actions = self._generate_all_actions()\n",
        "        \n",
        "        # Training history\n",
        "        self.reward_history = []\n",
        "        self.epsilon_history = []\n",
        "    \n",
        "    def _generate_all_actions(self) -> List[PromptAction]:\n",
        "        \"\"\"Generate all possible prompt configurations.\"\"\"\n",
        "        actions = []\n",
        "        for template_id in self.template_ids:\n",
        "            for tone in ToneStyle:\n",
        "                for detail in DetailLevel:\n",
        "                    for use_examples in [True, False]:\n",
        "                        for use_cot in [True, False]:\n",
        "                            # Skip invalid combinations\n",
        "                            template = self.templates[template_id]\n",
        "                            if use_cot and not template.supports_chain_of_thought:\n",
        "                                continue\n",
        "                            if use_examples and not template.supports_examples:\n",
        "                                continue\n",
        "                            actions.append(PromptAction(\n",
        "                                template_id, tone, detail, use_examples, use_cot\n",
        "                            ))\n",
        "        return actions\n",
        "    \n",
        "    def get_state(self, query: Query, user_id: str = None) -> str:\n",
        "        \"\"\"Convert query/user to state representation.\"\"\"\n",
        "        # Simple state: just query type\n",
        "        # Can be extended to include user_id for personalization\n",
        "        if user_id:\n",
        "            return f\"{query.query_type.value}_{user_id}\"\n",
        "        return query.query_type.value\n",
        "    \n",
        "    def select_action(self, state: str, training: bool = True) -> PromptAction:\n",
        "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            # Explore: random action\n",
        "            return random.choice(self.actions)\n",
        "        else:\n",
        "            # Exploit: best known action\n",
        "            state_q = self.q_table[state]\n",
        "            if not state_q:\n",
        "                return random.choice(self.actions)\n",
        "            \n",
        "            best_action_tuple = max(state_q.keys(), key=lambda a: state_q[a])\n",
        "            return PromptAction.from_tuple(best_action_tuple)\n",
        "    \n",
        "    def update(self, state: str, action: PromptAction, reward: float, \n",
        "               next_state: str = None):\n",
        "        \"\"\"Update Q-value using Q-learning update rule.\"\"\"\n",
        "        action_tuple = action.to_tuple()\n",
        "        old_value = self.q_table[state][action_tuple]\n",
        "        \n",
        "        # For prompt optimization, we typically don't have a \"next state\"\n",
        "        # in the traditional sense, so we use a simplified update\n",
        "        if next_state and self.q_table[next_state]:\n",
        "            max_next = max(self.q_table[next_state].values())\n",
        "        else:\n",
        "            max_next = 0\n",
        "        \n",
        "        # Q-learning update\n",
        "        new_value = old_value + self.learning_rate * (\n",
        "            reward + self.discount_factor * max_next - old_value\n",
        "        )\n",
        "        self.q_table[state][action_tuple] = new_value\n",
        "        \n",
        "        # Decay epsilon\n",
        "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
        "        \n",
        "        self.reward_history.append(reward)\n",
        "        self.epsilon_history.append(self.epsilon)\n",
        "    \n",
        "    def get_best_action(self, state: str) -> Optional[PromptAction]:\n",
        "        \"\"\"Get the best action for a state without exploration.\"\"\"\n",
        "        state_q = self.q_table[state]\n",
        "        if not state_q:\n",
        "            return None\n",
        "        best_action_tuple = max(state_q.keys(), key=lambda a: state_q[a])\n",
        "        return PromptAction.from_tuple(best_action_tuple)\n",
        "    \n",
        "    def get_policy_summary(self) -> pd.DataFrame:\n",
        "        \"\"\"Summarize learned policy.\"\"\"\n",
        "        rows = []\n",
        "        for state, actions in self.q_table.items():\n",
        "            if actions:\n",
        "                best_action_tuple = max(actions.keys(), key=lambda a: actions[a])\n",
        "                best_q = actions[best_action_tuple]\n",
        "                action = PromptAction.from_tuple(best_action_tuple)\n",
        "                rows.append({\n",
        "                    \"state\": state,\n",
        "                    \"template\": action.template_id,\n",
        "                    \"tone\": action.tone.value,\n",
        "                    \"detail\": action.detail.value,\n",
        "                    \"examples\": action.use_examples,\n",
        "                    \"cot\": action.use_chain_of_thought,\n",
        "                    \"q_value\": best_q\n",
        "                })\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "print(\"âœ… Q-Learning agent defined\")\n",
        "print(f\"   ðŸ“Š Action space size: {len(PromptQLearning(PROMPT_TEMPLATES).actions)} possible configurations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and Evaluation\n",
        "\n",
        "Let's train both bandit and Q-learning agents and compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_bandit(env: SimulatedLLMEnvironment, n_iterations: int = 1000,\n",
        "                 algorithm: str = \"thompson\") -> Tuple[PromptTemplateBandit, List[float]]:\n",
        "    \"\"\"Train a bandit agent on template selection.\"\"\"\n",
        "    bandit = PromptTemplateBandit(PROMPT_TEMPLATES, algorithm=algorithm)\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        # Get random query and user\n",
        "        query, user_id = env.get_random_query()\n",
        "        \n",
        "        # Select template using bandit\n",
        "        template_id = bandit.select_template(query.query_type)\n",
        "        \n",
        "        # Use default settings for other parameters\n",
        "        reward, _ = env.get_reward(\n",
        "            query, user_id, template_id,\n",
        "            tone=ToneStyle.FORMAL,\n",
        "            detail=DetailLevel.MODERATE,\n",
        "            use_examples=query.requires_examples,\n",
        "            use_cot=query.requires_reasoning\n",
        "        )\n",
        "        \n",
        "        # Update bandit\n",
        "        bandit.update(query.query_type, template_id, reward)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return bandit, rewards\n",
        "\n",
        "\n",
        "def train_qlearning(env: SimulatedLLMEnvironment, n_iterations: int = 2000,\n",
        "                    personalized: bool = False) -> Tuple[PromptQLearning, List[float]]:\n",
        "    \"\"\"Train a Q-learning agent on full prompt configuration.\"\"\"\n",
        "    agent = PromptQLearning(PROMPT_TEMPLATES, epsilon=0.3, epsilon_decay=0.998)\n",
        "    rewards = []\n",
        "    \n",
        "    for i in range(n_iterations):\n",
        "        # Get random query and user\n",
        "        query, user_id = env.get_random_query()\n",
        "        \n",
        "        # Get state\n",
        "        state = agent.get_state(query, user_id if personalized else None)\n",
        "        \n",
        "        # Select action\n",
        "        action = agent.select_action(state, training=True)\n",
        "        \n",
        "        # Get reward\n",
        "        reward, _ = env.get_reward(\n",
        "            query, user_id, action.template_id,\n",
        "            tone=action.tone,\n",
        "            detail=action.detail,\n",
        "            use_examples=action.use_examples,\n",
        "            use_cot=action.use_chain_of_thought\n",
        "        )\n",
        "        \n",
        "        # Update Q-values\n",
        "        agent.update(state, action, reward)\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return agent, rewards\n",
        "\n",
        "\n",
        "def evaluate_random_baseline(env: SimulatedLLMEnvironment, \n",
        "                            n_iterations: int = 500) -> List[float]:\n",
        "    \"\"\"Evaluate random template selection as baseline.\"\"\"\n",
        "    rewards = []\n",
        "    template_ids = list(PROMPT_TEMPLATES.keys())\n",
        "    \n",
        "    for _ in range(n_iterations):\n",
        "        query, user_id = env.get_random_query()\n",
        "        template_id = random.choice(template_ids)\n",
        "        tone = random.choice(list(ToneStyle))\n",
        "        detail = random.choice(list(DetailLevel))\n",
        "        \n",
        "        reward, _ = env.get_reward(\n",
        "            query, user_id, template_id,\n",
        "            tone=tone, detail=detail,\n",
        "            use_examples=random.choice([True, False]),\n",
        "            use_cot=random.choice([True, False])\n",
        "        )\n",
        "        rewards.append(reward)\n",
        "    \n",
        "    return rewards\n",
        "\n",
        "\n",
        "print(\"âœ… Training functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset environment\n",
        "sim_env = SimulatedLLMEnvironment()\n",
        "\n",
        "print(\"ðŸŽ¯ Training Prompt Optimization Agents...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Train bandit agents with different algorithms\n",
        "print(\"\\nðŸ“Š Training Multi-Armed Bandits...\")\n",
        "bandit_thompson, rewards_thompson = train_bandit(sim_env, n_iterations=1000, algorithm=\"thompson\")\n",
        "bandit_ucb, rewards_ucb = train_bandit(sim_env, n_iterations=1000, algorithm=\"ucb\")\n",
        "bandit_epsilon, rewards_epsilon = train_bandit(sim_env, n_iterations=1000, algorithm=\"epsilon_greedy\")\n",
        "\n",
        "# Train Q-learning agents\n",
        "print(\"\\nðŸ“Š Training Q-Learning Agents...\")\n",
        "qlearning_basic, rewards_qlearning = train_qlearning(sim_env, n_iterations=2000, personalized=False)\n",
        "qlearning_personal, rewards_qlearning_personal = train_qlearning(sim_env, n_iterations=2000, personalized=True)\n",
        "\n",
        "# Baseline\n",
        "print(\"\\nðŸ“Š Evaluating Random Baseline...\")\n",
        "rewards_random = evaluate_random_baseline(sim_env, n_iterations=500)\n",
        "\n",
        "print(\"\\nâœ… Training Complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def moving_average(data, window=50):\n",
        "    \"\"\"Calculate moving average.\"\"\"\n",
        "    return pd.Series(data).rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle(\"Prompt Optimization with Reinforcement Learning\", fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Bandit Algorithm Comparison\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(moving_average(rewards_thompson), label='Thompson Sampling', alpha=0.8)\n",
        "ax1.plot(moving_average(rewards_ucb), label='UCB', alpha=0.8)\n",
        "ax1.plot(moving_average(rewards_epsilon), label='Epsilon-Greedy', alpha=0.8)\n",
        "ax1.axhline(y=np.mean(rewards_random), color='gray', linestyle='--', label='Random Baseline')\n",
        "ax1.set_xlabel('Iteration')\n",
        "ax1.set_ylabel('Reward (Moving Avg)')\n",
        "ax1.set_title('Multi-Armed Bandit: Template Selection')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Q-Learning Performance\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(moving_average(rewards_qlearning, window=100), label='Q-Learning (Basic)', alpha=0.8)\n",
        "ax2.plot(moving_average(rewards_qlearning_personal, window=100), label='Q-Learning (Personalized)', alpha=0.8)\n",
        "ax2.axhline(y=np.mean(rewards_random), color='gray', linestyle='--', label='Random Baseline')\n",
        "ax2.set_xlabel('Iteration')\n",
        "ax2.set_ylabel('Reward (Moving Avg)')\n",
        "ax2.set_title('Q-Learning: Full Prompt Configuration')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Final Performance Comparison\n",
        "ax3 = axes[1, 0]\n",
        "methods = ['Random', 'Thompson', 'UCB', 'Îµ-Greedy', 'Q-Learn', 'Q-Learn\\n(Personal)']\n",
        "final_rewards = [\n",
        "    np.mean(rewards_random),\n",
        "    np.mean(rewards_thompson[-200:]),\n",
        "    np.mean(rewards_ucb[-200:]),\n",
        "    np.mean(rewards_epsilon[-200:]),\n",
        "    np.mean(rewards_qlearning[-400:]),\n",
        "    np.mean(rewards_qlearning_personal[-400:])\n",
        "]\n",
        "colors = ['gray', '#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12']\n",
        "bars = ax3.bar(methods, final_rewards, color=colors, edgecolor='black')\n",
        "ax3.set_ylabel('Average Reward')\n",
        "ax3.set_title('Final Performance Comparison')\n",
        "ax3.axhline(y=np.mean(rewards_random), color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, val in zip(bars, final_rewards):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Plot 4: Q-Learning Exploration vs Exploitation\n",
        "ax4 = axes[1, 1]\n",
        "ax4.plot(qlearning_basic.epsilon_history, label='Epsilon (Exploration Rate)', color='#e74c3c')\n",
        "ax4.set_xlabel('Iteration')\n",
        "ax4.set_ylabel('Epsilon')\n",
        "ax4.set_title('Q-Learning: Exploration Decay')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nðŸ“Š Performance Summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Method':<25} {'Mean Reward':<15} {'Std Dev':<15} {'Improvement'}\")\n",
        "print(\"-\"*60)\n",
        "baseline = np.mean(rewards_random)\n",
        "for name, rewards in [\n",
        "    ('Random Baseline', rewards_random),\n",
        "    ('Thompson Sampling', rewards_thompson[-200:]),\n",
        "    ('UCB', rewards_ucb[-200:]),\n",
        "    ('Epsilon-Greedy', rewards_epsilon[-200:]),\n",
        "    ('Q-Learning (Basic)', rewards_qlearning[-400:]),\n",
        "    ('Q-Learning (Personal)', rewards_qlearning_personal[-400:])\n",
        "]:\n",
        "    mean_r = np.mean(rewards)\n",
        "    std_r = np.std(rewards)\n",
        "    improvement = ((mean_r - baseline) / baseline) * 100\n",
        "    print(f\"{name:<25} {mean_r:<15.4f} {std_r:<15.4f} {improvement:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analyzing Learned Policies\n",
        "\n",
        "Let's examine what the RL agents learned about optimal prompt configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze Bandit learned preferences\n",
        "print(\"ðŸŽ° Multi-Armed Bandit: Learned Template Preferences\")\n",
        "print(\"=\"*60)\n",
        "best_templates = bandit_thompson.get_best_templates()\n",
        "print(\"\\nBest template per query type (Thompson Sampling):\")\n",
        "for query_type, template in best_templates.items():\n",
        "    template_name = PROMPT_TEMPLATES[template].name\n",
        "    print(f\"   {query_type:<15} â†’ {template_name}\")\n",
        "\n",
        "# Compare with ground truth\n",
        "print(\"\\nðŸ“‹ Ground Truth (Optimal Templates):\")\n",
        "for query_type, templates in sim_env.optimal_mappings.items():\n",
        "    template_names = [PROMPT_TEMPLATES[t].name for t in templates]\n",
        "    print(f\"   {query_type.value:<15} â†’ {', '.join(template_names)}\")\n",
        "\n",
        "# Analyze Q-Learning learned policy\n",
        "print(\"\\n\\nðŸ§  Q-Learning: Learned Full Prompt Configurations\")\n",
        "print(\"=\"*60)\n",
        "policy_df = qlearning_basic.get_policy_summary()\n",
        "if not policy_df.empty:\n",
        "    print(policy_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No policy learned yet.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize learned policy as heatmap\n",
        "print(\"\\nðŸ“Š Q-Learning Policy Visualization\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create policy matrix for visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Template preferences by query type\n",
        "ax1 = axes[0]\n",
        "query_types = [qt.value for qt in QueryType]\n",
        "template_ids = list(PROMPT_TEMPLATES.keys())\n",
        "\n",
        "# Build preference matrix from Q-values\n",
        "preference_matrix = np.zeros((len(query_types), len(template_ids)))\n",
        "for i, qt in enumerate(QueryType):\n",
        "    state = qt.value\n",
        "    if state in qlearning_basic.q_table:\n",
        "        for action_tuple, q_val in qlearning_basic.q_table[state].items():\n",
        "            template_idx = template_ids.index(action_tuple[0])\n",
        "            preference_matrix[i, template_idx] = max(preference_matrix[i, template_idx], q_val)\n",
        "\n",
        "# Normalize for visualization\n",
        "if preference_matrix.max() > 0:\n",
        "    preference_matrix = preference_matrix / preference_matrix.max()\n",
        "\n",
        "sns.heatmap(preference_matrix, xticklabels=template_ids, yticklabels=query_types,\n",
        "            annot=True, fmt='.2f', cmap='YlGnBu', ax=ax1)\n",
        "ax1.set_title('Learned Template Preferences by Query Type')\n",
        "ax1.set_xlabel('Template')\n",
        "ax1.set_ylabel('Query Type')\n",
        "\n",
        "# Plot 2: Personalized preferences (by user)\n",
        "ax2 = axes[1]\n",
        "user_ids = list(sim_env.user_profiles.keys())\n",
        "\n",
        "# Build user preference matrix\n",
        "user_pref_matrix = np.zeros((len(user_ids), len(template_ids)))\n",
        "for i, user_id in enumerate(user_ids):\n",
        "    for qt in QueryType:\n",
        "        state = f\"{qt.value}_{user_id}\"\n",
        "        if state in qlearning_personal.q_table:\n",
        "            for action_tuple, q_val in qlearning_personal.q_table[state].items():\n",
        "                template_idx = template_ids.index(action_tuple[0])\n",
        "                user_pref_matrix[i, template_idx] += max(0, q_val)\n",
        "\n",
        "# Normalize\n",
        "row_sums = user_pref_matrix.sum(axis=1, keepdims=True)\n",
        "row_sums[row_sums == 0] = 1\n",
        "user_pref_matrix = user_pref_matrix / row_sums\n",
        "\n",
        "sns.heatmap(user_pref_matrix, xticklabels=template_ids, yticklabels=user_ids,\n",
        "            annot=True, fmt='.2f', cmap='YlOrRd', ax=ax2)\n",
        "ax2.set_title('Personalized Template Preferences by User')\n",
        "ax2.set_xlabel('Template')\n",
        "ax2.set_ylabel('User Profile')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Production Integration Example\n",
        "\n",
        "Here's how you would integrate the trained RL agent into a production prompt optimization system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductionPromptOptimizer:\n",
        "    \"\"\"Production-ready prompt optimizer with RL-based selection.\"\"\"\n",
        "    \n",
        "    def __init__(self, templates: Dict[str, PromptTemplate],\n",
        "                 pretrained_agent: PromptQLearning = None):\n",
        "        self.templates = templates\n",
        "        self.agent = pretrained_agent or PromptQLearning(templates)\n",
        "        self.feedback_buffer = []\n",
        "        self.online_learning = True\n",
        "        \n",
        "    def optimize_prompt(self, query_text: str, query_type: QueryType, \n",
        "                       user_id: str = None) -> Tuple[str, PromptAction]:\n",
        "        \"\"\"Generate optimized prompt for a query.\"\"\"\n",
        "        # Create query object\n",
        "        query = Query(\n",
        "            text=query_text,\n",
        "            query_type=query_type,\n",
        "            complexity=0.5,  # Could be estimated from query\n",
        "            requires_examples=query_type == QueryType.CODING,\n",
        "            requires_reasoning=query_type == QueryType.ANALYTICAL\n",
        "        )\n",
        "        \n",
        "        # Get state and select action\n",
        "        state = self.agent.get_state(query, user_id)\n",
        "        action = self.agent.select_action(state, training=False)\n",
        "        \n",
        "        # Render the optimized prompt\n",
        "        template = self.templates[action.template_id]\n",
        "        optimized_prompt = template.render(\n",
        "            query=query_text,\n",
        "            tone=action.tone,\n",
        "            detail=action.detail,\n",
        "            chain_of_thought=action.use_chain_of_thought\n",
        "        )\n",
        "        \n",
        "        return optimized_prompt, action\n",
        "    \n",
        "    def record_feedback(self, query_type: QueryType, action: PromptAction,\n",
        "                       user_id: str, feedback_score: float):\n",
        "        \"\"\"Record user feedback for online learning.\"\"\"\n",
        "        self.feedback_buffer.append({\n",
        "            'query_type': query_type,\n",
        "            'action': action,\n",
        "            'user_id': user_id,\n",
        "            'feedback': feedback_score\n",
        "        })\n",
        "        \n",
        "        # Online learning update\n",
        "        if self.online_learning:\n",
        "            state = f\"{query_type.value}_{user_id}\" if user_id else query_type.value\n",
        "            self.agent.update(state, action, feedback_score)\n",
        "    \n",
        "    def get_optimization_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about prompt optimization.\"\"\"\n",
        "        if not self.feedback_buffer:\n",
        "            return {\"message\": \"No feedback recorded yet\"}\n",
        "        \n",
        "        df = pd.DataFrame(self.feedback_buffer)\n",
        "        return {\n",
        "            \"total_interactions\": len(df),\n",
        "            \"avg_feedback\": df['feedback'].mean(),\n",
        "            \"feedback_trend\": df['feedback'].rolling(10).mean().tolist()[-10:],\n",
        "            \"most_used_template\": df['action'].apply(lambda a: a.template_id).mode()[0]\n",
        "        }\n",
        "\n",
        "\n",
        "# Example usage\n",
        "optimizer = ProductionPromptOptimizer(PROMPT_TEMPLATES, pretrained_agent=qlearning_personal)\n",
        "\n",
        "# Demo: Optimize different queries\n",
        "print(\"ðŸš€ Production Prompt Optimizer Demo\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "demo_queries = [\n",
        "    (\"What is the capital of France?\", QueryType.FACTUAL, \"casual_user\"),\n",
        "    (\"Write me a haiku about coding\", QueryType.CREATIVE, \"creative_writer\"),\n",
        "    (\"Explain the complexity of quicksort\", QueryType.ANALYTICAL, \"tech_expert\"),\n",
        "    (\"Help me debug this Python error\", QueryType.CODING, \"student\"),\n",
        "]\n",
        "\n",
        "for query_text, query_type, user_id in demo_queries:\n",
        "    prompt, action = optimizer.optimize_prompt(query_text, query_type, user_id)\n",
        "    print(f\"\\nðŸ“ Query: {query_text}\")\n",
        "    print(f\"   User: {user_id}\")\n",
        "    print(f\"   Selected: {action.template_id} | {action.tone.value} | {action.detail.value}\")\n",
        "    print(f\"   Features: examples={action.use_examples}, cot={action.use_chain_of_thought}\")\n",
        "    print(f\"\\n   Generated Prompt Preview:\")\n",
        "    print(f\"   {prompt[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary\n",
        "\n",
        "This notebook demonstrated how **Reinforcement Learning** can be applied to **prompt optimization** in agentic workflows.\n",
        "\n",
        "### Key Techniques Covered\n",
        "\n",
        "| Technique | Use Case | Complexity |\n",
        "|-----------|----------|------------|\n",
        "| **Multi-Armed Bandit** | Template selection | Low - good for quick A/B testing |\n",
        "| **Thompson Sampling** | Exploration-exploitation balance | Medium - probabilistic selection |\n",
        "| **UCB** | Regret minimization | Medium - confidence-based exploration |\n",
        "| **Q-Learning** | Full prompt configuration | High - learns optimal combinations |\n",
        "| **Personalized Q-Learning** | User-specific optimization | High - adapts to individual preferences |\n",
        "\n",
        "### What Can Be Optimized\n",
        "\n",
        "1. **Prompt Template Selection** - Which persona/instruction works best\n",
        "2. **Tone & Style** - Formal, casual, technical, friendly\n",
        "3. **Detail Level** - Brief to comprehensive responses\n",
        "4. **Few-shot Examples** - When to include examples\n",
        "5. **Chain-of-Thought** - When reasoning steps help\n",
        "6. **Cost vs Quality** - Balance API costs with response quality\n",
        "\n",
        "### Reward Signals for Production\n",
        "\n",
        "- **Explicit Feedback**: User ratings, thumbs up/down\n",
        "- **Implicit Feedback**: Follow-up questions (negative), task completion (positive)\n",
        "- **Task Success**: Did the response solve the user's problem?\n",
        "- **Engagement**: Time spent, copy actions, shares\n",
        "- **Cost Efficiency**: API tokens used, latency\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Integrate with Real LLMs**: Replace simulation with actual API calls\n",
        "2. **Add Contextual Features**: Include query embeddings for better state representation\n",
        "3. **Implement RLHF**: Train a reward model from human preferences\n",
        "4. **A/B Testing Framework**: Deploy with proper experiment tracking\n",
        "5. **Online Learning**: Continuously improve from production feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nðŸŽ‰ Prompt Optimization with Reinforcement Learning - Complete!\")\n",
        "print(\"\\nThis notebook demonstrated:\")\n",
        "print(\"  â€¢ Multi-Armed Bandits for template selection (Thompson, UCB, Îµ-Greedy)\")\n",
        "print(\"  â€¢ Q-Learning for full prompt configuration optimization\")\n",
        "print(\"  â€¢ Personalized learning based on user preferences\")\n",
        "print(\"  â€¢ Simulated reward modeling with user feedback\")\n",
        "print(\"  â€¢ Production integration patterns\")\n",
        "print(\"\\nðŸ’¡ Key Insight: RL enables prompts to continuously improve\")\n",
        "print(\"   based on real user feedback, not just static engineering!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
