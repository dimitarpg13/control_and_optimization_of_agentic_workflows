{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ RL-Based Prompt Optimization with Real LLM Calls\n",
    "\n",
    "This notebook demonstrates **Reinforcement Learning** for dynamic prompt optimization\n",
    "using **real LLM API calls** via **LangChain/LangGraph**.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ              RL-BASED PROMPT OPTIMIZATION (Real LLM Calls)                   ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                              ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ  ‚îÇ  User   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  RL PROMPT AGENT  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   LLM SERVICE    ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îÇ  Query  ‚îÇ    ‚îÇ  (LangGraph)      ‚îÇ    ‚îÇ  (OpenAI /       ‚îÇ                ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ                   ‚îÇ    ‚îÇ   Anthropic)     ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ ‚Ä¢ Select Template ‚îÇ    ‚îÇ                  ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ ‚Ä¢ Choose Tone     ‚îÇ    ‚îÇ ‚Ä¢ Generate       ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ ‚Ä¢ Set Parameters  ‚îÇ    ‚îÇ   Response       ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îÇ       ‚îÇ                  ‚îÇ                        ‚îÇ                          ‚îÇ\n",
    "‚îÇ       ‚îÇ                  ‚ñº                        ‚ñº                          ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ  RL OPTIMIZER    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ  LLM-AS-JUDGE    ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ                  ‚îÇ    ‚îÇ  REWARD MODEL     ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ ‚Ä¢ Q-Learning     ‚îÇ    ‚îÇ                  ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îÇ         ‚îÇ ‚Ä¢ Thompson       ‚îÇ    ‚îÇ ‚Ä¢ Quality Score  ‚îÇ                ‚îÇ\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   Sampling       ‚îÇ    ‚îÇ ‚Ä¢ Relevance      ‚îÇ                ‚îÇ\n",
    "‚îÇ   Feedback      ‚îÇ ‚Ä¢ UCB            ‚îÇ    ‚îÇ ‚Ä¢ Helpfulness    ‚îÇ                ‚îÇ\n",
    "‚îÇ                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚Ä¢ Cost Tracking  ‚îÇ                ‚îÇ\n",
    "‚îÇ                                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## Key Differences from Simulated Version\n",
    "\n",
    "| Aspect | Simulated | This Notebook |\n",
    "|--------|-----------|---------------|\n",
    "| **LLM Calls** | Simulated scores | Real OpenAI / Anthropic API calls |\n",
    "| **Reward Signal** | Heuristic formula | LLM-as-Judge + token cost tracking |\n",
    "| **Workflow** | Manual loops | LangGraph state machine |\n",
    "| **Models** | None | GPT-4o-mini, Claude 3.5 Haiku |\n",
    "| **Cost Tracking** | Estimated | Real token usage from API |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- `OPENAI_API_KEY` environment variable set\n",
    "- `ANTHROPIC_API_KEY` environment variable set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q numpy pandas matplotlib seaborn scikit-learn \\\n",
    "    langchain langchain-openai langchain-anthropic langgraph \\\n",
    "    langchain-core pydantic tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any, Literal, TypedDict\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify API keys are set\n",
    "assert os.environ.get(\"OPENAI_API_KEY\"), \"‚ùå OPENAI_API_KEY not set!\"\n",
    "assert os.environ.get(\"ANTHROPIC_API_KEY\"), \"‚ùå ANTHROPIC_API_KEY not set!\"\n",
    "print(\"‚úÖ API keys verified\")\n",
    "print(f\"   OpenAI key: ...{os.environ['OPENAI_API_KEY'][-6:]}\")\n",
    "print(f\"   Anthropic key: ...{os.environ['ANTHROPIC_API_KEY'][-6:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Service Layer\n",
    "\n",
    "We set up LangChain models for both OpenAI and Anthropic, with token tracking for cost-aware optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMProvider(Enum):\n",
    "    \"\"\"Available LLM providers.\"\"\"\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured response from an LLM call.\"\"\"\n",
    "    content: str\n",
    "    provider: str\n",
    "    model: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_seconds: float\n",
    "    cost_estimate: float  # Estimated cost in USD\n",
    "\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"\n",
    "    Manages LLM calls to OpenAI and Anthropic with token/cost tracking.\n",
    "    \"\"\"\n",
    "\n",
    "    # Approximate pricing per 1M tokens (USD)\n",
    "    PRICING = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00},\n",
    "        \"claude-3-5-haiku-latest\": {\"input\": 0.80, \"output\": 4.00},\n",
    "        \"claude-3-5-sonnet-latest\": {\"input\": 3.00, \"output\": 15.00},\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "        anthropic_model: str = \"claude-3-5-haiku-latest\",\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "    ):\n",
    "        self.openai_model_name = openai_model\n",
    "        self.anthropic_model_name = anthropic_model\n",
    "\n",
    "        self.openai_llm = ChatOpenAI(\n",
    "            model=openai_model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        self.anthropic_llm = ChatAnthropic(\n",
    "            model=anthropic_model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Cumulative cost tracker\n",
    "        self.total_cost = 0.0\n",
    "        self.call_count = 0\n",
    "        self.call_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _estimate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Estimate cost in USD for a single call.\"\"\"\n",
    "        pricing = self.PRICING.get(model, {\"input\": 1.0, \"output\": 3.0})\n",
    "        cost = (input_tokens * pricing[\"input\"] + output_tokens * pricing[\"output\"]) / 1_000_000\n",
    "        return cost\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        user_message: str,\n",
    "        provider: LLMProvider = LLMProvider.OPENAI,\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Make a real LLM call and return structured response with token usage.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=user_message),\n",
    "        ]\n",
    "\n",
    "        if provider == LLMProvider.OPENAI:\n",
    "            llm = self.openai_llm\n",
    "            model_name = self.openai_model_name\n",
    "        else:\n",
    "            llm = self.anthropic_llm\n",
    "            model_name = self.anthropic_model_name\n",
    "\n",
    "        start = time.time()\n",
    "        result = llm.invoke(messages)\n",
    "        latency = time.time() - start\n",
    "\n",
    "        # Extract token usage from response metadata\n",
    "        usage = result.usage_metadata or {}\n",
    "        input_tokens = usage.get(\"input_tokens\", 0)\n",
    "        output_tokens = usage.get(\"output_tokens\", 0)\n",
    "        total_tokens = usage.get(\"total_tokens\", input_tokens + output_tokens)\n",
    "\n",
    "        cost = self._estimate_cost(model_name, input_tokens, output_tokens)\n",
    "        self.total_cost += cost\n",
    "        self.call_count += 1\n",
    "\n",
    "        response = LLMResponse(\n",
    "            content=result.content,\n",
    "            provider=provider.value,\n",
    "            model=model_name,\n",
    "            input_tokens=input_tokens,\n",
    "            output_tokens=output_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            latency_seconds=round(latency, 3),\n",
    "            cost_estimate=cost,\n",
    "        )\n",
    "\n",
    "        self.call_log.append({\n",
    "            \"model\": model_name,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"latency\": latency,\n",
    "            \"cost\": cost,\n",
    "        })\n",
    "\n",
    "        return response\n",
    "\n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return cumulative cost summary.\"\"\"\n",
    "        if not self.call_log:\n",
    "            return {\"total_calls\": 0, \"total_cost\": 0.0}\n",
    "        df = pd.DataFrame(self.call_log)\n",
    "        return {\n",
    "            \"total_calls\": self.call_count,\n",
    "            \"total_cost_usd\": round(self.total_cost, 6),\n",
    "            \"avg_latency_s\": round(df[\"latency\"].mean(), 3),\n",
    "            \"avg_input_tokens\": int(df[\"input_tokens\"].mean()),\n",
    "            \"avg_output_tokens\": int(df[\"output_tokens\"].mean()),\n",
    "            \"by_model\": df.groupby(\"model\")[\"cost\"].sum().to_dict(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Instantiate the service\n",
    "llm_service = LLMService(\n",
    "    openai_model=\"gpt-4o-mini\",\n",
    "    anthropic_model=\"claude-3-5-haiku-latest\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# Quick smoke test\n",
    "test_resp = llm_service.call(\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    user_message=\"Say hello in one sentence.\",\n",
    "    provider=LLMProvider.OPENAI,\n",
    ")\n",
    "print(\"‚úÖ LLM Service initialized and tested!\")\n",
    "print(f\"   Response: {test_resp.content[:100]}\")\n",
    "print(f\"   Tokens: {test_resp.total_tokens} | Latency: {test_resp.latency_seconds}s | Cost: ${test_resp.cost_estimate:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Components and Templates\n",
    "\n",
    "Same building blocks as the simulated version, but now each template produces a real system prompt sent to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryType(Enum):\n",
    "    \"\"\"Types of user queries that require different prompt strategies.\"\"\"\n",
    "    FACTUAL = \"factual\"\n",
    "    CREATIVE = \"creative\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    CODING = \"coding\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "\n",
    "\n",
    "class ToneStyle(Enum):\n",
    "    \"\"\"Tone options for prompt responses.\"\"\"\n",
    "    FORMAL = \"formal\"\n",
    "    CASUAL = \"casual\"\n",
    "    TECHNICAL = \"technical\"\n",
    "    FRIENDLY = \"friendly\"\n",
    "\n",
    "\n",
    "class DetailLevel(Enum):\n",
    "    \"\"\"Level of detail in responses.\"\"\"\n",
    "    BRIEF = \"brief\"\n",
    "    MODERATE = \"moderate\"\n",
    "    DETAILED = \"detailed\"\n",
    "    COMPREHENSIVE = \"comprehensive\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    \"\"\"A prompt template with configurable components.\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    system_instruction: str\n",
    "    supports_examples: bool = True\n",
    "    supports_chain_of_thought: bool = True\n",
    "\n",
    "    def render_system_prompt(\n",
    "        self,\n",
    "        tone: ToneStyle = ToneStyle.FORMAL,\n",
    "        detail: DetailLevel = DetailLevel.MODERATE,\n",
    "        use_chain_of_thought: bool = False,\n",
    "        examples: List[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Build the full system prompt.\"\"\"\n",
    "        parts = [self.system_instruction]\n",
    "\n",
    "        tone_map = {\n",
    "            ToneStyle.FORMAL: \"Use a professional and formal tone.\",\n",
    "            ToneStyle.CASUAL: \"Use a casual and conversational tone.\",\n",
    "            ToneStyle.TECHNICAL: \"Use precise technical language.\",\n",
    "            ToneStyle.FRIENDLY: \"Be warm and approachable.\",\n",
    "        }\n",
    "        parts.append(tone_map[tone])\n",
    "\n",
    "        detail_map = {\n",
    "            DetailLevel.BRIEF: \"Keep your response concise (2-3 sentences).\",\n",
    "            DetailLevel.MODERATE: \"Provide a balanced response with key details.\",\n",
    "            DetailLevel.DETAILED: \"Provide a thorough and detailed response.\",\n",
    "            DetailLevel.COMPREHENSIVE: \"Provide an exhaustive response covering all aspects.\",\n",
    "        }\n",
    "        parts.append(detail_map[detail])\n",
    "\n",
    "        if self.supports_examples and examples:\n",
    "            parts.append(\"\\nHere are some examples for reference:\")\n",
    "            for i, ex in enumerate(examples, 1):\n",
    "                parts.append(f\"  {i}. {ex}\")\n",
    "\n",
    "        if self.supports_chain_of_thought and use_chain_of_thought:\n",
    "            parts.append(\"\\nThink through this step by step before providing your final answer.\")\n",
    "\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Available templates ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "PROMPT_TEMPLATES: Dict[str, PromptTemplate] = {\n",
    "    \"standard\": PromptTemplate(\n",
    "        id=\"standard\",\n",
    "        name=\"Standard Assistant\",\n",
    "        system_instruction=\"You are a helpful AI assistant. Answer the user's question accurately and helpfully.\",\n",
    "    ),\n",
    "    \"expert\": PromptTemplate(\n",
    "        id=\"expert\",\n",
    "        name=\"Domain Expert\",\n",
    "        system_instruction=\"You are an expert in the relevant domain. Provide authoritative, well-researched answers with references where appropriate.\",\n",
    "    ),\n",
    "    \"teacher\": PromptTemplate(\n",
    "        id=\"teacher\",\n",
    "        name=\"Patient Teacher\",\n",
    "        system_instruction=\"You are a patient teacher. Explain concepts clearly, use analogies, and check for understanding.\",\n",
    "    ),\n",
    "    \"concise\": PromptTemplate(\n",
    "        id=\"concise\",\n",
    "        name=\"Concise Responder\",\n",
    "        system_instruction=\"You are a concise assistant. Provide direct, no-fluff answers.\",\n",
    "        supports_chain_of_thought=False,\n",
    "    ),\n",
    "    \"creative\": PromptTemplate(\n",
    "        id=\"creative\",\n",
    "        name=\"Creative Writer\",\n",
    "        system_instruction=\"You are a creative assistant. Think outside the box and provide imaginative, engaging responses.\",\n",
    "    ),\n",
    "    \"analyst\": PromptTemplate(\n",
    "        id=\"analyst\",\n",
    "        name=\"Data Analyst\",\n",
    "        system_instruction=\"You are a data analyst. Approach problems systematically with structured, data-driven insights.\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Defined {len(PROMPT_TEMPLATES)} prompt templates:\")\n",
    "for tid, t in PROMPT_TEMPLATES.items():\n",
    "    print(f\"   üìù {t.name} ({tid})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-as-Judge Reward Model\n",
    "\n",
    "Instead of simulated rewards, we use a **separate LLM call** to evaluate response quality on multiple axes.\n",
    "The judge LLM scores the response and we combine that with real token-cost data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityScores(BaseModel):\n",
    "    \"\"\"Pydantic model for structured judge output.\"\"\"\n",
    "    relevance: float = Field(ge=0, le=1, description=\"How relevant the response is to the query\")\n",
    "    accuracy: float = Field(ge=0, le=1, description=\"Factual accuracy of the response\")\n",
    "    helpfulness: float = Field(ge=0, le=1, description=\"How helpful the response is\")\n",
    "    tone_match: float = Field(ge=0, le=1, description=\"How well the tone matches the requested style\")\n",
    "    detail_match: float = Field(ge=0, le=1, description=\"How well the detail level matches the request\")\n",
    "    reasoning: str = Field(description=\"Brief explanation of the scores\")\n",
    "\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are an expert quality evaluator for AI-generated responses.\n",
    "You will receive:\n",
    "- The original user query\n",
    "- The system prompt used to generate the response\n",
    "- The AI-generated response\n",
    "- The requested tone and detail level\n",
    "\n",
    "Score the response on each dimension from 0.0 to 1.0.\n",
    "\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{{\n",
    "  \"relevance\": <float 0-1>,\n",
    "  \"accuracy\": <float 0-1>,\n",
    "  \"helpfulness\": <float 0-1>,\n",
    "  \"tone_match\": <float 0-1>,\n",
    "  \"detail_match\": <float 0-1>,\n",
    "  \"reasoning\": \"<brief explanation>\"\n",
    "}}\n",
    "\n",
    "Be fair but critical. Only give 1.0 for truly excellent performance on that dimension.\"\"\"\n",
    "\n",
    "\n",
    "class LLMJudge:\n",
    "    \"\"\"\n",
    "    Uses a separate LLM call to evaluate the quality of a response.\n",
    "    Produces a composite reward signal combining quality scores and cost.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_service: LLMService, judge_provider: LLMProvider = LLMProvider.OPENAI):\n",
    "        self.llm_service = llm_service\n",
    "        self.judge_provider = judge_provider\n",
    "        self.evaluation_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        query: str,\n",
    "        system_prompt_used: str,\n",
    "        response: str,\n",
    "        requested_tone: str,\n",
    "        requested_detail: str,\n",
    "    ) -> QualityScores:\n",
    "        \"\"\"Ask the judge LLM to score the response.\"\"\"\n",
    "        eval_message = (\n",
    "            f\"## User Query\\n{query}\\n\\n\"\n",
    "            f\"## System Prompt Used\\n{system_prompt_used[:500]}\\n\\n\"\n",
    "            f\"## AI Response\\n{response[:1500]}\\n\\n\"\n",
    "            f\"## Requested Style\\n- Tone: {requested_tone}\\n- Detail level: {requested_detail}\\n\\n\"\n",
    "            f\"Evaluate the response now.\"\n",
    "        )\n",
    "\n",
    "        judge_resp = self.llm_service.call(\n",
    "            system_prompt=JUDGE_SYSTEM_PROMPT,\n",
    "            user_message=eval_message,\n",
    "            provider=self.judge_provider,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Parse JSON from judge response\n",
    "            raw = judge_resp.content.strip()\n",
    "            # Handle markdown code fences if present\n",
    "            if raw.startswith(\"```\"):\n",
    "                raw = raw.split(\"```\")[1]\n",
    "                if raw.startswith(\"json\"):\n",
    "                    raw = raw[4:]\n",
    "            scores = QualityScores(**json.loads(raw))\n",
    "        except Exception:\n",
    "            # Fallback: neutral scores\n",
    "            scores = QualityScores(\n",
    "                relevance=0.5, accuracy=0.5, helpfulness=0.5,\n",
    "                tone_match=0.5, detail_match=0.5,\n",
    "                reasoning=\"Judge parsing failed; using default scores.\",\n",
    "            )\n",
    "\n",
    "        self.evaluation_log.append({\n",
    "            \"relevance\": scores.relevance,\n",
    "            \"accuracy\": scores.accuracy,\n",
    "            \"helpfulness\": scores.helpfulness,\n",
    "            \"tone_match\": scores.tone_match,\n",
    "            \"detail_match\": scores.detail_match,\n",
    "        })\n",
    "\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def composite_reward(\n",
    "        scores: QualityScores,\n",
    "        token_cost: float,\n",
    "        latency: float,\n",
    "        cost_weight: float = 0.15,\n",
    "        latency_weight: float = 0.05,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute a single scalar reward from quality scores and efficiency metrics.\n",
    "\n",
    "        reward = quality_mean - cost_weight * normalized_cost - latency_weight * normalized_latency\n",
    "        \"\"\"\n",
    "        quality = np.mean([\n",
    "            scores.relevance,\n",
    "            scores.accuracy,\n",
    "            scores.helpfulness,\n",
    "            scores.tone_match,\n",
    "            scores.detail_match,\n",
    "        ])\n",
    "        # Normalize cost (assume ~$0.001 per call for gpt-4o-mini is 'normal')\n",
    "        norm_cost = min(token_cost / 0.002, 1.0)\n",
    "        # Normalize latency (assume 2s is 'normal')\n",
    "        norm_latency = min(latency / 4.0, 1.0)\n",
    "\n",
    "        reward = quality - cost_weight * norm_cost - latency_weight * norm_latency\n",
    "        return float(np.clip(reward, 0, 1))\n",
    "\n",
    "\n",
    "judge = LLMJudge(llm_service, judge_provider=LLMProvider.OPENAI)\n",
    "print(\"‚úÖ LLM-as-Judge reward model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Bank and User Profiles\n",
    "\n",
    "We define a set of real queries (sent to the LLM) and user profiles that encode preferences for the RL agents to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Query:\n",
    "    \"\"\"A user query with metadata.\"\"\"\n",
    "    text: str\n",
    "    query_type: QueryType\n",
    "    complexity: float  # 0-1\n",
    "    requires_examples: bool = False\n",
    "    requires_reasoning: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserProfile:\n",
    "    \"\"\"User profile encoding preferences (used in state representation).\"\"\"\n",
    "    user_id: str\n",
    "    preferred_tone: ToneStyle\n",
    "    preferred_detail: DetailLevel\n",
    "    preferred_templates: List[str]\n",
    "    expertise_level: float = 0.5  # 0-1\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Query bank ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "QUERY_BANK: List[Query] = [\n",
    "    # Factual\n",
    "    Query(\"What is the difference between TCP and UDP?\", QueryType.FACTUAL, 0.4),\n",
    "    Query(\"Explain how transformers work in deep learning.\", QueryType.FACTUAL, 0.7, requires_reasoning=True),\n",
    "    Query(\"What causes the northern lights?\", QueryType.FACTUAL, 0.3),\n",
    "    # Creative\n",
    "    Query(\"Write a short poem about the beauty of mathematics.\", QueryType.CREATIVE, 0.5),\n",
    "    Query(\"Come up with three creative names for a coffee shop run by robots.\", QueryType.CREATIVE, 0.3),\n",
    "    # Analytical\n",
    "    Query(\"Compare the pros and cons of microservices vs monolithic architecture.\", QueryType.ANALYTICAL, 0.6, requires_reasoning=True),\n",
    "    Query(\"What are the trade-offs between batch and stream processing?\", QueryType.ANALYTICAL, 0.7, requires_reasoning=True),\n",
    "    # Coding\n",
    "    Query(\"Write a Python function that implements binary search.\", QueryType.CODING, 0.4, requires_examples=True),\n",
    "    Query(\"Show how to implement a simple LRU cache in Python.\", QueryType.CODING, 0.6, requires_examples=True),\n",
    "    # Conversational\n",
    "    Query(\"What are some good strategies for staying productive while working remotely?\", QueryType.CONVERSATIONAL, 0.3),\n",
    "    Query(\"I'm feeling overwhelmed with my workload. Any advice?\", QueryType.CONVERSATIONAL, 0.2),\n",
    "]\n",
    "\n",
    "# ‚îÄ‚îÄ User profiles ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "USER_PROFILES: Dict[str, UserProfile] = {\n",
    "    \"tech_expert\": UserProfile(\n",
    "        user_id=\"tech_expert\",\n",
    "        preferred_tone=ToneStyle.TECHNICAL,\n",
    "        preferred_detail=DetailLevel.DETAILED,\n",
    "        preferred_templates=[\"expert\", \"analyst\"],\n",
    "        expertise_level=0.9,\n",
    "    ),\n",
    "    \"casual_user\": UserProfile(\n",
    "        user_id=\"casual_user\",\n",
    "        preferred_tone=ToneStyle.FRIENDLY,\n",
    "        preferred_detail=DetailLevel.MODERATE,\n",
    "        preferred_templates=[\"standard\", \"teacher\"],\n",
    "        expertise_level=0.3,\n",
    "    ),\n",
    "    \"busy_pro\": UserProfile(\n",
    "        user_id=\"busy_pro\",\n",
    "        preferred_tone=ToneStyle.FORMAL,\n",
    "        preferred_detail=DetailLevel.BRIEF,\n",
    "        preferred_templates=[\"concise\", \"expert\"],\n",
    "        expertise_level=0.7,\n",
    "    ),\n",
    "    \"student\": UserProfile(\n",
    "        user_id=\"student\",\n",
    "        preferred_tone=ToneStyle.FRIENDLY,\n",
    "        preferred_detail=DetailLevel.COMPREHENSIVE,\n",
    "        preferred_templates=[\"teacher\", \"standard\"],\n",
    "        expertise_level=0.4,\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Query bank: {len(QUERY_BANK)} queries\")\n",
    "print(f\"‚úÖ User profiles: {len(USER_PROFILES)} profiles\")\n",
    "for uid, up in USER_PROFILES.items():\n",
    "    print(f\"   üë§ {uid}: tone={up.preferred_tone.value}, detail={up.preferred_detail.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real LLM Environment\n",
    "\n",
    "The environment wraps the LLM service and the judge to:\n",
    "1. Build a system prompt from the RL-selected configuration\n",
    "2. Call the LLM with the user query\n",
    "3. Score the response with the judge\n",
    "4. Return a composite reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PromptAction:\n",
    "    \"\"\"A complete prompt configuration selected by the RL agent.\"\"\"\n",
    "    template_id: str\n",
    "    tone: ToneStyle\n",
    "    detail: DetailLevel\n",
    "    use_examples: bool\n",
    "    use_chain_of_thought: bool\n",
    "    provider: LLMProvider = LLMProvider.OPENAI\n",
    "\n",
    "    def to_tuple(self) -> tuple:\n",
    "        return (\n",
    "            self.template_id,\n",
    "            self.tone.value,\n",
    "            self.detail.value,\n",
    "            self.use_examples,\n",
    "            self.use_chain_of_thought,\n",
    "            self.provider.value,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_tuple(cls, t: tuple) -> \"PromptAction\":\n",
    "        return cls(\n",
    "            template_id=t[0],\n",
    "            tone=ToneStyle(t[1]),\n",
    "            detail=DetailLevel(t[2]),\n",
    "            use_examples=t[3],\n",
    "            use_chain_of_thought=t[4],\n",
    "            provider=LLMProvider(t[5]) if len(t) > 5 else LLMProvider.OPENAI,\n",
    "        )\n",
    "\n",
    "\n",
    "class RealLLMEnvironment:\n",
    "    \"\"\"\n",
    "    Environment that makes real LLM calls and evaluates responses via an LLM judge.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_service: LLMService,\n",
    "        judge: LLMJudge,\n",
    "        templates: Dict[str, PromptTemplate],\n",
    "        queries: List[Query],\n",
    "        user_profiles: Dict[str, UserProfile],\n",
    "        cost_weight: float = 0.15,\n",
    "        latency_weight: float = 0.05,\n",
    "    ):\n",
    "        self.llm_service = llm_service\n",
    "        self.judge = judge\n",
    "        self.templates = templates\n",
    "        self.queries = queries\n",
    "        self.user_profiles = user_profiles\n",
    "        self.cost_weight = cost_weight\n",
    "        self.latency_weight = latency_weight\n",
    "        self.interaction_history: List[Dict[str, Any]] = []\n",
    "\n",
    "    def get_random_query(self) -> Tuple[Query, str]:\n",
    "        query = random.choice(self.queries)\n",
    "        user_id = random.choice(list(self.user_profiles.keys()))\n",
    "        return query, user_id\n",
    "\n",
    "    def step(self, query: Query, user_id: str, action: PromptAction) -> Tuple[float, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Execute one full RL step:\n",
    "          1. Build system prompt from action\n",
    "          2. Call LLM\n",
    "          3. Judge the response\n",
    "          4. Compute composite reward\n",
    "        Returns (reward, info_dict).\n",
    "        \"\"\"\n",
    "        template = self.templates[action.template_id]\n",
    "\n",
    "        # 1) Render system prompt\n",
    "        system_prompt = template.render_system_prompt(\n",
    "            tone=action.tone,\n",
    "            detail=action.detail,\n",
    "            use_chain_of_thought=action.use_chain_of_thought,\n",
    "        )\n",
    "\n",
    "        # 2) Call the LLM\n",
    "        llm_response = self.llm_service.call(\n",
    "            system_prompt=system_prompt,\n",
    "            user_message=query.text,\n",
    "            provider=action.provider,\n",
    "        )\n",
    "\n",
    "        # 3) Judge the response\n",
    "        scores = self.judge.evaluate(\n",
    "            query=query.text,\n",
    "            system_prompt_used=system_prompt,\n",
    "            response=llm_response.content,\n",
    "            requested_tone=action.tone.value,\n",
    "            requested_detail=action.detail.value,\n",
    "        )\n",
    "\n",
    "        # 4) Composite reward\n",
    "        reward = LLMJudge.composite_reward(\n",
    "            scores,\n",
    "            token_cost=llm_response.cost_estimate,\n",
    "            latency=llm_response.latency_seconds,\n",
    "            cost_weight=self.cost_weight,\n",
    "            latency_weight=self.latency_weight,\n",
    "        )\n",
    "\n",
    "        info = {\n",
    "            \"query\": query.text,\n",
    "            \"query_type\": query.query_type.value,\n",
    "            \"user_id\": user_id,\n",
    "            \"template\": action.template_id,\n",
    "            \"tone\": action.tone.value,\n",
    "            \"detail\": action.detail.value,\n",
    "            \"use_examples\": action.use_examples,\n",
    "            \"use_cot\": action.use_chain_of_thought,\n",
    "            \"provider\": action.provider.value,\n",
    "            \"response_preview\": llm_response.content[:200],\n",
    "            \"input_tokens\": llm_response.input_tokens,\n",
    "            \"output_tokens\": llm_response.output_tokens,\n",
    "            \"latency\": llm_response.latency_seconds,\n",
    "            \"cost\": llm_response.cost_estimate,\n",
    "            \"relevance\": scores.relevance,\n",
    "            \"accuracy\": scores.accuracy,\n",
    "            \"helpfulness\": scores.helpfulness,\n",
    "            \"tone_match\": scores.tone_match,\n",
    "            \"detail_match\": scores.detail_match,\n",
    "            \"reward\": reward,\n",
    "        }\n",
    "        self.interaction_history.append(info)\n",
    "        return reward, info\n",
    "\n",
    "\n",
    "# Create the real environment\n",
    "env = RealLLMEnvironment(\n",
    "    llm_service=llm_service,\n",
    "    judge=judge,\n",
    "    templates=PROMPT_TEMPLATES,\n",
    "    queries=QUERY_BANK,\n",
    "    user_profiles=USER_PROFILES,\n",
    ")\n",
    "print(\"‚úÖ Real LLM environment created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RL Algorithms: Multi-Armed Bandits\n",
    "\n",
    "Identical bandit algorithms to the simulated notebook ‚Äî the only change is that rewards now come from real LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit(ABC):\n",
    "    \"\"\"Abstract base class for bandit algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, n_arms: int, arm_names: List[str] = None):\n",
    "        self.n_arms = n_arms\n",
    "        self.arm_names = arm_names or [f\"arm_{i}\" for i in range(n_arms)]\n",
    "        self.counts = np.zeros(n_arms)\n",
    "        self.values = np.zeros(n_arms)\n",
    "        self.history: List[Dict] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def select_arm(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def update(self, arm: int, reward: float):\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        self.values[arm] += (reward - self.values[arm]) / n\n",
    "        self.history.append({\"arm\": arm, \"reward\": reward})\n",
    "\n",
    "    def get_best_arm(self) -> int:\n",
    "        return int(np.argmax(self.values))\n",
    "\n",
    "\n",
    "class EpsilonGreedyBandit(MultiArmedBandit):\n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1, arm_names: List[str] = None):\n",
    "        super().__init__(n_arms, arm_names)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        return self.get_best_arm()\n",
    "\n",
    "\n",
    "class UCBBandit(MultiArmedBandit):\n",
    "    def __init__(self, n_arms: int, c: float = 2.0, arm_names: List[str] = None):\n",
    "        super().__init__(n_arms, arm_names)\n",
    "        self.c = c\n",
    "        self.total_counts = 0\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        self.total_counts += 1\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        ucb = self.values + self.c * np.sqrt(np.log(self.total_counts) / self.counts)\n",
    "        return int(np.argmax(ucb))\n",
    "\n",
    "\n",
    "class ThompsonSamplingBandit(MultiArmedBandit):\n",
    "    def __init__(self, n_arms: int, arm_names: List[str] = None):\n",
    "        super().__init__(n_arms, arm_names)\n",
    "        self.alpha = np.ones(n_arms)\n",
    "        self.beta_param = np.ones(n_arms)\n",
    "\n",
    "    def select_arm(self) -> int:\n",
    "        samples = np.random.beta(self.alpha, self.beta_param)\n",
    "        return int(np.argmax(samples))\n",
    "\n",
    "    def update(self, arm: int, reward: float):\n",
    "        super().update(arm, reward)\n",
    "        if reward > 0.5:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta_param[arm] += 1\n",
    "\n",
    "\n",
    "class PromptTemplateBandit:\n",
    "    \"\"\"Bandit-based template selector: one bandit per query type.\"\"\"\n",
    "\n",
    "    def __init__(self, templates: Dict[str, PromptTemplate], algorithm: str = \"thompson\"):\n",
    "        self.template_ids = list(templates.keys())\n",
    "        n = len(self.template_ids)\n",
    "        self.bandits: Dict[QueryType, MultiArmedBandit] = {}\n",
    "        for qt in QueryType:\n",
    "            if algorithm == \"epsilon_greedy\":\n",
    "                self.bandits[qt] = EpsilonGreedyBandit(n, 0.15, self.template_ids)\n",
    "            elif algorithm == \"ucb\":\n",
    "                self.bandits[qt] = UCBBandit(n, 2.0, self.template_ids)\n",
    "            else:\n",
    "                self.bandits[qt] = ThompsonSamplingBandit(n, self.template_ids)\n",
    "\n",
    "    def select_template(self, query_type: QueryType) -> str:\n",
    "        arm = self.bandits[query_type].select_arm()\n",
    "        return self.template_ids[arm]\n",
    "\n",
    "    def update(self, query_type: QueryType, template_id: str, reward: float):\n",
    "        arm = self.template_ids.index(template_id)\n",
    "        self.bandits[query_type].update(arm, reward)\n",
    "\n",
    "    def get_best_templates(self) -> Dict[str, str]:\n",
    "        return {\n",
    "            qt.value: self.template_ids[self.bandits[qt].get_best_arm()]\n",
    "            for qt in QueryType\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Bandit algorithms defined: Epsilon-Greedy, UCB, Thompson Sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RL Algorithm: Q-Learning for Full Prompt Configuration\n",
    "\n",
    "Q-Learning optimizes the *entire* prompt configuration (template + tone + detail + chain-of-thought + provider)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptQLearning:\n",
    "    \"\"\"Q-Learning agent for full prompt configuration optimization.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        templates: Dict[str, PromptTemplate],\n",
    "        learning_rate: float = 0.1,\n",
    "        discount_factor: float = 0.95,\n",
    "        epsilon: float = 0.3,\n",
    "        epsilon_decay: float = 0.99,\n",
    "        min_epsilon: float = 0.05,\n",
    "        include_provider: bool = True,\n",
    "    ):\n",
    "        self.templates = templates\n",
    "        self.template_ids = list(templates.keys())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.include_provider = include_provider\n",
    "\n",
    "        self.q_table: Dict[str, Dict[tuple, float]] = defaultdict(lambda: defaultdict(float))\n",
    "        self.actions = self._generate_actions()\n",
    "        self.reward_history: List[float] = []\n",
    "        self.epsilon_history: List[float] = []\n",
    "\n",
    "    def _generate_actions(self) -> List[PromptAction]:\n",
    "        actions = []\n",
    "        providers = [LLMProvider.OPENAI, LLMProvider.ANTHROPIC] if self.include_provider else [LLMProvider.OPENAI]\n",
    "        for tid in self.template_ids:\n",
    "            tmpl = self.templates[tid]\n",
    "            for tone in ToneStyle:\n",
    "                for detail in DetailLevel:\n",
    "                    for cot in [True, False]:\n",
    "                        if cot and not tmpl.supports_chain_of_thought:\n",
    "                            continue\n",
    "                        for prov in providers:\n",
    "                            actions.append(PromptAction(\n",
    "                                template_id=tid,\n",
    "                                tone=tone,\n",
    "                                detail=detail,\n",
    "                                use_examples=False,  # simplified\n",
    "                                use_chain_of_thought=cot,\n",
    "                                provider=prov,\n",
    "                            ))\n",
    "        return actions\n",
    "\n",
    "    def get_state(self, query: Query, user_id: str = None) -> str:\n",
    "        if user_id:\n",
    "            return f\"{query.query_type.value}_{user_id}\"\n",
    "        return query.query_type.value\n",
    "\n",
    "    def select_action(self, state: str, training: bool = True) -> PromptAction:\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        state_q = self.q_table[state]\n",
    "        if not state_q:\n",
    "            return random.choice(self.actions)\n",
    "        best = max(state_q.keys(), key=lambda a: state_q[a])\n",
    "        return PromptAction.from_tuple(best)\n",
    "\n",
    "    def update(self, state: str, action: PromptAction, reward: float, next_state: str = None):\n",
    "        at = action.to_tuple()\n",
    "        old = self.q_table[state][at]\n",
    "        max_next = max(self.q_table[next_state].values()) if (next_state and self.q_table[next_state]) else 0\n",
    "        self.q_table[state][at] = old + self.learning_rate * (\n",
    "            reward + self.discount_factor * max_next - old\n",
    "        )\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        self.reward_history.append(reward)\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "\n",
    "    def get_best_action(self, state: str) -> Optional[PromptAction]:\n",
    "        sq = self.q_table[state]\n",
    "        if not sq:\n",
    "            return None\n",
    "        return PromptAction.from_tuple(max(sq.keys(), key=lambda a: sq[a]))\n",
    "\n",
    "    def get_policy_summary(self) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for state, actions in self.q_table.items():\n",
    "            if actions:\n",
    "                best_t = max(actions.keys(), key=lambda a: actions[a])\n",
    "                a = PromptAction.from_tuple(best_t)\n",
    "                rows.append({\n",
    "                    \"state\": state,\n",
    "                    \"template\": a.template_id,\n",
    "                    \"tone\": a.tone.value,\n",
    "                    \"detail\": a.detail.value,\n",
    "                    \"cot\": a.use_chain_of_thought,\n",
    "                    \"provider\": a.provider.value,\n",
    "                    \"q_value\": round(actions[best_t], 4),\n",
    "                })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Q-Learning agent defined\")\n",
    "print(f\"   Action space: {len(PromptQLearning(PROMPT_TEMPLATES).actions)} configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LangGraph Workflow: RL Prompt Optimization Loop\n",
    "\n",
    "We use **LangGraph** to orchestrate the RL optimization loop as a state machine:\n",
    "\n",
    "```\n",
    "START ‚Üí select_action ‚Üí call_llm ‚Üí judge_response ‚Üí compute_reward ‚Üí (loop or END)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLWorkflowState(TypedDict):\n",
    "    \"\"\"State flowing through the LangGraph optimization loop.\"\"\"\n",
    "    # Input\n",
    "    query: str\n",
    "    query_type: str\n",
    "    user_id: str\n",
    "    # RL agent picks\n",
    "    template_id: str\n",
    "    tone: str\n",
    "    detail: str\n",
    "    use_cot: bool\n",
    "    provider: str\n",
    "    # LLM response\n",
    "    system_prompt: str\n",
    "    response: str\n",
    "    input_tokens: int\n",
    "    output_tokens: int\n",
    "    latency: float\n",
    "    cost: float\n",
    "    # Judge scores\n",
    "    relevance: float\n",
    "    accuracy: float\n",
    "    helpfulness: float\n",
    "    tone_match: float\n",
    "    detail_match: float\n",
    "    # Reward\n",
    "    reward: float\n",
    "    # Iteration tracking\n",
    "    iteration: int\n",
    "    max_iterations: int\n",
    "    all_rewards: list\n",
    "\n",
    "\n",
    "def build_rl_workflow(\n",
    "    rl_agent,  # PromptQLearning or PromptTemplateBandit\n",
    "    llm_service: LLMService,\n",
    "    judge: LLMJudge,\n",
    "    templates: Dict[str, PromptTemplate],\n",
    "    queries: List[Query],\n",
    "    user_profiles: Dict[str, UserProfile],\n",
    "    agent_type: str = \"qlearning\",  # or \"bandit\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a LangGraph state machine that runs the RL training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def select_action_node(state: RLWorkflowState) -> dict:\n",
    "        \"\"\"RL agent selects a prompt configuration.\"\"\"\n",
    "        query = random.choice(queries)\n",
    "        user_id = random.choice(list(user_profiles.keys()))\n",
    "\n",
    "        if agent_type == \"qlearning\":\n",
    "            q_state = rl_agent.get_state(query, user_id)\n",
    "            action = rl_agent.select_action(q_state, training=True)\n",
    "            return {\n",
    "                \"query\": query.text,\n",
    "                \"query_type\": query.query_type.value,\n",
    "                \"user_id\": user_id,\n",
    "                \"template_id\": action.template_id,\n",
    "                \"tone\": action.tone.value,\n",
    "                \"detail\": action.detail.value,\n",
    "                \"use_cot\": action.use_chain_of_thought,\n",
    "                \"provider\": action.provider.value,\n",
    "            }\n",
    "        else:  # bandit\n",
    "            template_id = rl_agent.select_template(query.query_type)\n",
    "            return {\n",
    "                \"query\": query.text,\n",
    "                \"query_type\": query.query_type.value,\n",
    "                \"user_id\": user_id,\n",
    "                \"template_id\": template_id,\n",
    "                \"tone\": ToneStyle.FORMAL.value,\n",
    "                \"detail\": DetailLevel.MODERATE.value,\n",
    "                \"use_cot\": query.requires_reasoning,\n",
    "                \"provider\": LLMProvider.OPENAI.value,\n",
    "            }\n",
    "\n",
    "    def call_llm_node(state: RLWorkflowState) -> dict:\n",
    "        \"\"\"Call the real LLM with the selected prompt configuration.\"\"\"\n",
    "        template = templates[state[\"template_id\"]]\n",
    "        system_prompt = template.render_system_prompt(\n",
    "            tone=ToneStyle(state[\"tone\"]),\n",
    "            detail=DetailLevel(state[\"detail\"]),\n",
    "            use_chain_of_thought=state[\"use_cot\"],\n",
    "        )\n",
    "        resp = llm_service.call(\n",
    "            system_prompt=system_prompt,\n",
    "            user_message=state[\"query\"],\n",
    "            provider=LLMProvider(state[\"provider\"]),\n",
    "        )\n",
    "        return {\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"response\": resp.content,\n",
    "            \"input_tokens\": resp.input_tokens,\n",
    "            \"output_tokens\": resp.output_tokens,\n",
    "            \"latency\": resp.latency_seconds,\n",
    "            \"cost\": resp.cost_estimate,\n",
    "        }\n",
    "\n",
    "    def judge_response_node(state: RLWorkflowState) -> dict:\n",
    "        \"\"\"Use the LLM judge to score the response.\"\"\"\n",
    "        scores = judge.evaluate(\n",
    "            query=state[\"query\"],\n",
    "            system_prompt_used=state[\"system_prompt\"],\n",
    "            response=state[\"response\"],\n",
    "            requested_tone=state[\"tone\"],\n",
    "            requested_detail=state[\"detail\"],\n",
    "        )\n",
    "        return {\n",
    "            \"relevance\": scores.relevance,\n",
    "            \"accuracy\": scores.accuracy,\n",
    "            \"helpfulness\": scores.helpfulness,\n",
    "            \"tone_match\": scores.tone_match,\n",
    "            \"detail_match\": scores.detail_match,\n",
    "        }\n",
    "\n",
    "    def compute_reward_node(state: RLWorkflowState) -> dict:\n",
    "        \"\"\"Compute composite reward and update RL agent.\"\"\"\n",
    "        scores = QualityScores(\n",
    "            relevance=state[\"relevance\"],\n",
    "            accuracy=state[\"accuracy\"],\n",
    "            helpfulness=state[\"helpfulness\"],\n",
    "            tone_match=state[\"tone_match\"],\n",
    "            detail_match=state[\"detail_match\"],\n",
    "            reasoning=\"\",\n",
    "        )\n",
    "        reward = LLMJudge.composite_reward(\n",
    "            scores,\n",
    "            token_cost=state[\"cost\"],\n",
    "            latency=state[\"latency\"],\n",
    "        )\n",
    "\n",
    "        # Update the RL agent\n",
    "        if agent_type == \"qlearning\":\n",
    "            q_query = Query(state[\"query\"], QueryType(state[\"query_type\"]), 0.5)\n",
    "            q_state = rl_agent.get_state(q_query, state[\"user_id\"])\n",
    "            action = PromptAction(\n",
    "                template_id=state[\"template_id\"],\n",
    "                tone=ToneStyle(state[\"tone\"]),\n",
    "                detail=DetailLevel(state[\"detail\"]),\n",
    "                use_examples=False,\n",
    "                use_chain_of_thought=state[\"use_cot\"],\n",
    "                provider=LLMProvider(state[\"provider\"]),\n",
    "            )\n",
    "            rl_agent.update(q_state, action, reward)\n",
    "        else:\n",
    "            rl_agent.update(\n",
    "                QueryType(state[\"query_type\"]),\n",
    "                state[\"template_id\"],\n",
    "                reward,\n",
    "            )\n",
    "\n",
    "        prev_rewards = state.get(\"all_rewards\", []) or []\n",
    "\n",
    "        # Print progress\n",
    "        it = state[\"iteration\"] + 1\n",
    "        print(f\"   [{it}/{state['max_iterations']}] reward={reward:.3f} \"\n",
    "              f\"template={state['template_id']} tone={state['tone']} \"\n",
    "              f\"detail={state['detail']} provider={state['provider']}\")\n",
    "\n",
    "        return {\n",
    "            \"reward\": reward,\n",
    "            \"iteration\": it,\n",
    "            \"all_rewards\": prev_rewards + [reward],\n",
    "        }\n",
    "\n",
    "    def should_continue(state: RLWorkflowState) -> Literal[\"select_action\", \"__end__\"]:\n",
    "        if state[\"iteration\"] >= state[\"max_iterations\"]:\n",
    "            return \"__end__\"\n",
    "        return \"select_action\"\n",
    "\n",
    "    # ‚îÄ‚îÄ Build the graph ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    workflow = StateGraph(RLWorkflowState)\n",
    "    workflow.add_node(\"select_action\", select_action_node)\n",
    "    workflow.add_node(\"call_llm\", call_llm_node)\n",
    "    workflow.add_node(\"judge_response\", judge_response_node)\n",
    "    workflow.add_node(\"compute_reward\", compute_reward_node)\n",
    "\n",
    "    workflow.add_edge(START, \"select_action\")\n",
    "    workflow.add_edge(\"select_action\", \"call_llm\")\n",
    "    workflow.add_edge(\"call_llm\", \"judge_response\")\n",
    "    workflow.add_edge(\"judge_response\", \"compute_reward\")\n",
    "    workflow.add_conditional_edges(\"compute_reward\", should_continue)\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "print(\"‚úÖ LangGraph RL workflow builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training with Real LLM Calls\n",
    "\n",
    "We train both **Thompson Sampling bandit** and **Q-Learning** agents using the LangGraph workflow.\n",
    "\n",
    "> ‚ö†Ô∏è **Cost note:** Each iteration makes **2 LLM calls** (one generation + one judge). We keep iterations modest to control costs. With `gpt-4o-mini` this typically costs < \\$0.10 for 30 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "N_BANDIT_ITERS = 30   # Thompson Sampling training iterations\n",
    "N_QLEARN_ITERS = 30   # Q-Learning training iterations\n",
    "\n",
    "print(f\"üìã Training plan:\")\n",
    "print(f\"   Bandit iterations:     {N_BANDIT_ITERS}\")\n",
    "print(f\"   Q-Learning iterations: {N_QLEARN_ITERS}\")\n",
    "print(f\"   Total LLM calls:       ~{(N_BANDIT_ITERS + N_QLEARN_ITERS) * 2} (generation + judge)\")\n",
    "print(f\"   Estimated cost:        ~${(N_BANDIT_ITERS + N_QLEARN_ITERS) * 2 * 0.0005:.2f} (gpt-4o-mini)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Train Thompson Sampling Bandit ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\nüé∞ Training Thompson Sampling Bandit...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bandit_agent = PromptTemplateBandit(PROMPT_TEMPLATES, algorithm=\"thompson\")\n",
    "\n",
    "bandit_workflow = build_rl_workflow(\n",
    "    rl_agent=bandit_agent,\n",
    "    llm_service=llm_service,\n",
    "    judge=judge,\n",
    "    templates=PROMPT_TEMPLATES,\n",
    "    queries=QUERY_BANK,\n",
    "    user_profiles=USER_PROFILES,\n",
    "    agent_type=\"bandit\",\n",
    ")\n",
    "\n",
    "initial_state = {\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": N_BANDIT_ITERS,\n",
    "    \"all_rewards\": [],\n",
    "    \"query\": \"\", \"query_type\": \"\", \"user_id\": \"\",\n",
    "    \"template_id\": \"\", \"tone\": \"\", \"detail\": \"\",\n",
    "    \"use_cot\": False, \"provider\": \"\",\n",
    "    \"system_prompt\": \"\", \"response\": \"\",\n",
    "    \"input_tokens\": 0, \"output_tokens\": 0,\n",
    "    \"latency\": 0.0, \"cost\": 0.0,\n",
    "    \"relevance\": 0.0, \"accuracy\": 0.0,\n",
    "    \"helpfulness\": 0.0, \"tone_match\": 0.0,\n",
    "    \"detail_match\": 0.0, \"reward\": 0.0,\n",
    "}\n",
    "\n",
    "bandit_result = bandit_workflow.invoke(initial_state)\n",
    "\n",
    "bandit_rewards = bandit_result[\"all_rewards\"]\n",
    "print(f\"\\n‚úÖ Bandit training complete: {len(bandit_rewards)} iterations\")\n",
    "print(f\"   Mean reward: {np.mean(bandit_rewards):.4f}\")\n",
    "print(f\"   Best templates learned: {bandit_agent.get_best_templates()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Train Q-Learning Agent ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\nüß† Training Q-Learning Agent...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "qlearn_agent = PromptQLearning(\n",
    "    PROMPT_TEMPLATES,\n",
    "    learning_rate=0.15,\n",
    "    epsilon=0.4,\n",
    "    epsilon_decay=0.97,\n",
    "    min_epsilon=0.1,\n",
    "    include_provider=True,\n",
    ")\n",
    "\n",
    "qlearn_workflow = build_rl_workflow(\n",
    "    rl_agent=qlearn_agent,\n",
    "    llm_service=llm_service,\n",
    "    judge=judge,\n",
    "    templates=PROMPT_TEMPLATES,\n",
    "    queries=QUERY_BANK,\n",
    "    user_profiles=USER_PROFILES,\n",
    "    agent_type=\"qlearning\",\n",
    ")\n",
    "\n",
    "qlearn_result = qlearn_workflow.invoke({\n",
    "    \"iteration\": 0,\n",
    "    \"max_iterations\": N_QLEARN_ITERS,\n",
    "    \"all_rewards\": [],\n",
    "    \"query\": \"\", \"query_type\": \"\", \"user_id\": \"\",\n",
    "    \"template_id\": \"\", \"tone\": \"\", \"detail\": \"\",\n",
    "    \"use_cot\": False, \"provider\": \"\",\n",
    "    \"system_prompt\": \"\", \"response\": \"\",\n",
    "    \"input_tokens\": 0, \"output_tokens\": 0,\n",
    "    \"latency\": 0.0, \"cost\": 0.0,\n",
    "    \"relevance\": 0.0, \"accuracy\": 0.0,\n",
    "    \"helpfulness\": 0.0, \"tone_match\": 0.0,\n",
    "    \"detail_match\": 0.0, \"reward\": 0.0,\n",
    "})\n",
    "\n",
    "qlearn_rewards = qlearn_result[\"all_rewards\"]\n",
    "print(f\"\\n‚úÖ Q-Learning training complete: {len(qlearn_rewards)} iterations\")\n",
    "print(f\"   Mean reward: {np.mean(qlearn_rewards):.4f}\")\n",
    "print(f\"   Final epsilon: {qlearn_agent.epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Cost summary so far ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "cost_summary = llm_service.get_cost_summary()\n",
    "print(\"\\nüí∞ Cost Summary After Training\")\n",
    "print(\"=\" * 60)\n",
    "for k, v in cost_summary.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window=5):\n",
    "    return pd.Series(data).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"RL Prompt Optimization ‚Äî Real LLM Calls\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# ‚îÄ‚îÄ Plot 1: Reward curves ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax = axes[0, 0]\n",
    "if bandit_rewards:\n",
    "    ax.plot(moving_average(bandit_rewards), label=\"Thompson Sampling\", marker=\".\", markersize=3)\n",
    "if qlearn_rewards:\n",
    "    ax.plot(moving_average(qlearn_rewards), label=\"Q-Learning\", marker=\".\", markersize=3)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Reward (Moving Avg)\")\n",
    "ax.set_title(\"Training Reward Curves\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ‚îÄ‚îÄ Plot 2: Per-iteration reward comparison ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax = axes[0, 1]\n",
    "methods = [\"Thompson\\nSampling\", \"Q-Learning\"]\n",
    "means = [np.mean(bandit_rewards) if bandit_rewards else 0,\n",
    "         np.mean(qlearn_rewards) if qlearn_rewards else 0]\n",
    "stds = [np.std(bandit_rewards) if bandit_rewards else 0,\n",
    "        np.std(qlearn_rewards) if qlearn_rewards else 0]\n",
    "colors = [\"#2ecc71\", \"#9b59b6\"]\n",
    "bars = ax.bar(methods, means, yerr=stds, color=colors, edgecolor=\"black\", capsize=5)\n",
    "for bar, val in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "            f\"{val:.3f}\", ha=\"center\", fontsize=11)\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_title(\"Average Reward Comparison\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# ‚îÄ‚îÄ Plot 3: Judge quality breakdown (recent interactions) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax = axes[1, 0]\n",
    "if judge.evaluation_log:\n",
    "    last_n = min(20, len(judge.evaluation_log))\n",
    "    eval_df = pd.DataFrame(judge.evaluation_log[-last_n:])\n",
    "    dims = [\"relevance\", \"accuracy\", \"helpfulness\", \"tone_match\", \"detail_match\"]\n",
    "    means_q = [eval_df[d].mean() for d in dims]\n",
    "    ax.barh(dims, means_q, color=sns.color_palette(\"viridis\", len(dims)), edgecolor=\"black\")\n",
    "    for i, v in enumerate(means_q):\n",
    "        ax.text(v + 0.01, i, f\"{v:.2f}\", va=\"center\", fontsize=10)\n",
    "    ax.set_xlim(0, 1.1)\n",
    "ax.set_title(\"Judge Quality Scores (recent)\")\n",
    "ax.set_xlabel(\"Score (0-1)\")\n",
    "\n",
    "# ‚îÄ‚îÄ Plot 4: Cumulative cost ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax = axes[1, 1]\n",
    "if llm_service.call_log:\n",
    "    cum_cost = np.cumsum([c[\"cost\"] for c in llm_service.call_log])\n",
    "    ax.plot(cum_cost, color=\"#e74c3c\", linewidth=2)\n",
    "    ax.fill_between(range(len(cum_cost)), cum_cost, alpha=0.15, color=\"#e74c3c\")\n",
    "    ax.set_xlabel(\"LLM Call #\")\n",
    "    ax.set_ylabel(\"Cumulative Cost (USD)\")\n",
    "    ax.set_title(f\"Total Cost: ${cum_cost[-1]:.4f}\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analyzing Learned Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Bandit learned preferences ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"üé∞ Thompson Sampling Bandit ‚Äî Learned Template Preferences\")\n",
    "print(\"=\" * 60)\n",
    "best_map = bandit_agent.get_best_templates()\n",
    "for qt, tid in best_map.items():\n",
    "    print(f\"   {qt:<18} ‚Üí {PROMPT_TEMPLATES[tid].name}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Q-Learning learned policy ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\nüß† Q-Learning ‚Äî Learned Prompt Configurations\")\n",
    "print(\"=\" * 60)\n",
    "policy_df = qlearn_agent.get_policy_summary()\n",
    "if not policy_df.empty:\n",
    "    print(policy_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"   (No policy entries ‚Äî increase training iterations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Policy heatmap ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if not policy_df.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    query_types = [qt.value for qt in QueryType]\n",
    "    template_ids = list(PROMPT_TEMPLATES.keys())\n",
    "\n",
    "    pref_matrix = np.zeros((len(query_types), len(template_ids)))\n",
    "    for i, qt in enumerate(QueryType):\n",
    "        state = qt.value\n",
    "        if state in qlearn_agent.q_table:\n",
    "            for at, qv in qlearn_agent.q_table[state].items():\n",
    "                j = template_ids.index(at[0])\n",
    "                pref_matrix[i, j] = max(pref_matrix[i, j], qv)\n",
    "\n",
    "    sns.heatmap(\n",
    "        pref_matrix, xticklabels=template_ids, yticklabels=query_types,\n",
    "        annot=True, fmt=\".3f\", cmap=\"YlGnBu\", ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Q-Learning: Template Preference by Query Type\")\n",
    "    ax.set_xlabel(\"Template\")\n",
    "    ax.set_ylabel(\"Query Type\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping heatmap ‚Äî no Q-table entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Live Demo: Optimized Prompt Generation\n",
    "\n",
    "Use the trained agents to generate optimized prompts and get real LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_optimized_prompt(\n",
    "    query_text: str,\n",
    "    query_type: QueryType,\n",
    "    user_id: str,\n",
    "    agent: PromptQLearning,\n",
    "    llm_svc: LLMService,\n",
    "    templates: Dict[str, PromptTemplate],\n",
    ") -> None:\n",
    "    \"\"\"Run a single optimized query and display results.\"\"\"\n",
    "    q = Query(query_text, query_type, 0.5)\n",
    "    state = agent.get_state(q, user_id)\n",
    "    action = agent.select_action(state, training=False)\n",
    "\n",
    "    tmpl = templates[action.template_id]\n",
    "    sys_prompt = tmpl.render_system_prompt(\n",
    "        tone=action.tone,\n",
    "        detail=action.detail,\n",
    "        use_chain_of_thought=action.use_chain_of_thought,\n",
    "    )\n",
    "\n",
    "    resp = llm_svc.call(sys_prompt, query_text, provider=action.provider)\n",
    "\n",
    "    print(f\"üìù Query: {query_text}\")\n",
    "    print(f\"   üë§ User: {user_id}\")\n",
    "    print(f\"   üéØ Config: template={action.template_id} | tone={action.tone.value} \"\n",
    "          f\"| detail={action.detail.value} | cot={action.use_chain_of_thought} \"\n",
    "          f\"| provider={action.provider.value}\")\n",
    "    print(f\"   üìä Tokens: {resp.total_tokens} | Latency: {resp.latency_seconds}s \"\n",
    "          f\"| Cost: ${resp.cost_estimate:.6f}\")\n",
    "    print(f\"   üí¨ Response:\\n{resp.content[:500]}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "print(\"üöÄ Live Demo ‚Äî Optimized Prompts via Trained Q-Learning Agent\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "demo_queries = [\n",
    "    (\"What is the CAP theorem?\", QueryType.FACTUAL, \"tech_expert\"),\n",
    "    (\"Write a creative tagline for a green-energy startup.\", QueryType.CREATIVE, \"casual_user\"),\n",
    "    (\"What are the key differences between REST and GraphQL?\", QueryType.ANALYTICAL, \"busy_pro\"),\n",
    "    (\"Write a Python decorator that retries a function 3 times.\", QueryType.CODING, \"student\"),\n",
    "]\n",
    "\n",
    "for qt, qtype, uid in demo_queries:\n",
    "    demo_optimized_prompt(qt, qtype, uid, qlearn_agent, llm_service, PROMPT_TEMPLATES)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Production Integration Class\n",
    "\n",
    "A drop-in class you can use in a real application to serve RL-optimized prompts with online learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionPromptOptimizer:\n",
    "    \"\"\"\n",
    "    Production-ready prompt optimizer.\n",
    "\n",
    "    - Wraps a trained Q-Learning agent\n",
    "    - Calls real LLMs via LangChain\n",
    "    - Evaluates via LLM-as-judge for online learning\n",
    "    - Tracks costs and quality over time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        templates: Dict[str, PromptTemplate],\n",
    "        llm_service: LLMService,\n",
    "        judge: LLMJudge,\n",
    "        pretrained_agent: PromptQLearning = None,\n",
    "        online_learning: bool = True,\n",
    "    ):\n",
    "        self.templates = templates\n",
    "        self.llm_service = llm_service\n",
    "        self.judge = judge\n",
    "        self.agent = pretrained_agent or PromptQLearning(templates)\n",
    "        self.online_learning = online_learning\n",
    "        self.serving_log: List[Dict[str, Any]] = []\n",
    "\n",
    "    def serve(\n",
    "        self,\n",
    "        query_text: str,\n",
    "        query_type: QueryType,\n",
    "        user_id: str = None,\n",
    "        evaluate: bool = True,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Serve an optimized response for a user query.\n",
    "\n",
    "        1. Agent selects optimal prompt config\n",
    "        2. LLM generates response\n",
    "        3. (Optional) Judge evaluates and agent updates online\n",
    "        \"\"\"\n",
    "        q = Query(query_text, query_type, 0.5)\n",
    "        state = self.agent.get_state(q, user_id)\n",
    "        action = self.agent.select_action(state, training=False)\n",
    "\n",
    "        tmpl = self.templates[action.template_id]\n",
    "        sys_prompt = tmpl.render_system_prompt(\n",
    "            tone=action.tone,\n",
    "            detail=action.detail,\n",
    "            use_chain_of_thought=action.use_chain_of_thought,\n",
    "        )\n",
    "\n",
    "        resp = self.llm_service.call(sys_prompt, query_text, provider=action.provider)\n",
    "\n",
    "        result = {\n",
    "            \"response\": resp.content,\n",
    "            \"config\": {\n",
    "                \"template\": action.template_id,\n",
    "                \"tone\": action.tone.value,\n",
    "                \"detail\": action.detail.value,\n",
    "                \"cot\": action.use_chain_of_thought,\n",
    "                \"provider\": action.provider.value,\n",
    "            },\n",
    "            \"tokens\": resp.total_tokens,\n",
    "            \"latency\": resp.latency_seconds,\n",
    "            \"cost\": resp.cost_estimate,\n",
    "        }\n",
    "\n",
    "        # Optional: evaluate and update online\n",
    "        if evaluate and self.online_learning:\n",
    "            scores = self.judge.evaluate(\n",
    "                query=query_text,\n",
    "                system_prompt_used=sys_prompt,\n",
    "                response=resp.content,\n",
    "                requested_tone=action.tone.value,\n",
    "                requested_detail=action.detail.value,\n",
    "            )\n",
    "            reward = LLMJudge.composite_reward(\n",
    "                scores, token_cost=resp.cost_estimate, latency=resp.latency_seconds\n",
    "            )\n",
    "            self.agent.update(state, action, reward)\n",
    "            result[\"quality_scores\"] = {\n",
    "                \"relevance\": scores.relevance,\n",
    "                \"accuracy\": scores.accuracy,\n",
    "                \"helpfulness\": scores.helpfulness,\n",
    "                \"tone_match\": scores.tone_match,\n",
    "                \"detail_match\": scores.detail_match,\n",
    "            }\n",
    "            result[\"reward\"] = reward\n",
    "\n",
    "        self.serving_log.append(result)\n",
    "        return result\n",
    "\n",
    "    def record_human_feedback(self, query_text: str, query_type: QueryType,\n",
    "                              user_id: str, action: PromptAction, score: float):\n",
    "        \"\"\"Record explicit human feedback for RLHF-style updates.\"\"\"\n",
    "        state = self.agent.get_state(Query(query_text, query_type, 0.5), user_id)\n",
    "        self.agent.update(state, action, score)\n",
    "\n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        if not self.serving_log:\n",
    "            return {\"message\": \"No interactions yet\"}\n",
    "        df = pd.DataFrame(self.serving_log)\n",
    "        stats = {\n",
    "            \"total_served\": len(df),\n",
    "            \"total_cost_usd\": round(df[\"cost\"].sum(), 6),\n",
    "            \"avg_latency_s\": round(df[\"latency\"].mean(), 3),\n",
    "            \"avg_tokens\": int(df[\"tokens\"].mean()),\n",
    "        }\n",
    "        if \"reward\" in df.columns:\n",
    "            stats[\"avg_reward\"] = round(df[\"reward\"].mean(), 4)\n",
    "        return stats\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Quick demo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "optimizer = ProductionPromptOptimizer(\n",
    "    templates=PROMPT_TEMPLATES,\n",
    "    llm_service=llm_service,\n",
    "    judge=judge,\n",
    "    pretrained_agent=qlearn_agent,\n",
    "    online_learning=True,\n",
    ")\n",
    "\n",
    "result = optimizer.serve(\n",
    "    query_text=\"Explain the concept of eventual consistency in distributed systems.\",\n",
    "    query_type=QueryType.FACTUAL,\n",
    "    user_id=\"tech_expert\",\n",
    ")\n",
    "\n",
    "print(\"üöÄ Production Optimizer ‚Äî Single Serve\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Config:  {result['config']}\")\n",
    "print(f\"Tokens:  {result['tokens']}  |  Latency: {result['latency']}s  |  Cost: ${result['cost']:.6f}\")\n",
    "if \"reward\" in result:\n",
    "    print(f\"Reward:  {result['reward']:.4f}\")\n",
    "    print(f\"Quality: {result['quality_scores']}\")\n",
    "print(f\"\\nResponse:\\n{result['response'][:600]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Cost Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí∞ Final Cost Report\")\n",
    "print(\"=\" * 60)\n",
    "final_cost = llm_service.get_cost_summary()\n",
    "for k, v in final_cost.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "print(f\"\\n   Optimizer serving stats: {optimizer.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Summary\n",
    "\n",
    "This notebook demonstrated **RL-based prompt optimization with real LLM calls**:\n",
    "\n",
    "| Component | Implementation |\n",
    "|-----------|---------------|\n",
    "| **LLM Providers** | OpenAI (`gpt-4o-mini`) + Anthropic (`claude-3.5-haiku`) via LangChain |\n",
    "| **Reward Signal** | LLM-as-Judge (5 quality dimensions) + real token cost + latency |\n",
    "| **RL Algorithms** | Thompson Sampling Bandit, Q-Learning |\n",
    "| **Workflow Engine** | LangGraph state machine (select ‚Üí call ‚Üí judge ‚Üí reward ‚Üí loop) |\n",
    "| **Action Space** | Template √ó Tone √ó Detail √ó CoT √ó Provider |\n",
    "| **Online Learning** | Continuous improvement from production feedback |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LLM-as-Judge** provides reliable automated quality signals without human labeling\n",
    "2. **Thompson Sampling** is excellent for quickly learning which template works best per query type\n",
    "3. **Q-Learning** discovers richer configurations (template + tone + detail + provider combos)\n",
    "4. **Cost-aware rewards** naturally push the agent toward efficient configurations\n",
    "5. **Online learning** means the system keeps improving from every production interaction\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "- Start with **Thompson Sampling** (simpler, faster convergence)\n",
    "- Graduate to **Q-Learning** when you need multi-dimensional optimization\n",
    "- Add **explicit human feedback** alongside LLM-judge for RLHF-style training\n",
    "- Use **A/B testing** to validate RL improvements against static baselines\n",
    "- Monitor **cost per quality unit** to ensure efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéâ RL Prompt Optimization with Real LLM Calls ‚Äî Complete!\")\n",
    "print(\"\\nThis notebook demonstrated:\")\n",
    "print(\"  ‚Ä¢ Real LLM calls via LangChain (OpenAI + Anthropic)\")\n",
    "print(\"  ‚Ä¢ LLM-as-Judge reward model with 5 quality dimensions\")\n",
    "print(\"  ‚Ä¢ Thompson Sampling bandit for template selection\")\n",
    "print(\"  ‚Ä¢ Q-Learning for full prompt configuration optimization\")\n",
    "print(\"  ‚Ä¢ LangGraph state machine for the RL training loop\")\n",
    "print(\"  ‚Ä¢ Real token cost tracking and cost-aware rewards\")\n",
    "print(\"  ‚Ä¢ Production-ready optimizer with online learning\")\n",
    "print(f\"\\nüí∞ Total session cost: ${llm_service.total_cost:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
