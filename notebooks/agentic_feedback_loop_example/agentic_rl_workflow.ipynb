{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/agentic_architectures_and_design_patterns/blob/main/notebooks/reinforcement_learning/agentic_rl_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmFsYwchx5D"
      },
      "source": [
        "# Agentic Workflow with Reinforcement Learning Loop\n",
        "\n",
        "This notebook demonstrates a sophisticated multi-agent system where agents learn optimal behaviors through reinforcement learning. The architecture combines:\n",
        "- **Multi-Agent Coordination**: Agents with specialized roles\n",
        "- **Reinforcement Learning**: PPO-based policy optimization\n",
        "- **Tool Integration**: Agents can use external tools/APIs\n",
        "- **Adaptive Workflow**: Learning-based task allocation\n",
        "- **Production Patterns**: Monitoring, logging, and error handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dURwmbqYhx5E"
      },
      "source": [
        "## 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTOrgTtzhx5F"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install stable-baselines3 gymnasium numpy pandas matplotlib seaborn\n",
        "!pip install langchain langchain-openai langgraph tensorboard\n",
        "!pip install ray[rllib] wandb mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c0VCm1Ohx5F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional, Any, Union\n",
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1jY5nczhx5G"
      },
      "source": [
        "## 2. Multi-Agent Environment Definition\n",
        "\n",
        "Define a custom environment where multiple agents collaborate on tasks, learning optimal policies through RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b6EzOWwhx5G"
      },
      "outputs": [],
      "source": [
        "class AgentRole(Enum):\n",
        "    \"\"\"Defines specialized agent roles in the system\"\"\"\n",
        "    RESEARCHER = \"researcher\"\n",
        "    ANALYZER = \"analyzer\"\n",
        "    EXECUTOR = \"executor\"\n",
        "    VALIDATOR = \"validator\"\n",
        "    COORDINATOR = \"coordinator\"\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    \"\"\"Represents a task in the workflow\"\"\"\n",
        "    id: str\n",
        "    type: str\n",
        "    complexity: float\n",
        "    requirements: List[str]\n",
        "    deadline: float\n",
        "    priority: float\n",
        "    status: str = \"pending\"\n",
        "    assigned_agent: Optional[str] = None\n",
        "    completion_time: Optional[float] = None\n",
        "    quality_score: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class AgentState:\n",
        "    \"\"\"Tracks individual agent state\"\"\"\n",
        "    id: str\n",
        "    role: AgentRole\n",
        "    capacity: float\n",
        "    expertise: Dict[str, float]\n",
        "    current_load: float = 0.0\n",
        "    completed_tasks: int = 0\n",
        "    success_rate: float = 1.0\n",
        "    collaboration_score: float = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8iAuEXzhx5G"
      },
      "outputs": [],
      "source": [
        "class MultiAgentTaskEnvironment(gym.Env):\n",
        "    \"\"\"Multi-agent environment for collaborative task execution with RL\"\"\"\n",
        "\n",
        "    def __init__(self, n_agents: int = 4, max_tasks: int = 10):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.max_tasks = max_tasks\n",
        "        self.current_step = 0\n",
        "        self.max_steps = 200\n",
        "\n",
        "        # Initialize agents with different roles\n",
        "        self.agents = self._initialize_agents()\n",
        "\n",
        "        # Task queue and completed tasks\n",
        "        self.task_queue = deque()\n",
        "        self.active_tasks = {}\n",
        "        self.completed_tasks = []\n",
        "\n",
        "        # Define observation and action spaces\n",
        "        # Observation: [agent_states, task_queue_state, collaboration_matrix]\n",
        "        obs_dim = n_agents * 7 + max_tasks * 5 + n_agents * n_agents\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Action: [task_assignment, resource_allocation, collaboration_request]\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0, high=1, shape=(n_agents * 3,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.episode_rewards = []\n",
        "        self.task_completion_times = []\n",
        "        self.collaboration_matrix = np.ones((n_agents, n_agents))\n",
        "\n",
        "    def _initialize_agents(self) -> List[AgentState]:\n",
        "        \"\"\"Initialize agents with diverse roles and capabilities\"\"\"\n",
        "        agents = []\n",
        "        roles = list(AgentRole)[:self.n_agents]\n",
        "\n",
        "        for i, role in enumerate(roles):\n",
        "            expertise = {\n",
        "                \"research\": np.random.uniform(0.5, 1.0),\n",
        "                \"analysis\": np.random.uniform(0.5, 1.0),\n",
        "                \"execution\": np.random.uniform(0.5, 1.0),\n",
        "                \"validation\": np.random.uniform(0.5, 1.0)\n",
        "            }\n",
        "\n",
        "            # Boost expertise based on role\n",
        "            if role == AgentRole.RESEARCHER:\n",
        "                expertise[\"research\"] = min(1.0, expertise[\"research\"] + 0.3)\n",
        "            elif role == AgentRole.ANALYZER:\n",
        "                expertise[\"analysis\"] = min(1.0, expertise[\"analysis\"] + 0.3)\n",
        "\n",
        "            agents.append(AgentState(\n",
        "                id=f\"agent_{i}\",\n",
        "                role=role,\n",
        "                capacity=np.random.uniform(0.8, 1.0),\n",
        "                expertise=expertise\n",
        "            ))\n",
        "\n",
        "        return agents\n",
        "\n",
        "    def _generate_task(self) -> Task:\n",
        "        \"\"\"Generate a new task with random properties\"\"\"\n",
        "        task_types = [\"research\", \"analysis\", \"execution\", \"validation\"]\n",
        "        task_type = np.random.choice(task_types)\n",
        "\n",
        "        return Task(\n",
        "            id=f\"task_{np.random.randint(10000)}\",\n",
        "            type=task_type,\n",
        "            complexity=np.random.uniform(0.3, 1.0),\n",
        "            requirements=[np.random.choice(task_types) for _ in range(np.random.randint(1, 3))],\n",
        "            deadline=np.random.uniform(10, 50),\n",
        "            priority=np.random.uniform(0.1, 1.0)\n",
        "        )\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        \"\"\"Construct observation vector from current state\"\"\"\n",
        "        obs = []\n",
        "\n",
        "        # Agent states\n",
        "        for agent in self.agents:\n",
        "            obs.extend([\n",
        "                agent.capacity,\n",
        "                agent.current_load,\n",
        "                agent.completed_tasks / max(1, self.current_step),\n",
        "                agent.success_rate,\n",
        "                agent.collaboration_score,\n",
        "                agent.expertise.get(\"research\", 0),\n",
        "                agent.expertise.get(\"analysis\", 0)\n",
        "            ])\n",
        "\n",
        "        # Task queue state\n",
        "        for i in range(self.max_tasks):\n",
        "            if i < len(self.task_queue):\n",
        "                task = list(self.task_queue)[i]\n",
        "                obs.extend([\n",
        "                    task.complexity,\n",
        "                    task.priority,\n",
        "                    task.deadline,\n",
        "                    1.0,  # task exists\n",
        "                    0.0   # not yet assigned\n",
        "                ])\n",
        "            else:\n",
        "                obs.extend([0, 0, 0, 0, 0])\n",
        "\n",
        "        # Collaboration matrix (flattened)\n",
        "        obs.extend(self.collaboration_matrix.flatten())\n",
        "\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
        "        \"\"\"Execute action and return new state\"\"\"\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Parse actions\n",
        "        action = action.reshape(self.n_agents, 3)\n",
        "\n",
        "        # Process task assignments\n",
        "        reward = 0\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            if len(self.task_queue) > 0 and action[i, 0] > 0.5:\n",
        "                task = self.task_queue.popleft()\n",
        "                self._assign_task(agent, task)\n",
        "\n",
        "                # Calculate immediate reward for assignment\n",
        "                match_score = agent.expertise.get(task.type, 0.5)\n",
        "                reward += match_score * task.priority\n",
        "\n",
        "        # Process active tasks\n",
        "        completed_this_step = []\n",
        "        for task_id, (task, agent) in list(self.active_tasks.items()):\n",
        "            progress = agent.expertise.get(task.type, 0.5) * agent.capacity\n",
        "\n",
        "            # Check if task completed\n",
        "            if np.random.random() < progress:\n",
        "                self._complete_task(task, agent)\n",
        "                completed_this_step.append(task)\n",
        "\n",
        "                # Calculate completion reward\n",
        "                time_bonus = max(0, 1 - (self.current_step / task.deadline))\n",
        "                quality = agent.expertise.get(task.type, 0.5) * agent.success_rate\n",
        "                reward += (quality * task.priority * (1 + time_bonus)) * 10\n",
        "\n",
        "        # Update collaboration matrix based on actions\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(self.n_agents):\n",
        "                if i != j and action[i, 2] > 0.7 and action[j, 2] > 0.7:\n",
        "                    self.collaboration_matrix[i, j] *= 1.01\n",
        "                    reward += 0.5  # Collaboration bonus\n",
        "\n",
        "        # Generate new tasks\n",
        "        if np.random.random() < 0.3:\n",
        "            self.task_queue.append(self._generate_task())\n",
        "\n",
        "        # Calculate penalties\n",
        "        queue_penalty = len(self.task_queue) * 0.1\n",
        "        overdue_penalty = sum(1 for t in self.active_tasks.values()\n",
        "                            if self.current_step > t[0].deadline) * 0.5\n",
        "        reward -= (queue_penalty + overdue_penalty)\n",
        "\n",
        "        # Check termination\n",
        "        done = self.current_step >= self.max_steps\n",
        "        truncated = False\n",
        "\n",
        "        # Prepare info\n",
        "        info = {\n",
        "            \"completed_tasks\": len(completed_this_step),\n",
        "            \"queue_length\": len(self.task_queue),\n",
        "            \"active_tasks\": len(self.active_tasks),\n",
        "            \"avg_success_rate\": np.mean([a.success_rate for a in self.agents])\n",
        "        }\n",
        "\n",
        "        return self._get_observation(), reward, done, truncated, info\n",
        "\n",
        "    def _assign_task(self, agent: AgentState, task: Task):\n",
        "        \"\"\"Assign a task to an agent\"\"\"\n",
        "        task.assigned_agent = agent.id\n",
        "        task.status = \"active\"\n",
        "        self.active_tasks[task.id] = (task, agent)\n",
        "        agent.current_load += task.complexity\n",
        "\n",
        "    def _complete_task(self, task: Task, agent: AgentState):\n",
        "        \"\"\"Mark a task as completed\"\"\"\n",
        "        task.status = \"completed\"\n",
        "        task.completion_time = self.current_step\n",
        "        task.quality_score = agent.expertise.get(task.type, 0.5) * agent.success_rate\n",
        "\n",
        "        self.completed_tasks.append(task)\n",
        "        del self.active_tasks[task.id]\n",
        "\n",
        "        agent.current_load = max(0, agent.current_load - task.complexity)\n",
        "        agent.completed_tasks += 1\n",
        "        agent.success_rate = 0.95 * agent.success_rate + 0.05 * task.quality_score\n",
        "\n",
        "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.agents = self._initialize_agents()\n",
        "        self.task_queue = deque([self._generate_task() for _ in range(3)])\n",
        "        self.active_tasks = {}\n",
        "        self.completed_tasks = []\n",
        "        self.collaboration_matrix = np.ones((self.n_agents, self.n_agents))\n",
        "\n",
        "        return self._get_observation(), {}\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Render current environment state\"\"\"\n",
        "        print(f\"\\nStep: {self.current_step}\")\n",
        "        print(f\"Tasks in queue: {len(self.task_queue)}\")\n",
        "        print(f\"Active tasks: {len(self.active_tasks)}\")\n",
        "        print(f\"Completed tasks: {len(self.completed_tasks)}\")\n",
        "        for agent in self.agents:\n",
        "            print(f\"  {agent.id} ({agent.role.value}): Load={agent.current_load:.2f}, \"\n",
        "                  f\"Completed={agent.completed_tasks}, Success={agent.success_rate:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPN4ahiahx5H"
      },
      "source": [
        "## 3. Agentic Learning System with PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o6Zx9xZhx5H"
      },
      "outputs": [],
      "source": [
        "class AgenticRLSystem:\n",
        "    \"\"\"Orchestrates multi-agent reinforcement learning workflow\"\"\"\n",
        "\n",
        "    def __init__(self, env_class, n_envs: int = 4):\n",
        "        self.env_class = env_class\n",
        "        self.n_envs = n_envs\n",
        "\n",
        "        # Create vectorized environment for parallel training\n",
        "        self.env = DummyVecEnv([lambda: env_class() for _ in range(n_envs)])\n",
        "        self.eval_env = DummyVecEnv([lambda: env_class()])\n",
        "\n",
        "        # Initialize PPO model with custom network architecture\n",
        "        self.model = self._create_model()\n",
        "\n",
        "        # Metrics tracking\n",
        "        self.training_history = {\n",
        "            \"episode_rewards\": [],\n",
        "            \"episode_lengths\": [],\n",
        "            \"success_rates\": [],\n",
        "            \"collaboration_scores\": []\n",
        "        }\n",
        "\n",
        "        # Setup callbacks\n",
        "        self.callbacks = self._setup_callbacks()\n",
        "\n",
        "    def _create_model(self) -> PPO:\n",
        "        \"\"\"Create PPO model with custom architecture\"\"\"\n",
        "        policy_kwargs = dict(\n",
        "            net_arch=[\n",
        "                dict(pi=[256, 256, 128], vf=[256, 256, 128])\n",
        "            ],\n",
        "            activation_fn=nn.ReLU\n",
        "        )\n",
        "\n",
        "        model = PPO(\n",
        "            \"MlpPolicy\",\n",
        "            self.env,\n",
        "            learning_rate=3e-4,\n",
        "            n_steps=2048,\n",
        "            batch_size=64,\n",
        "            n_epochs=10,\n",
        "            gamma=0.99,\n",
        "            gae_lambda=0.95,\n",
        "            clip_range=0.2,\n",
        "            clip_range_vf=None,\n",
        "            ent_coef=0.01,\n",
        "            vf_coef=0.5,\n",
        "            max_grad_norm=0.5,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=1,\n",
        "            tensorboard_log=\"./tensorboard_logs/\"\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _setup_callbacks(self):\n",
        "        \"\"\"Setup training callbacks for monitoring and checkpointing\"\"\"\n",
        "        eval_callback = EvalCallback(\n",
        "            self.eval_env,\n",
        "            best_model_save_path=\"./models/best_model/\",\n",
        "            log_path=\"./logs/\",\n",
        "            eval_freq=5000,\n",
        "            deterministic=True,\n",
        "            render=False,\n",
        "            n_eval_episodes=10\n",
        "        )\n",
        "\n",
        "        checkpoint_callback = CheckpointCallback(\n",
        "            save_freq=10000,\n",
        "            save_path=\"./models/checkpoints/\",\n",
        "            name_prefix=\"agentic_rl_model\"\n",
        "        )\n",
        "\n",
        "        return [eval_callback, checkpoint_callback]\n",
        "\n",
        "    def train(self, total_timesteps: int = 100000):\n",
        "        \"\"\"Train the multi-agent system\"\"\"\n",
        "        logger.info(f\"Starting training for {total_timesteps} timesteps\")\n",
        "\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=self.callbacks,\n",
        "            log_interval=10,\n",
        "            progress_bar=True\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training completed\")\n",
        "        return self.model\n",
        "\n",
        "    def evaluate(self, n_episodes: int = 10) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate the trained model\"\"\"\n",
        "        env = self.env_class()\n",
        "\n",
        "        episode_rewards = []\n",
        "        episode_lengths = []\n",
        "        task_completion_rates = []\n",
        "\n",
        "        for episode in range(n_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            done = False\n",
        "            episode_reward = 0\n",
        "            episode_length = 0\n",
        "\n",
        "            while not done:\n",
        "                action, _ = self.model.predict(obs, deterministic=True)\n",
        "                obs, reward, done, truncated, info = env.step(action)\n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_lengths.append(episode_length)\n",
        "\n",
        "            if len(env.completed_tasks) > 0:\n",
        "                completion_rate = len(env.completed_tasks) / (len(env.completed_tasks) + len(env.task_queue))\n",
        "                task_completion_rates.append(completion_rate)\n",
        "\n",
        "        metrics = {\n",
        "            \"mean_reward\": np.mean(episode_rewards),\n",
        "            \"std_reward\": np.std(episode_rewards),\n",
        "            \"mean_episode_length\": np.mean(episode_lengths),\n",
        "            \"task_completion_rate\": np.mean(task_completion_rates) if task_completion_rates else 0\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def save_model(self, path: str):\n",
        "        \"\"\"Save the trained model\"\"\"\n",
        "        self.model.save(path)\n",
        "        logger.info(f\"Model saved to {path}\")\n",
        "\n",
        "    def load_model(self, path: str):\n",
        "        \"\"\"Load a pre-trained model\"\"\"\n",
        "        self.model = PPO.load(path, env=self.env)\n",
        "        logger.info(f\"Model loaded from {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ohljvebhx5I"
      },
      "source": [
        "## 4. Advanced Agent Behaviors and Tool Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUpQn_kqhx5I"
      },
      "outputs": [],
      "source": [
        "class ToolRegistry:\n",
        "    \"\"\"Registry for agent tools and capabilities\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tools = {}\n",
        "        self._register_default_tools()\n",
        "\n",
        "    def _register_default_tools(self):\n",
        "        \"\"\"Register default tools available to agents\"\"\"\n",
        "        self.register_tool(\n",
        "            \"web_search\",\n",
        "            self._mock_web_search,\n",
        "            description=\"Search for information online\",\n",
        "            cost=0.1\n",
        "        )\n",
        "        self.register_tool(\n",
        "            \"data_analysis\",\n",
        "            self._mock_data_analysis,\n",
        "            description=\"Analyze structured data\",\n",
        "            cost=0.2\n",
        "        )\n",
        "        self.register_tool(\n",
        "            \"code_generation\",\n",
        "            self._mock_code_generation,\n",
        "            description=\"Generate code snippets\",\n",
        "            cost=0.3\n",
        "        )\n",
        "\n",
        "    def register_tool(self, name: str, func, description: str, cost: float):\n",
        "        \"\"\"Register a new tool\"\"\"\n",
        "        self.tools[name] = {\n",
        "            \"function\": func,\n",
        "            \"description\": description,\n",
        "            \"cost\": cost,\n",
        "            \"usage_count\": 0\n",
        "        }\n",
        "\n",
        "    async def use_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Use a registered tool\"\"\"\n",
        "        if tool_name not in self.tools:\n",
        "            return {\"error\": f\"Tool {tool_name} not found\"}\n",
        "\n",
        "        tool = self.tools[tool_name]\n",
        "        tool[\"usage_count\"] += 1\n",
        "\n",
        "        try:\n",
        "            result = await tool[\"function\"](**kwargs)\n",
        "            return {\n",
        "                \"success\": True,\n",
        "                \"result\": result,\n",
        "                \"cost\": tool[\"cost\"]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"success\": False,\n",
        "                \"error\": str(e),\n",
        "                \"cost\": tool[\"cost\"]\n",
        "            }\n",
        "\n",
        "    async def _mock_web_search(self, query: str) -> str:\n",
        "        \"\"\"Mock web search tool\"\"\"\n",
        "        await asyncio.sleep(0.1)  # Simulate API call\n",
        "        return f\"Search results for: {query}\"\n",
        "\n",
        "    async def _mock_data_analysis(self, data: Any) -> Dict:\n",
        "        \"\"\"Mock data analysis tool\"\"\"\n",
        "        await asyncio.sleep(0.2)\n",
        "        return {\"insights\": \"Data patterns detected\", \"confidence\": 0.85}\n",
        "\n",
        "    async def _mock_code_generation(self, prompt: str) -> str:\n",
        "        \"\"\"Mock code generation tool\"\"\"\n",
        "        await asyncio.sleep(0.3)\n",
        "        return f\"def generated_function():\\n    # Code for: {prompt}\\n    pass\"\n",
        "\n",
        "class SmartAgent:\n",
        "    \"\"\"Individual agent with learning capabilities and tool usage\"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: str, role: AgentRole, tool_registry: ToolRegistry):\n",
        "        self.id = agent_id\n",
        "        self.role = role\n",
        "        self.tool_registry = tool_registry\n",
        "\n",
        "        # Learning parameters\n",
        "        self.q_table = {}  # Simple Q-learning for tool selection\n",
        "        self.epsilon = 0.1  # Exploration rate\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.95\n",
        "\n",
        "        # Performance tracking\n",
        "        self.task_history = []\n",
        "        self.tool_effectiveness = {tool: 1.0 for tool in tool_registry.tools}\n",
        "\n",
        "    async def process_task(self, task: Task) -> Dict[str, Any]:\n",
        "        \"\"\"Process a task using available tools and learned strategies\"\"\"\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Select tool based on Q-learning\n",
        "        selected_tool = self._select_tool(task)\n",
        "\n",
        "        # Use the selected tool\n",
        "        tool_result = await self.tool_registry.use_tool(\n",
        "            selected_tool,\n",
        "            query=f\"Process task: {task.type}\"\n",
        "        )\n",
        "\n",
        "        # Calculate reward\n",
        "        processing_time = (datetime.now() - start_time).total_seconds()\n",
        "        reward = self._calculate_reward(tool_result, processing_time, task)\n",
        "\n",
        "        # Update Q-table\n",
        "        self._update_q_table(task.type, selected_tool, reward)\n",
        "\n",
        "        # Record task completion\n",
        "        self.task_history.append({\n",
        "            \"task_id\": task.id,\n",
        "            \"tool_used\": selected_tool,\n",
        "            \"reward\": reward,\n",
        "            \"processing_time\": processing_time\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            \"agent_id\": self.id,\n",
        "            \"task_id\": task.id,\n",
        "            \"tool_used\": selected_tool,\n",
        "            \"result\": tool_result,\n",
        "            \"processing_time\": processing_time,\n",
        "            \"reward\": reward\n",
        "        }\n",
        "\n",
        "    def _select_tool(self, task: Task) -> str:\n",
        "        \"\"\"Select tool using epsilon-greedy strategy\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: random tool selection\n",
        "            return np.random.choice(list(self.tool_registry.tools.keys()))\n",
        "        else:\n",
        "            # Exploit: select best tool based on Q-values\n",
        "            state = task.type\n",
        "            if state not in self.q_table:\n",
        "                self.q_table[state] = {tool: 0.0 for tool in self.tool_registry.tools}\n",
        "\n",
        "            return max(self.q_table[state], key=self.q_table[state].get)\n",
        "\n",
        "    def _calculate_reward(self, tool_result: Dict, processing_time: float, task: Task) -> float:\n",
        "        \"\"\"Calculate reward based on tool effectiveness and efficiency\"\"\"\n",
        "        base_reward = 1.0 if tool_result.get(\"success\", False) else -0.5\n",
        "        time_penalty = -processing_time * 0.1\n",
        "        cost_penalty = -tool_result.get(\"cost\", 0) * 0.5\n",
        "        priority_bonus = task.priority * 0.5\n",
        "\n",
        "        return base_reward + time_penalty + cost_penalty + priority_bonus\n",
        "\n",
        "    def _update_q_table(self, state: str, action: str, reward: float):\n",
        "        \"\"\"Update Q-table using Q-learning update rule\"\"\"\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {tool: 0.0 for tool in self.tool_registry.tools}\n",
        "\n",
        "        old_value = self.q_table[state][action]\n",
        "        # Simplified update without next state (terminal state assumed)\n",
        "        self.q_table[state][action] = old_value + self.learning_rate * (reward - old_value)\n",
        "\n",
        "        # Update tool effectiveness tracking\n",
        "        self.tool_effectiveness[action] = (\n",
        "            0.95 * self.tool_effectiveness[action] + 0.05 * (reward > 0)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Yn1qkwhx5I"
      },
      "source": [
        "## 5. Training and Evaluation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c67JLiUyhx5J"
      },
      "outputs": [],
      "source": [
        "def train_agentic_rl_system():\n",
        "    \"\"\"Main training pipeline for the agentic RL system\"\"\"\n",
        "\n",
        "    # Initialize environment and system\n",
        "    print(\"Initializing Agentic RL System...\")\n",
        "    system = AgenticRLSystem(MultiAgentTaskEnvironment, n_envs=4)\n",
        "\n",
        "    # Training configuration\n",
        "    training_config = {\n",
        "        \"total_timesteps\": 50000,\n",
        "        \"eval_frequency\": 5000,\n",
        "        \"n_eval_episodes\": 10\n",
        "    }\n",
        "\n",
        "    print(f\"\\nStarting training with {training_config['total_timesteps']} timesteps...\")\n",
        "\n",
        "    # Train the system\n",
        "    trained_model = system.train(total_timesteps=training_config['total_timesteps'])\n",
        "\n",
        "    # Evaluate performance\n",
        "    print(\"\\nEvaluating trained model...\")\n",
        "    metrics = system.evaluate(n_episodes=training_config['n_eval_episodes'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training Complete - Performance Metrics:\")\n",
        "    print(\"=\"*50)\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"{key}: {value:.3f}\")\n",
        "\n",
        "    # Save the model\n",
        "    system.save_model(\"./models/trained_agentic_rl_model\")\n",
        "\n",
        "    return system, metrics\n",
        "\n",
        "# Train the system\n",
        "# Uncomment to run training (will take some time)\n",
        "# system, metrics = train_agentic_rl_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pi-RcLrhx5J"
      },
      "source": [
        "## 6. Advanced Monitoring and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdh736CZhx5J"
      },
      "outputs": [],
      "source": [
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitor and visualize system performance\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.metrics_history = {\n",
        "            \"timestamps\": [],\n",
        "            \"rewards\": [],\n",
        "            \"task_completions\": [],\n",
        "            \"agent_utilization\": [],\n",
        "            \"collaboration_scores\": [],\n",
        "            \"tool_usage\": {}\n",
        "        }\n",
        "\n",
        "    def record_metrics(self, timestamp: float, env_info: Dict, agent_metrics: Dict):\n",
        "        \"\"\"Record performance metrics\"\"\"\n",
        "        self.metrics_history[\"timestamps\"].append(timestamp)\n",
        "        self.metrics_history[\"rewards\"].append(env_info.get(\"reward\", 0))\n",
        "        self.metrics_history[\"task_completions\"].append(env_info.get(\"completed_tasks\", 0))\n",
        "        self.metrics_history[\"agent_utilization\"].append(agent_metrics.get(\"avg_utilization\", 0))\n",
        "        self.metrics_history[\"collaboration_scores\"].append(agent_metrics.get(\"collaboration_score\", 0))\n",
        "\n",
        "    def plot_performance(self):\n",
        "        \"\"\"Create performance visualization dashboard\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle(\"Agentic RL System Performance Dashboard\", fontsize=16)\n",
        "\n",
        "        # Plot 1: Rewards over time\n",
        "        axes[0, 0].plot(self.metrics_history[\"timestamps\"],\n",
        "                       self.metrics_history[\"rewards\"],\n",
        "                       'b-', label='Episode Rewards')\n",
        "        axes[0, 0].set_xlabel('Time Steps')\n",
        "        axes[0, 0].set_ylabel('Reward')\n",
        "        axes[0, 0].set_title('Learning Progress')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        # Plot 2: Task completion rate\n",
        "        axes[0, 1].plot(self.metrics_history[\"timestamps\"],\n",
        "                       self.metrics_history[\"task_completions\"],\n",
        "                       'g-', label='Tasks Completed')\n",
        "        axes[0, 1].set_xlabel('Time Steps')\n",
        "        axes[0, 1].set_ylabel('Tasks')\n",
        "        axes[0, 1].set_title('Task Completion Rate')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].legend()\n",
        "\n",
        "        # Plot 3: Agent utilization\n",
        "        axes[1, 0].plot(self.metrics_history[\"timestamps\"],\n",
        "                       self.metrics_history[\"agent_utilization\"],\n",
        "                       'r-', label='Avg Utilization')\n",
        "        axes[1, 0].set_xlabel('Time Steps')\n",
        "        axes[1, 0].set_ylabel('Utilization %')\n",
        "        axes[1, 0].set_title('Agent Resource Utilization')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].legend()\n",
        "\n",
        "        # Plot 4: Collaboration scores\n",
        "        axes[1, 1].plot(self.metrics_history[\"timestamps\"],\n",
        "                       self.metrics_history[\"collaboration_scores\"],\n",
        "                       'm-', label='Collaboration Score')\n",
        "        axes[1, 1].set_xlabel('Time Steps')\n",
        "        axes[1, 1].set_ylabel('Score')\n",
        "        axes[1, 1].set_title('Multi-Agent Collaboration')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_report(self) -> pd.DataFrame:\n",
        "        \"\"\"Generate performance report as DataFrame\"\"\"\n",
        "        report_data = {\n",
        "            'Metric': ['Total Rewards', 'Avg Task Completion', 'Peak Utilization',\n",
        "                      'Avg Collaboration Score', 'Total Time Steps'],\n",
        "            'Value': [\n",
        "                np.sum(self.metrics_history[\"rewards\"]),\n",
        "                np.mean(self.metrics_history[\"task_completions\"]),\n",
        "                np.max(self.metrics_history[\"agent_utilization\"]),\n",
        "                np.mean(self.metrics_history[\"collaboration_scores\"]),\n",
        "                len(self.metrics_history[\"timestamps\"])\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return pd.DataFrame(report_data)\n",
        "\n",
        "# Example usage\n",
        "monitor = PerformanceMonitor()\n",
        "\n",
        "# Simulate some metrics recording\n",
        "for i in range(100):\n",
        "    monitor.record_metrics(\n",
        "        timestamp=i,\n",
        "        env_info={\"reward\": np.random.randn() * 10 + i * 0.1, \"completed_tasks\": np.random.randint(0, 5)},\n",
        "        agent_metrics={\"avg_utilization\": np.random.uniform(0.3, 0.9), \"collaboration_score\": np.random.uniform(0.5, 1.0)}\n",
        "    )\n",
        "\n",
        "# Visualize performance\n",
        "monitor.plot_performance()\n",
        "\n",
        "# Generate report\n",
        "report = monitor.generate_report()\n",
        "print(\"\\nPerformance Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUZtvEVkhx5J"
      },
      "source": [
        "## 7. Production Deployment Considerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcYPCMf6hx5J"
      },
      "outputs": [],
      "source": [
        "class ProductionAgenticSystem:\n",
        "    \"\"\"Production-ready agentic RL system with monitoring and error handling\"\"\"\n",
        "\n",
        "    def __init__(self, model_path: str, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # Load trained model\n",
        "        self.model = None  # Would load actual model here\n",
        "\n",
        "        # Initialize components\n",
        "        self.tool_registry = ToolRegistry()\n",
        "        self.monitor = PerformanceMonitor()\n",
        "\n",
        "        # Error handling and circuit breaker\n",
        "        self.error_count = 0\n",
        "        self.max_errors = config.get(\"max_errors\", 10)\n",
        "        self.circuit_breaker_open = False\n",
        "\n",
        "        # Rate limiting\n",
        "        self.request_times = deque(maxlen=100)\n",
        "        self.max_requests_per_minute = config.get(\"max_rpm\", 60)\n",
        "\n",
        "    async def process_request(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Process incoming request with production safeguards\"\"\"\n",
        "\n",
        "        # Check circuit breaker\n",
        "        if self.circuit_breaker_open:\n",
        "            return {\"error\": \"Circuit breaker open - system temporarily unavailable\"}\n",
        "\n",
        "        # Rate limiting\n",
        "        if not self._check_rate_limit():\n",
        "            return {\"error\": \"Rate limit exceeded\"}\n",
        "\n",
        "        try:\n",
        "            # Validate request\n",
        "            validation_result = self._validate_request(request)\n",
        "            if not validation_result[\"valid\"]:\n",
        "                return {\"error\": validation_result[\"message\"]}\n",
        "\n",
        "            # Process with timeout\n",
        "            result = await asyncio.wait_for(\n",
        "                self._process_with_agents(request),\n",
        "                timeout=self.config.get(\"timeout\", 30)\n",
        "            )\n",
        "\n",
        "            # Reset error count on success\n",
        "            self.error_count = 0\n",
        "\n",
        "            # Record metrics\n",
        "            self._record_metrics(result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        except asyncio.TimeoutError:\n",
        "            self._handle_error(\"Timeout\")\n",
        "            return {\"error\": \"Request timeout\"}\n",
        "\n",
        "        except Exception as e:\n",
        "            self._handle_error(str(e))\n",
        "            return {\"error\": f\"Processing error: {str(e)}\"}\n",
        "\n",
        "    def _check_rate_limit(self) -> bool:\n",
        "        \"\"\"Check if request is within rate limits\"\"\"\n",
        "        current_time = datetime.now()\n",
        "        self.request_times.append(current_time)\n",
        "\n",
        "        # Count requests in last minute\n",
        "        minute_ago = current_time.timestamp() - 60\n",
        "        recent_requests = sum(1 for t in self.request_times\n",
        "                            if t.timestamp() > minute_ago)\n",
        "\n",
        "        return recent_requests < self.max_requests_per_minute\n",
        "\n",
        "    def _validate_request(self, request: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Validate incoming request\"\"\"\n",
        "        required_fields = [\"task_type\", \"priority\"]\n",
        "\n",
        "        for field in required_fields:\n",
        "            if field not in request:\n",
        "                return {\"valid\": False, \"message\": f\"Missing required field: {field}\"}\n",
        "\n",
        "        if request[\"priority\"] < 0 or request[\"priority\"] > 1:\n",
        "            return {\"valid\": False, \"message\": \"Priority must be between 0 and 1\"}\n",
        "\n",
        "        return {\"valid\": True}\n",
        "\n",
        "    async def _process_with_agents(self, request: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"Process request using trained agents\"\"\"\n",
        "        # This would use the actual trained model and agents\n",
        "        await asyncio.sleep(0.1)  # Simulate processing\n",
        "\n",
        "        return {\n",
        "            \"status\": \"completed\",\n",
        "            \"request_id\": request.get(\"id\", \"unknown\"),\n",
        "            \"result\": \"Processed successfully\",\n",
        "            \"processing_time\": 0.1,\n",
        "            \"agents_used\": [\"agent_0\", \"agent_1\"]\n",
        "        }\n",
        "\n",
        "    def _handle_error(self, error_message: str):\n",
        "        \"\"\"Handle errors with circuit breaker pattern\"\"\"\n",
        "        logger.error(f\"Error occurred: {error_message}\")\n",
        "        self.error_count += 1\n",
        "\n",
        "        if self.error_count >= self.max_errors:\n",
        "            self.circuit_breaker_open = True\n",
        "            logger.critical(\"Circuit breaker opened due to excessive errors\")\n",
        "\n",
        "            # Schedule circuit breaker reset\n",
        "            asyncio.create_task(self._reset_circuit_breaker())\n",
        "\n",
        "    async def _reset_circuit_breaker(self):\n",
        "        \"\"\"Reset circuit breaker after cooldown period\"\"\"\n",
        "        await asyncio.sleep(self.config.get(\"circuit_breaker_cooldown\", 60))\n",
        "        self.circuit_breaker_open = False\n",
        "        self.error_count = 0\n",
        "        logger.info(\"Circuit breaker reset\")\n",
        "\n",
        "    def _record_metrics(self, result: Dict):\n",
        "        \"\"\"Record processing metrics\"\"\"\n",
        "        # Would send to monitoring system (Prometheus, DataDog, etc.)\n",
        "        pass\n",
        "\n",
        "    def get_health_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get system health status for monitoring\"\"\"\n",
        "        return {\n",
        "            \"status\": \"unhealthy\" if self.circuit_breaker_open else \"healthy\",\n",
        "            \"error_count\": self.error_count,\n",
        "            \"circuit_breaker\": \"open\" if self.circuit_breaker_open else \"closed\",\n",
        "            \"recent_requests\": len(self.request_times),\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "# Example production configuration\n",
        "production_config = {\n",
        "    \"max_errors\": 10,\n",
        "    \"max_rpm\": 60,\n",
        "    \"timeout\": 30,\n",
        "    \"circuit_breaker_cooldown\": 60,\n",
        "    \"model_path\": \"./models/trained_agentic_rl_model\",\n",
        "    \"enable_monitoring\": True,\n",
        "    \"log_level\": \"INFO\"\n",
        "}\n",
        "\n",
        "# Initialize production system\n",
        "prod_system = ProductionAgenticSystem(\"./models/model.pkl\", production_config)\n",
        "\n",
        "# Example health check\n",
        "health_status = prod_system.get_health_status()\n",
        "print(\"\\nSystem Health Status:\")\n",
        "for key, value in health_status.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwe3T1mhx5K"
      },
      "source": [
        "## 8. MLflow Integration for Experiment Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3AqZ7Y8hx5K"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "class MLflowExperimentTracker:\n",
        "    \"\"\"Track RL experiments with MLflow\"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name: str = \"agentic_rl_workflow\"):\n",
        "        self.experiment_name = experiment_name\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        self.client = MlflowClient()\n",
        "\n",
        "    def start_run(self, run_name: str, tags: Dict[str, str] = None):\n",
        "        \"\"\"Start a new MLflow run\"\"\"\n",
        "        mlflow.start_run(run_name=run_name)\n",
        "\n",
        "        if tags:\n",
        "            for key, value in tags.items():\n",
        "                mlflow.set_tag(key, value)\n",
        "\n",
        "    def log_hyperparameters(self, params: Dict[str, Any]):\n",
        "        \"\"\"Log hyperparameters\"\"\"\n",
        "        for key, value in params.items():\n",
        "            mlflow.log_param(key, value)\n",
        "\n",
        "    def log_metrics(self, metrics: Dict[str, float], step: int = None):\n",
        "        \"\"\"Log metrics\"\"\"\n",
        "        for key, value in metrics.items():\n",
        "            mlflow.log_metric(key, value, step=step)\n",
        "\n",
        "    def log_model(self, model, model_name: str):\n",
        "        \"\"\"Log trained model\"\"\"\n",
        "        mlflow.pytorch.log_model(model, model_name)\n",
        "\n",
        "    def log_artifacts(self, artifact_path: str):\n",
        "        \"\"\"Log additional artifacts\"\"\"\n",
        "        mlflow.log_artifacts(artifact_path)\n",
        "\n",
        "    def end_run(self):\n",
        "        \"\"\"End the current MLflow run\"\"\"\n",
        "        mlflow.end_run()\n",
        "\n",
        "    def compare_runs(self, metric_name: str = \"mean_reward\") -> pd.DataFrame:\n",
        "        \"\"\"Compare different experiment runs\"\"\"\n",
        "        experiment = self.client.get_experiment_by_name(self.experiment_name)\n",
        "        runs = self.client.search_runs(\n",
        "            experiment_ids=[experiment.experiment_id],\n",
        "            order_by=[f\"metrics.{metric_name} DESC\"]\n",
        "        )\n",
        "\n",
        "        comparison_data = []\n",
        "        for run in runs:\n",
        "            comparison_data.append({\n",
        "                \"run_id\": run.info.run_id,\n",
        "                \"run_name\": run.info.run_name,\n",
        "                metric_name: run.data.metrics.get(metric_name, None),\n",
        "                \"status\": run.info.status\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(comparison_data)\n",
        "\n",
        "# Example usage\n",
        "tracker = MLflowExperimentTracker(\"agentic_rl_experiments\")\n",
        "\n",
        "# Simulate an experiment run\n",
        "def run_tracked_experiment():\n",
        "    # Start run\n",
        "    tracker.start_run(\n",
        "        run_name=\"ppo_multi_agent_v1\",\n",
        "        tags={\"algorithm\": \"PPO\", \"environment\": \"MultiAgentTask\", \"version\": \"1.0\"}\n",
        "    )\n",
        "\n",
        "    # Log hyperparameters\n",
        "    hyperparams = {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"n_agents\": 4,\n",
        "        \"batch_size\": 64,\n",
        "        \"gamma\": 0.99,\n",
        "        \"n_steps\": 2048\n",
        "    }\n",
        "    tracker.log_hyperparameters(hyperparams)\n",
        "\n",
        "    # Simulate training and log metrics\n",
        "    for step in range(10):\n",
        "        metrics = {\n",
        "            \"mean_reward\": np.random.randn() * 10 + step * 5,\n",
        "            \"task_completion_rate\": np.random.uniform(0.5, 1.0),\n",
        "            \"avg_episode_length\": np.random.randint(100, 200)\n",
        "        }\n",
        "        tracker.log_metrics(metrics, step=step)\n",
        "\n",
        "    # End run\n",
        "    tracker.end_run()\n",
        "\n",
        "    print(\"Experiment logged to MLflow\")\n",
        "\n",
        "# Run the tracked experiment\n",
        "# Uncomment to run with MLflow tracking\n",
        "# run_tracked_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IdYgZoNhx5K"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a comprehensive agentic workflow with reinforcement learning, featuring:\n",
        "\n",
        "1. **Multi-Agent Environment**: Custom Gymnasium environment for collaborative task execution\n",
        "2. **PPO-based Learning**: Agents learn optimal policies through reinforcement learning\n",
        "3. **Tool Integration**: Agents can use external tools with learned selection strategies\n",
        "4. **Production Patterns**: Circuit breakers, rate limiting, and comprehensive error handling\n",
        "5. **Monitoring & Observability**: Performance tracking and visualization\n",
        "6. **MLflow Integration**: Experiment tracking and model versioning\n",
        "\n",
        "The system combines:\n",
        "- **Reinforcement Learning**: PPO for policy optimization\n",
        "- **Multi-Agent Coordination**: Collaborative task solving\n",
        "- **Adaptive Workflows**: Learning-based task allocation\n",
        "- **Production Readiness**: Error handling, monitoring, and deployment considerations\n",
        "\n",
        "This architecture is suitable for complex real-world applications requiring adaptive, learning-based multi-agent systems."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}