# Resources on Reinforcement Learning techniques, methods and Algorithms in Agentic Workflows and in LLM Training

## books

[Multi-Agent Reinforcement Learning: Foundations and Modern Approaches, Stefano V. Albrecht et al, 2024](https://github.com/dimitarpg13/control_and_optimization_of_agentic_workflows/blob/main/books/Multi_Agent_Reinforcement_Learning_Albrecht_2024.pdf)

[Reinforcement Learning from Human Feedback: A Short Intro to RLHF and Post-Training focused on Large Language Models, Nathan Lambert 2026](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Reinforcement_Learning_from_Human_Feedback_A_short_introduction_to_RLHF_and_post-training_focused_on_language_models_Lambert_2026.pdf)


## articles

### Reinforcement Learning from Human Feedback (RLHF)

[A Little Bit of Reinforcement Learning from Human Feedback: A short introduction to RLHF and post-training focused on
language models. Nathan Lambert, 2025](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/A_Little_Bit_of_Reinforcement_Learning_from_Human_Feedback_Lambert_2025.pdf)

[Asymptotics of Language Model Alignment, Joy Qiping Yang et al, U. of Sydney, Google DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/human_like_reasoning/Asymptotics_of_Language_Model_Alignment_Yang_2024.pdf)

[Controlled Decoding from Language Models, Sidharth Mudgal et al, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Controlled_Decoding_from_Language_Models_Mudgal_2024.pdf)

[Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities, Carl Orge Retzlaff et al, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Human-in-the-Loop_Reinforcement_Learning_A_Survey_and_Position_on_Requirements_Challenges_and_Opportunities_Retzlaff_2024.pdf)

[Deep Reinforcement Learnng from Human Preferences, Paul Christiano et al, OpenAI, 2017](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Deep_reinforcement_learning_from_human_preferences_Paul_Christiano_OpenAI_2017.pdf)

[Algorithms for Inverse Reinforcement Learning, Andrew Ng, Stuart Russel, Stanford, 2000](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/Algorithms_for_Inverse_Reinforcement_Learning_Ng_Russel_2000.pdf)

[RL with KL penalties is better viewed as Bayesian inference, Tomasz Korbak et al, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/RL_with_KL_penalties_is_better_viewed_as_Bayesian_inference_Korbak_2022.pdf)

[Training Language Models to Follow Instructions With Human Feedback, L. Ouyang et al, OpenAI, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Training_language_models_to_follow_instructions_with_human_feedback_Ouyang_OpenAI_2022.pdf)

[Fine Tuning Language Models from Human Preferences, Daniel M. Ziegler et al, OpenAI, 2020](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Fine-Tuning_Language_Models_from_Human_Preferences_Ziegler_OpenAI_2020.pdf)

[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Y. Bai et al, Anthropic, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Training_a_Helpful_and_Harmless_Assistant_with_Reinforcement_Learning_from_Human_Feedback_Bai_Anthropic_2022.pdf)

[Learning to Summarize from Human Feedback, Nisan Stiennon et al, OpenAI, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Learning_to_summarize_from_human_feedback_Stiennon_OpenAI_2020.pdf)

[Illustrating Reinforcement Learning from Human Feedback (RLHF), Hugging Face article, 2022, Nathan Lambert, Louis Castricato, Leandro von Werra
, Alex Havrilla](https://huggingface.co/blog/rlhf)

[Learning from human preferences, Dario Amodei, OpenAI blog, 2017](https://openai.com/index/learning-from-human-preferences)

[Reinforcement Learning fro Human Feedback, Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)

[A General Theoretical Paradygm to Understand Learning from Human Preferences, M. Azar et al, Google DeepMind, 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/A_General_Theoretical_Paradigm_to_Understand_Learning_from_Human_Preferences_Azar_2023.pdf)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model, Rafel Rafailov et al, Stanford U., 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_wrokflows/blob/main/articles/RLHF/Direct_Preference_Optimization-Your_Language_Model_is_Secretly_a_Reward_Model_Rafailov_Stanford_2023.pdf)

[SLiC-HF: Sequence Likelihood Calibration with Human Feedback, Y. Zhao et al, Google Deepmind, 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/SLiC-HF-Sequence_Likelihood_Calibration_with_Human_Feedback_Zhao_Google_2023.pdf)

[KTO: Model Alignment as Prospect Theoretic Optimization, K. Ethayarajh et al, Stanford U., 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/KTO-Model_Alignment_as_Prospect_Theoretic_Optimization_Ethayaragh_2023.pdf)

[ORPO: Monolythic Preference Optimization without Reference Model, Hong, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/ORPO-Monolithic_Preference_Optimization_without_Reference_Model_Hong_2024.pdf)

### Human-Like Reasoning in LLMs via pure RL

[Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs, J. Wang et al, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Learning_to_Think-Information-Theoretic_Reinforcement_Fine-Tuning_for_LLMs_Wang_2025.pdf)

[Reinforcement Learning for Reasoning in Large Language Models with One Training Example, Y. Wang et al, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Reinforcement_Learning_for_Reasoning_in_Large_Language_Models_with_One_Training_Example_Wang_2025.pdf)

[TTRL: Test-Time Reinforcement Learning, Y. Zuo et al, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/TTRL-Test-Time_Reinforcement_Learning_Zuo_2025.pdf)

[Absolute Zero: Reinforced Self-play Reasoning with Zero Data, A. Zhao et al, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Absolute_Zero-Reinforced_Self-play_Reasoning_with_Zero_Data_Zhao_2025.pdf)

[Process Reward Models That Think, M. Khaliffa et al, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Proces_Reward_Models_That_Think_Khalifa_2025.pdf)

[Concise Reasoning via Reinforcement Learning, M Fatemi et al, Wand AI, 2025](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Concise_Reasoning_via_Reinforcement_Learning_Fatemi_2025.pdf)

[Asymptotics of Language Model Alignment, Joy Qiping Yang et al, U. of Sydney, Google DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Asymptotics_of_Language_Model_Alignment_Yang_2024.pdf)

[RL on Incorrect Synthetic Data Scales The Efficiency of LLM Math Reasoning by Eight-Fold, Amrith Setlur et al, CMU, DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/RL_on_Incorrect_Synthetic_Data_Scales_the_Efficiency_of_LLM_Math_Reasoning_by_Eight-Fold_Setlur_CMU_2024.pdf)

[Improve Mathematical Reasoning in Language Models by Automated Process Supervision, L. Luo et al, DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Improve_Mathematical_Reasoning_in_Language_Models_by_Automated_Process_Supervision_Luo_2024.pdf)

[Solving math word problems with processand outcome-based feedback, J. Uesato et al, DeepMind, 2022](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Solving_math_word_problems_with_process_and_outcome-based_feedback_Uesato_DeepMind_2022.pdf)

[Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking, Eric Zelikman et al, Stanford U., 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Quiet-STaR-Language_Models_Can_Teach_Themselves_to_Think_Before_Speaking_Zelikman_2024.pdf)

[STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning, E. Zelikman et al, 2022](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/STaR-Self-Taught_Reasoner_Bootstrapping_Reasoning_With_Reasoning_Zelikman_2022.pdf)

[Training Language Models to Self-Correct via Reinforcement Learning,  Aviral Kumar et al, Google DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_and_game_theory/blob/main/articles/ReinforcementLearning/human_like_reasoning/Training_Language_Models_to_Self-Correct_via_Reinforcement_Learning_Kumar_2024.pdf)


### Reinforcement Learning - Based Feedback Loop in Agentic Workflows

