# Resources on Reinforcement Learning techniques, methods and Algorithms in Agentic Workflows and in LLM Training

## books

[Reinforcement Learning from Human Feedback: A Short Intro to RLHF and Post-Training focused on Large Language Models, Nathan Lambert 2026](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/reinforcement_learning_from_human_feedback/Reinforcement_Learning_from_Human_Feedback_A_short_introduction_to_RLHF_and_post-training_focused_on_language_models_Lambert_2026.pdf)


## articles

### Reinforcement Learning from Human Feedback (RLHF)

[A Little Bit of Reinforcement Learning from Human Feedback: A short introduction to RLHF and post-training focused on
language models. Nathan Lambert, 2025](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/A_Little_Bit_of_Reinforcement_Learning_from_Human_Feedback_Lambert_2025.pdf)

[Asymptotics of Language Model Alignment, Joy Qiping Yang et al, U. of Sydney, Google DeepMind, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/human_like_reasoning/Asymptotics_of_Language_Model_Alignment_Yang_2024.pdf)

[Controlled Decoding from Language Models, Sidharth Mudgal et al, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Controlled_Decoding_from_Language_Models_Mudgal_2024.pdf)

[Human-in-the-Loop Reinforcement Learning: A Survey and Position on Requirements, Challenges, and Opportunities, Carl Orge Retzlaff et al, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Human-in-the-Loop_Reinforcement_Learning_A_Survey_and_Position_on_Requirements_Challenges_and_Opportunities_Retzlaff_2024.pdf)

[Deep Reinforcement Learnng from Human Preferences, Paul Christiano et al, OpenAI, 2017](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Deep_reinforcement_learning_from_human_preferences_Paul_Christiano_OpenAI_2017.pdf)

[Algorithms for Inverse Reinforcement Learning, Andrew Ng, Stuart Russel, Stanford, 2000](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/Algorithms_for_Inverse_Reinforcement_Learning_Ng_Russel_2000.pdf)

[RL with KL penalties is better viewed as Bayesian inference, Tomasz Korbak et al, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/RL_with_KL_penalties_is_better_viewed_as_Bayesian_inference_Korbak_2022.pdf)

[Training Language Models to Follow Instructions With Human Feedback, L. Ouyang et al, OpenAI, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Training_language_models_to_follow_instructions_with_human_feedback_Ouyang_OpenAI_2022.pdf)

[Fine Tuning Language Models from Human Preferences, Daniel M. Ziegler et al, OpenAI, 2020](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Fine-Tuning_Language_Models_from_Human_Preferences_Ziegler_OpenAI_2020.pdf)

[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Y. Bai et al, Anthropic, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Training_a_Helpful_and_Harmless_Assistant_with_Reinforcement_Learning_from_Human_Feedback_Bai_Anthropic_2022.pdf)

[Learning to Summarize from Human Feedback, Nisan Stiennon et al, OpenAI, 2022](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/Learning_to_summarize_from_human_feedback_Stiennon_OpenAI_2020.pdf)

[Illustrating Reinforcement Learning from Human Feedback (RLHF), Hugging Face article, 2022, Nathan Lambert, Louis Castricato, Leandro von Werra
, Alex Havrilla](https://huggingface.co/blog/rlhf)

[Learning from human preferences, Dario Amodei, OpenAI blog, 2017](https://openai.com/index/learning-from-human-preferences)

[Reinforcement Learning fro Human Feedback, Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)

[A General Theoretical Paradygm to Understand Learning from Human Preferences, M. Azar et al, Google DeepMind, 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/A_General_Theoretical_Paradigm_to_Understand_Learning_from_Human_Preferences_Azar_2023.pdf)

[Direct Preference Optimization: Your Language Model is Secretly a Reward Model, Rafel Rafailov et al, Stanford U., 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_wrokflows/blob/main/articles/RLHF/Direct_Preference_Optimization-Your_Language_Model_is_Secretly_a_Reward_Model_Rafailov_Stanford_2023.pdf)

[SLiC-HF: Sequence Likelihood Calibration with Human Feedback, Y. Zhao et al, Google Deepmind, 2023](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/SLiC-HF-Sequence_Likelihood_Calibration_with_Human_Feedback_Zhao_Google_2023.pdf)

[KTO: Model Alignment as Prospect Theoretic Optimization, K. Ethayarajh et al, Stanford U., 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/KTO-Model_Alignment_as_Prospect_Theoretic_Optimization_Ethayaragh_2023.pdf)

[ORPO: Monolythic Preference Optimization without Reference Model, Hong, 2024](https://github.com/dimitarpg13/reinforcement_learning_in_agentic_workflows/blob/main/articles/RLHF/ORPO-Monolithic_Preference_Optimization_without_Reference_Model_Hong_2024.pdf)

### Human-Like Reasoning in LLMs via pure RL

### Reinforcement Learning - Based Feedback Loop in Agentic Workflows

